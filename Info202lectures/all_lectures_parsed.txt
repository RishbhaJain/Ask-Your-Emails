
================================================================================
FILE: 10_metadata_databases.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 10: Structuring Data, Metadata, Databases

--- Page 2 ---
Today’s Outline
Structuring Information
Metadata
Intro to Databases
Schemas
This Week’s Assignment

--- Page 3 ---
Structuring Information
Start with Develop
Decide what will be
something in the descriptions for
retained and what
world that needs to what is being
will be ignored
be organized retained
Iterate on those
Assign descriptions
descriptions,
to the things that
comparing them to
are being organized
evaluation criteria

--- Page 4 ---
Purpose:
Label your photo
bird
plant
rock

--- Page 5 ---
iNaturalist founded by Ken-ichi Ueda as a MIMS project
Here speaking at our commencement in 2023

--- Page 6 ---
Purpose:
Update your
iNaturalist
profile
Pelagic cormorant,
Breeding adult
Sea fig ice plant
Sandstone

--- Page 7 ---
Purpose:
International
Bird Count
Urile pelagicus
Count: 14
Date and Time:
4/10/2021
10:00 PT
Latitude / Longitude:
38.6910446187637,
-123.43691202010656

--- Page 8 ---
Data, Descriptions, Metadata, Metadata Description
Phenomena in Data: What Data MetaData Metadata
the World Items are Representation Standard
Collected

--- Page 9 ---
What is the Metadata?
Phenomena in Data: What Data MetaData Metadata
the World Items are Representation Standard
Collected
Photos of birds, Pixels
stored on hard
drive

--- Page 10 ---
W I M ?
HAT S ETADATA
• Data that describes other data / information
• Example:
• Information: blog post
• Metadata: language, length, author, date,
publisher, keywords, sentiment, …
• Can be created manually or automatically

--- Page 11 ---
W D W N M ?
HY O E EED ETADATA
Main reason: To Organize Collections
Expanding on this:
• To make representation of information consistent
• To make retrieval / search efficient and effective
• To enable sharing and re-use of information
• To support auditing (records of changes)

--- Page 12 ---
Data, Descriptions, Metadata, Metadata Description
Phenomena in Data: What Data MetaData Metadata
the World Items are Representation Standard
Collected
Photos of birds, Pixels Image size, EXIF (automatic)
stored on hard Image format, IPTC (manual);
drive Dat/time taken, now XMP is a
Lat/Long, new standard
Camera settings

--- Page 13 ---
You can see
the image
metadata by
right-clicking
on the photo
in some apps

--- Page 15 ---
Example: A Specimens Database
https://essig.berkeley.edu/

--- Page 16 ---
Example: A Specimens Database

--- Page 17 ---
Data, Descriptions, Metadata, Metadata Description
Phenomena in Data: What Data MetaData Metadata
the World Items are Representation Description
Collected Language
Photos of birds, Pixels Image size, EXIF (automatic)
stored on hard Image format, IPTC (manual);
drive Dat/time taken, now XMP is a
Lat/Long, new standard
Camera settings
Real bird Physical bird Measurements, Specimens
specimen type, date, database
instrument, etc Schema (e.g.
EME)

--- Page 18 ---
Part of a Specimens Database Schema
https://essigdb.berkeley.edu/schema.txt

--- Page 19 ---
Another Bird Specimens Database

--- Page 20 ---
Bird Specimens Database
(selected attributes)
https://collections.nmnh.si.edu/search/birds/?v=g0

--- Page 21 ---
Bird Specimens Database
(selected attributes)
https://collections.nmnh.si.edu/search/birds/?v=g0

--- Page 22 ---
Bird Species XML (more on this next lecture)
http://webdam.inria.fr/Jorge/prog/birds.xml

--- Page 23 ---
Data, Descriptions, Metadata, Metadata Description
Phenomena in Data: What Data MetaData Metadata
the World Items are Representation Description
Collected Language
Photos of birds, Pixels Image size, EXIF (automatic)
stored on hard Image format, IPTC (manual);
drive Dat/time taken, now XMP is a
Lat/Long, new standard
Camera settings
Real bird Physical bird Measurements, Specimens
specimen type, date, database
instrument, etc Schema (e.g.
EME)
PDF document of Dollar amount Date, invoice Database
Work done an invoice embedded in number, payee, Schema
on a contract PDF amount due, etc

--- Page 24 ---
E :
XERCISE
I
DENTIFY THE
For Spotify
M
ETADATA

--- Page 25 ---
E :
XERCISE
• Look several Kaggle.com competitions
• Examine the metadata
• What does it consist of?
• How are they similar and different across
collections?

--- Page 26 ---
Databases

--- Page 27 ---
D
ATABASES
• Collections of records
• Highly structured
• Very much associated with computerization
• Mainly needed when there are requirements for:
• Scaling with large amounts of data
• Accessing reliably over time by many parties

--- Page 28 ---
Three Database Revolutions:
What To Draw from This
Reading:
• An overall impression of the history
• The relationship between computer hardware and
developments in data storage and retrieval
• The role of DBMSs in the past and in relation to data
science today
• Don’t sweat the technical details; this should be a
useful contextualizing reference when you hear/read
unfamiliar terms.

--- Page 29 ---
Reading: Three Database Revolutions

--- Page 30 ---
D S H
ATABASE YSTEMS ISTORY
• The database systems field began in the 60s with computerization
• Took off in the 1970s thanks to a new “data model” – a way of
representing and thinking about data – Codd’s Relational Model
• Led to commercial and open source relational databases
• Relational databases are still at the core of the software
technology stack of most companies today.

--- Page 31 ---
UC Berkeley and RDBMS History
Jim Gray: Michael Stonebraker
1998 Turing Award recipient 2014 Turing award recipient
First CS PhD student at UCB Former UCB professor

--- Page 32 ---
UC Berkeley and RDBMS History
Matei Zaharia, Ion Stoica,, et al.,
Margo Seltzer, Keith Bostic, Mike Olson
Berkeley Profs; Spark Project led to
Berkeley PhD and Researchers
Databricks

--- Page 33 ---
R D M
ELATIONAL ATABASE ANAGEMENT
S
YSTEMS
(RDBMS)
HUGELY successful – nearly 50 years!
• System R and Ingres in 1973-4
• Oracle in 1977
SQL a huge improvement over ad hoc programming
ENORMOUS amounts of research and development into
making RDBMS very fast, very reliable
Nothing could do better for decades

--- Page 34 ---
T R D M
HE ELATIONAL ATA ODEL
• A relation is a set of rows (also called tuples)
• Each row consists of a predefined set of attributes
• A database is a collection of relations
• These relations together define the data model
Relation / Table Attributes / Columns
Name Price Category Brand
Climber $120 Boot REI
Rows / Tuples / Records Lita $98 Flats West
Arigato $55 Sneaker Keds
Slide adapted from Aditya Parameswaran

--- Page 35 ---
The Relational
Model
wikipedia

--- Page 36 ---
R D M
ELATIONAL ATABASE ANAGEMENT
S
YSTEMS
(RDBMS)
• Efficient, reliable for transaction processing:
• Flight reservations
Transfer $1000 from A to B’s account
• Financial transactions: 1. Debit A’s account
2. Credit B’s account
3. Update metadata
• What happens if the system shuts down in the
middle?
• ACID Model of Transaction Processing handles
it

--- Page 37 ---
R D : ACID M
ELATIONAL ATABASES ODEL
: C , R
GOAL ONSISTENCY ELIABILITY
• Atomicity
• Each transaction is treated as a single unit, which either succeeds
completely, or fails completely:
• Consistency / Correctness
• A transaction can only bring the database from one valid state to another
• Isolation (Concurrency)
• Concurrent execution of transactions leaves the database in the same state
that would have been obtained if the transactions were executed
sequentially
• Durability
• Once a transaction has been committed, it will remain committed even in
the case of a system failure

--- Page 38 ---
T A
RANSACTIONS VS NALYSIS
• Databases are often subdivided into:
• OLTP (Online Transaction Processing)
• OLAP (Online Analytical Processing)
• OLTP focuses on “transactions”, OLAP focuses
on “large-scale analysis”

--- Page 39 ---
T G D
HIRD ENERATION ATABASES
• Is still an active area of development
• Different solutions tailored to different applications
• Semi-structured data
• Streaming data
• Sometimes “bolt-on” solutions to relational
databases; sometimes entirely new solutions
• MongoDB is the most popular

--- Page 40 ---
To Learn More…
To learn about SQL, database
normalization, primary and foreign
keys, ER diagrams, etc, etc….
Prof Aditya Parameswaran:

--- Page 41 ---
W S ?
HAT IS A CHEMA
• Schema: The overall structure of the metadata
• Database schema: relation names, attribute names, attribute
types, keys that link relations, and rules that enforce structure
• XML schema: the possible types of content in a document
and the rules that govern the structure and values of that
content.

--- Page 42 ---
R S
ELATIONAL CHEMA
• Schema: the structure, format or scaffolding
• Schema for a relation:
• Relation names plus attribute names & types
• Product (Name String, Price Float, Category String, Manuf. String)
• Schema for a Database:
• Collection of schemas for many relations, and the keys that link
them, and in some cases rules that enforce constraints among
relations
• Product(…)
Metadata: organized with the
• Brand (…)
Schema
Data is: Instance of a database
• Schema in RDB changes very rarely
with “values filled in”
Slide adapted from Aditya Parameswaran

--- Page 43 ---
T R M : W ’ M ?
HE ELATIONAL ODEL HAT S ISSING
• Not good for semi-structured data
• Documents
• Web pages
• Not good for hierarchically structured data
• Not good for graph-structured datga

--- Page 44 ---
R S -S
EPRESENTING EMI TRUCTURED
D
ATA
• Semi-structured data is less “rigid” than structured data
• As semi-structured data became available online, this exposed a need for new
representations (beyond relational)
• XML and JSON became the most popular
• They allow for a flexible format, multi-valued attributes, and nested attributes
• Because of their self-describing or markup nature, they are commonly used
for interchange of data
• And for describing metadata about documents

--- Page 45 ---
I S -S D N ?
S EMI TRUCTURED ATA EW
• In some ways, it is old; similar to the systems described in phase 1
of the 3 Database Revolutions paper.
• Some of these were nested / hierarchical in structure
• Most were eventually abandoned
• Representation redundancies, leads to errors when making changes
• Difficult to efficiently retrieve across relations (link authors to papers)
• Today: there are specialized systems for specialized tasks
• There are XML and JSON-oriented database systems
• MongoDB is very popular for semi-structured data
Slide adapted from Aditya Parameswaran

--- Page 46 ---
T W ’ A
HIS EEK S SSIGNMENT
You have two weeks to complete it!

--- Page 47 ---
Class Familiarity w/XML, HTML, etc
¼ to ½ the class not familiar / beginner
If you are advanced, feel free to go beyond the assignment

--- Page 48 ---
T W ’ A
HIS EEK S SSIGNMENT
T G
HREE MAIN OALS
• Goal 1: practice with metadata markup
• Mark up some content in JSON
• Mark up some content in XML

--- Page 49 ---
T W ’ A
HIS EEK S SSIGNMENT
• Goal 2: exposure to building a simple website
• A gentle introduction to editing HTML
• Switching CSS files to see different effects
• Learning how to post content to github pages
• Goal 3: more JSON experience
• Create a dataset in JSON
• Display it in an HTML table

--- Page 50 ---
The Website

--- Page 51 ---
The Website

--- Page 52 ---
N T
EXT IME
• Semi-structured Data
• Markup Languages:
• HTML
• XML
• Data format
• JSON


================================================================================
FILE: 11_schemas_markup.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 11: Metadata Descriptor Languages

--- Page 2 ---
Today’s Outline
Brief Review
Markup Languages (HTML, XML)
Data Formats (JSON)
Metadata Standards

--- Page 3 ---
W S ?
HAT IS A CHEMA
The structure, format, scaffolding of
metadata

--- Page 4 ---
T R D M
HE ELATIONAL ATA ODEL
• A relation is a set of rows (also called tuples)
• Each row consists of a predefined set of attributes
• A database is a collection of relations
• These relations together define the data model
Relation / Table Attributes / Columns
Name Price Category Brand
Climber $120 Boot REI
Rows / Tuples / Records Lita $98 Flats West
Arigato $55 Sneaker Keds
Slide adapted from Aditya Parameswaran

--- Page 5 ---
T R M : W ’ M ?
HE ELATIONAL ODEL HAT S ISSING
• Not good for semi-structured data
• Documents
• Web pages
• Hierarchically structured (nested) information

--- Page 6 ---
W T T F T R
HAT O AKE ROM HIS EADING
(XML F )
OUNDATIONS
• What HTML and CSS are for – and not for
• HTML vs XML – the difference
• Separation of content and presentation
• Markup Language Syntax basics
• The Main Goals of XML
• The Role of Schemas (like DTDs) in XML
• The ability to make a simple XML DTD

--- Page 7 ---
Another good XML Reading
https://www.tei-c.org/release/doc/tei-p5-doc/en/html/SG.html

--- Page 8 ---
W
HAT IS A
M L ?
ARKUP ANGUAGE
A set of instructions on a manuscript or tags in an electronic document to
determine styles of type, makeup of pages, and the like.
https://www.thefreedictionary.com/markup

--- Page 9 ---
C . S . P
ONTENT VS TRUCTURE VS RESENTATION
Content - "what does it mean"
Structure - "how it is organized or assembled"
Presentation - "how does it look" / "how is it displayed"
9

--- Page 10 ---
HTML & CSS
• HTML represents the structure of the document
• A small, restricted set of tags
• CSS represents the style (formatting) rules
• Neither represent the meaning or semantics

--- Page 11 ---
HTML From Your Homework Assignment
(index.html)

--- Page 12 ---
HTML From Your Homework Assignment
(index.html)

--- Page 13 ---
CSS From Your Homework Assignment
(style.css, from Bootstrap)

--- Page 14 ---
HTML/CSS B
AND THE ROWSER
• HTML / CSS are visually rendered in a web browser
(chrome, firefox, etc).
• The web browser has software to convert the tags into visual display
• The W3C sets the standards for how to render the tags
https://developer.mozilla.org/en-US/docs/Learn/CSS/First_steps/How_CSS_works

--- Page 15 ---
T HTML
EXT VS
Viewed by human Processed by web browser

--- Page 16 ---
W XML?
HAT IS

--- Page 17 ---
W XML?
HAT IS
• XML stands for eXtended Markup Language
• Descended from SGML (Standard Generalized Markup Language)
• An international standard (SGML- ISO 8879:1986)
• Adopted by the W3C in 1998
• A generic language for describing the content and structure of documents, and markup
that can be used for those documents. Can be used for data too.
• XML enabled web applications to exchange data in an understandable text format
• Transformed the way information is exchanged online
• JSON is perhaps more popular for data (vs documents)
• What it is NOT:
• Not a visual document description
• Not an application specific markup
• Not proprietary

--- Page 18 ---
T E XML
HE XTENSIBLE IN
• HTML: a fixed set of format-oriented tags
• Meant to be processed “by eye”
• The web browser makes it visual
• XML: you decide on the tag set
• Meant to be processed by computer
• Supports nesting / hierarchy

--- Page 19 ---
XML HTML
VS
<TransportSchedule Type=“Airline”>
<Segment Id=“United Airlines #200”>
<Origin>San Francisco</Origin>
<DepartTime>9:30 AM</DepartTime>
<Destination>Honolulu</Destination>
<ArriveTime>12:30 PM</ArriveTime>
<Price Currency=“USD”>368.50</Price>
</Segment>
</TransportSchedule>
Notice: content vs presentation

--- Page 20 ---
XML can be nested (hierarchical)
http://webdam.inria.fr/Jorge/prog/birds.xml

--- Page 21 ---
XML S V
CHEMA AND ALIDATION
• An XML document REQUIRES ONLY the document instance
• But for real world usage, a DTD or Schema (XSD) is important.
• DTD is the older, simpler version of the XSD Schema
• DTD: Document Type Definition
• More recently: Relax NG is used for schema definition
• The XML document is validated against the DTD/Schema
• Specialized validation programs are used for this.
• A modern list can be found here: https://relaxng.org/

--- Page 22 ---
Sample XML
From TDO

--- Page 23 ---
E DTD
XAMPLE
The major components:
Entity Declarations
Element Declarations
Attribute Declarations
#PCDATA: text (character data)
#IMPLIED means optional attribute
( x | y) means OR
+ means non-empty list
https://www.w3schools.com/xml/xml_dtd_intro.asp
https://www.w3schools.com/xml/xml_dtd_attributes.asp

--- Page 24 ---
A E
TTRIBUTES XAMPLE
There are a variety of special defaults and data types that
can be given in attribute definitions
<!ATTLIST memo status (PUBLIC | CONFIDENTIAL) PUBLIC>
This sets the default status for “memo” to public, but allows the
status for “memo” be set to either public or confidential.

--- Page 25 ---
L ’ T O
ET S EST UR
U !
NDERSTANDING
https://pollev.com/i202

--- Page 26 ---
M S
ETADATA TANDARDS

--- Page 27 ---
T M D -S
HERE ARE ANY OMAIN PECIFIC
M S
ETADATA TANDARDS
• Naming and ID systems (URIs, ISBN)
• Bibliographic / Document description (MARC, RDF, Dublin Core,
TEI)
• Music (SMDL)
• Images and objects (CIMI, Getty Art Categories, VRA Core
Categories)
• Numeric Data (SDSM, ICPSR)
• Geospatial Data (FGDC, ASTM)

--- Page 28 ---
D C
UBLIN ORE
• Well-known, early standard
• Simple metadata for describing internet
resources.
• For “Document-Like Objects”
• 15 Elements.

--- Page 29 ---
Dublin Core Elements
• Title • Format
• Creator • Resource Identifier
• Source
• Subject
• Language
• Description
• Relation
• Publisher
• Coverage
• Other Contributors
• Rights Management
• Date
• Resource Type

--- Page 30 ---
TEI (Text Encoding Initiative) Standard
(Used in the Digital Humanities, here
Drama)
https://teibyexample.org/tutorials/TBED05v00.htm

--- Page 32 ---
Simple TEI Example
https://www.tei-c.org/release/doc/tei-p5-doc/en/html/SG.html

--- Page 33 ---
Simple TEI Example
https://www.tei-c.org/release/doc/tei-p5-doc/en/html/SG.html

--- Page 34 ---
A simple schema written in Relax NG
https://www.tei-c.org/release/doc/tei-p5-doc/en/html/SG.html

--- Page 35 ---
E : W U
XERCISE HAT IS THE NDERLYING
S ?
CHEMA
• Look at the example of song XML markup:
• https://gist.github.com/jasonbaldridge/2597611
• Based on this, what do you think the schema is?
• Refer to this poetry example

--- Page 36 ---
M Q L :
ETADATA UERY ANGUAGES
XQ
UERY
• General purpose query language for XML
• W3C standard
• Derived from earlier ones
• XPath
• XML-QL
This query asks for a list of the
unique speakers in each act of
Shakespeare's play Hamlet:
https://en.wikipedia.org/wiki/XQuery

--- Page 37 ---
Metadata Query Languages
This query returns names and emails of every person in
the dataset:
https://en.wikipedia.org/wiki/SPARQL

--- Page 38 ---
SQL vs XQuery
SQL (Relational Data) XQuery (semi-structured)
• Flat; rows and • Nested
columns
• Data is highly variable
• Data is uniform
• Elements are ordered
• Rows are unordered
http://wwwlgis.informatik.uni-kl.de/cms/fileadmin/courses/SS2014/Neuere_Entwicklungen/Chapter_8_-_XQuery.pdf

--- Page 39 ---
Ontology vs HTML
Ontology HTML
• For representing data • For presenting info
• Specific relation types • Links don’t have types
• Inference over relations • Links don’t have
semantics (except “get the
linked page”)
http://wwwlgis.informatik.uni-kl.de/cms/fileadmin/courses/SS2014/Neuere_Entwicklungen/Chapter_8_-_XQuery.pdf

--- Page 40 ---
S F O
CHEMA OR NTOLOGIES
• There has been a lot of development over decades
• Currently, OWL is the standard
• OWL: Web Ontology Language
• Most commonly stored in RDF / XML format
• RDF: Resource Description Framework
• Can use JSON
• Represents information as Triples (also called Tuples)
• Protégé is a popular, free GUI for ontologies
Jean-Baptiste, Ontologies with Python, O’Reilly,
2020

--- Page 41 ---
Protégé Tool for Editing Ontologies
Jean-Baptiste, Ontologies with Python, O’Reilly, https://protege.stanford.edu/
2020

--- Page 42 ---
S / C / S
TORING OMPUTING EARCHING
O
NTOLOGIES
• Owlready
• Python interface to OWL
• Neo4j
• Java interface for graphs
• QuadStore
• RDF-backed database for Node.js
• Quad: (subject, predicate, object, graph)
• SPARQL
• Query language for searching in an RDF graph

--- Page 43 ---
Architecture of the Owl-ready Python
Interface for Ontologies
Jean-Baptiste, Ontologies with Python, O’Reilly,
2020

--- Page 44 ---
D F
ATA ORMATS
JSON

--- Page 45 ---
JSON
• Stands for Javascript Object Notation
• Textual representation for complex data types
• Java stores data as binary, JSON can be either
• Now is widely used for transmitting data between
applications and storing complex data
• Especially widely used in internet applications
• Easily manipulated by javascript in the browser
• More structure than a CSV file
Slide adapted from Aditya Parameswaran

--- Page 46 ---
JSON Can Represent Hierarchy

--- Page 47 ---
JSON Can Represent Hierarchy
Slide adapted from Aditya Parameswaran

--- Page 48 ---
A JSON
DVANTAGES OF
• Flexibility in set of attributes
• Can add a new attribute in one tuple without changing others
• Human-readable, key-value pairs
• Easily parsed within JavaScript in the browser
• Smaller in space required than XML

--- Page 49 ---
D JSON
ISADVANTAGES OF
• JSON encourages redundancy in the
representation
• This makes it take up more space than some
other representations, but it is more human
readable
• Not efficient to flexibly search at scale
• Standard JSON does not have a schema.
• Can be seen as a pro or a con – makes it flexible
but harder to know what you are getting with a
new file

--- Page 50 ---
JSON vs XML
https://www.guru99.com/json-vs-xml-difference.html

--- Page 51 ---
JSON XML
VS
JSON XML
Data format Markup language
Smaller space Larger space
Types: string, integer, array, Boolean Typeless
Relatively easy to read Relatively difficult to read
Objects easily accessible as objects by Requires complex parsing
programs
Does not support namespaces, fewer Supports name spaces, many encodings
encodings
No widely adopted schema language Several schema languages (DTD, XSD, etc)
https://www.guru99.com/json-vs-xml-difference.html

--- Page 52 ---
JSON Y A
IN OUR SSIGNMENT
( ; )
THIS IS MY LIST YOU WILL MAKE YOUR OWN
List of
publications
Each publication has a
list of attributes
Notice the ”key: value” notation
to separate key-value pairs

--- Page 53 ---
JSON Y A
IN OUR SSIGNMENT
• The file list.html contains:
• HTML code that
• Creates the header
• Creates the body, including a div called
showDataJSON
• Javascript code that
• Reads in the JSON data
• Creates an HTML table
• Fills in the table with the contents of the JSON data
• Places the table at the div called showDataJSON

--- Page 54 ---
J HTML
AVASCRIPT WITH IN YOUR
ASSIGNMENT
• Javascript goes beyond the HTML standard
• It requires a web server to run
• In our assignment, we use github pages as our web
server
More details:
https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_web_server

--- Page 55 ---
The Website

--- Page 56 ---
The Website

--- Page 58 ---
Page header and nav bar

--- Page 59 ---
Section for table
Table will be filled in with json data

--- Page 61 ---
Javascript code
Reads in the json
Creates the table object
Sets the table style

--- Page 62 ---
Creates objects for the
table headers
Creates objects for the
table rows
Inserts the table
objects into the HTML

--- Page 63 ---
S
UMMARY
• Metadata is data about data
• Schemas are the structure of metadata for a collection
• Relational databases are a particular type of data
organization that allow fast, reliable processing
• Markup languages provide a standardized, computerizable
representation of metadata and data, representing structure,
presentation, and/or semantics.

--- Page 64 ---
Next week’s focus
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation


================================================================================
FILE: 12_lexical_relations.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 12: Lexical Relations

--- Page 2 ---
M R HTML D
ORE ECENT EVELOPMENTS
• HTML5 includes some tags that assign meaning to the
structural elements
• <nav>, <article>, <section>, <aside>, etc
• Data structuring standards are growing in importance
• JSON-LD, RDFa, making us of Schema.org markup
• Purposes:
• Aid in machine parsing, for search engine optimization
• Align with accessibility goals to aid screen readers
https://almanac.httparchive.org/en/2024/structured-data

--- Page 3 ---
Semantic HTML
https://www.semrush.com/blog/semantic-html5-guide/

--- Page 4 ---
Semantic HTML
Suk Min Huang’s lab notes: https://groups.ischool.berkeley.edu/i253/fl25/slides/week1-lab1.pdf

--- Page 5 ---
Semantic HTML tags
https://www.w3schools.com/html/html5_semantic_elements.asp https://almanac.httparchive.org/en/2024/accessibility

--- Page 6 ---
This Week’s Focus
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

--- Page 7 ---
Today’s Outline
The Vocabulary Problem
Semantic vs Lexical
Lexical Relations
WordNet
Controlled Vocabs in IA

--- Page 8 ---
T V P
HE OCABULARY ROBLEM
The same idea can be (and is) expressed an astonishing number of ways.
This variability has its roots in how we think and use language.
This causes problems for many IT systems.

--- Page 9 ---
From Lecture 3:
Representations of Names Can Be Noisy
Jones, R. Fingerprinting and Metadata Progress Report. Last.fm: The Blog, 2008., via Hemerly, Making Metadata: The Case of MusicBrainz, SSRN 2011

--- Page 10 ---
V P E :
OCABULARY ROBLEM XAMPLE
N S R
AMING FOR MART OOMS
EXERCISE:
What should the names
of devices be to allow for
controlling each
component by voice?
pollev.com/I202
Meghan Clark, One-Shot Interactions with Intelligent Assistants in
Unfamiliar Smart Spaces, UCB Dissertation, 2021

--- Page 11 ---
V P E :
OCABULARY ROBLEM XAMPLE
N S R
AMING FOR MART OOMS
How did she solve this? Augmented reality and
autosuggest!
Meghan Clark, One-Shot Interactions with Intelligent Assistants in Unfamiliar Smart Spaces, UCB Dissertation, 2021

--- Page 12 ---
S : V P E :
UMMARY OCABULARY ROBLEM XAMPLE
N S R
AMING FOR MART OOMS
• Device names were mostly not guessable
• Popular names could apply to multiple devices
• The order of naming had an effect
• If someone started with the left blind group, they would call them “blinds”,
thus affecting subsequent names
• Names do not always capture relationships
• Group: “spot lights” but left light called “recessed light”
• Frames of reference can differ
• Front / back vs left / right – depended on assumptions of orientation
Meghan Clark, One-Shot Interactions with Intelligent Assistants in Unfamiliar Smart Spaces, UCB Dissertation, 2021

--- Page 13 ---
L ’ D D L ’
ET S IVE EEPER INTO ANGUAGE S
C
OMPLEXITIES

--- Page 14 ---
S L
EMANTIC VS EXICAL

--- Page 15 ---
S L
EMANTIC VS EXICAL
• Semantic: the meaning
• Lexical: how the meaning is expressed in words
“What's in a name? That which we call a rose
By any other name would smell as sweet”
Shakespeare, Romeo and Juliet
Debby Hudson

--- Page 16 ---
L R
EXICAL ELATIONSHIPS
• Synonymy
• Polysemy
• Homography
• Hyperonymy / Hyponymy
• Metonymy

--- Page 17 ---
S
YNONYMS
• Different lexical forms that have the same meaning in some or all contexts
More formally, two lexemes are strict synonyms if:
couch / sofa
• They can be substituted for each other in all situations
auto / car
• They have the same meaning
• Very few synonyms meet the strict requirement:
This is a big day.
This is a large day.
Nurse, how did the patient do today? Doctor, he vomited this morning.
Nurse, how did the patient do today? Doctor, he puked this morning.

--- Page 18 ---
P
OLYSEMY
Polysemous words have multiple senses -- often loosely related
“bank" (financial) has several related senses:
bank: Let’s walk over to the bank. (meaning the building on Shattuck)
bank: The bank’s stock went up today. (an institution, e.g., Wells Fargo)
bank: Put your money in the bank. (where money is kept, abstract)

--- Page 19 ---
H . P
OMOGRAPHY VS OLYSEMY
• A homograph is a word with multiple senses that are not related
• bat (stick), bat (flying mammal)
• Polysemy: senses that are closely related (close to the prototype)
• money (a medium of exchange), money (as wealth; he comes from money)
• What looks like homography may be polysemy, often for historical reasons
• mole (the animal), mole (the spy)
• Homophones: like homographs, but have different spellings but pronounced the same
way
• write / right

--- Page 20 ---
H H
YPONYMY AND YPERNYMY
• When words encode IS-A or inclusion relationships, the word for the more
specific class is the HYPONYM and the other is the HYPERNYM
• This builds a "lexical hierarchy" that represents the "semantic hierarchy"
• Often used to situate basic categories with respect to superordinate and
subordinate categories
vehicle is a hypernym of car
mango is a hyponym of fruit
Wikipedia

--- Page 21 ---
DIFFERENT LEXICAL FORM SAME LEXICAL FORM
hypernyms polysemes
(superordinate)
WORD
hyponyms
(subordinate) homographs

--- Page 22 ---
DIFFERENT LEXICAL FORM SAME LEXICAL FORM
hypernyms polysemes
cooking utensil
(superordinate)
pot (betting)
pot (flower)
pot
kettle
hyponyms pot
coffee pot
(subordinate) homographs

--- Page 23 ---
DIFFERENT LEXICAL FORM SAME LEXICAL FORM
hypernyms hypernyms
(superordinate) (superordinate)
synonyms polysemes
WORD
sibling terms homographs
hyponyms
hyponyms
(subordinate)
(subordinate)

--- Page 24 ---
DIFFERENT LEXICAL FORM SAME LEXICAL FORM
hypernyms hypernyms
(superordinate) (superordinate)
---
fundamental measure
synonyms polysemes
time period (menstrual)
period
(amount of time)
sibling terms homographs
temperature, mass, length
period (punctuation)
hyponyms
hyponyms
week, past, eve… (subordinate)
(subordinate)
trial period, test period

--- Page 25 ---
Meronymy and Holonymy
The lexical expression of the
part-whole relation.
wire vs. trap:
Wire is meronym of trap
Trap is holonym of wire
From U.S. Patent No. 6,655,077, titled “Trap for a mouse.”

--- Page 26 ---
Metonymy
Substituting the name of an attribute or feature
for the name of the thing itself.
“We are hearing from California
Hall that we have budget cuts.”
“She has a good ear for music.”
(metonymy and polysemy)
UC Berkeley

--- Page 27 ---
T M ! L F !
HERE ARE ORE ANGUAGE IS UN
• Applied Natural Language Processing (I 256)
David Bamman
• Oriented towards interdisciplinary students I School Professor
• Focus on using existing tools
• Natural Language Processing (I 259)
• In-depth on algorithms and implementation

--- Page 28 ---
W N
ORD ET
• The big kahuna of lexical resources (>100,000 nouns)
• Nothing else was free and online when it started
• Main concept: the synset
• Instead of alphabetical ordering, organized by semantic
properties and relationships
• Includes useful(?) functions
• Similarity
• Stemming
• Created by psychologists & linguists
• Originally English only, now many languages
• http://compling.hss.ntu.edu.sg/omw/

--- Page 29 ---
E (I ): E W N
XERCISE NDIVIDUAL XPLORE ORD ET
Now try this one: visuwords.com
visuwords.com
Find synonyms, meronyms, antonyms, etc
Examples: “bank”, “money”, “joy”

--- Page 30 ---
W W W N
HAT IS RONG WITH ORD ET
Missing words like “kahuna”
Missing links
Missing many noun-noun compounds
Uneven hierarchy levels
General; often you need specific vocabularies

--- Page 31 ---
C V
ONTROLLED OCABULARIES IN
I A
NFORMATION RCHITECTURE

--- Page 32 ---
What to take from this reading
Ch 10:
Thesauri, Controlled Vocabularies, & Metadata
• Another definition of Metadata
• Synonym rings, authority files, and a
different meaning of “thesauri”
• How these are used in websites
• (Only assigned to read through
Thesaurus Standards, but feel free to read
their take on Poly Hierarchy & Faceted
Metadata)

--- Page 33 ---
Types of Controlled Vocabularies
Rosenfeld et al., Information Architecture, O’Reilly,
2015

--- Page 34 ---
F L 3: C
ROM ECTURE ONTROLLED
V :
OCABULARY
T R N N P
RY TO EDUCE THE OISY AME ROBLEM
“A fixed or closed set of description terms in
some domain with precise definitions that is used
instead of the vocabulary that people would
otherwise use.”
-- TDO

--- Page 35 ---
F L 3:
ROM ECTURE
T C V
YPES OF ONTROLLED OCABULARIES
Dictionaries
Today’s reading /
•
lecture adds a
Authoritative names
• few more related
Identifier Standards ideas
•
Subject heading lists
•
Classification systems
•
40

--- Page 36 ---
“Synonym Ring”
Correspondence
to lexical
relations?
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 37 ---
Lexical Relations, Pairwise on Arrows
orthographic
variant hypernym
sibling sibling
hyponym
orthographic
variant
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 38 ---
Authority Files
Indicates the preferred
term(s) among a list of
related terms
Example: correct
spelling of a
misspelled brand
name
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 39 ---
Authority Files
Indicates the preferred
term(s) among a list of
related terms
Example: scientific (or
generic) name of a
drug
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 40 ---
Authority Files
Indicates the preferred
term(s) among a list of
related terms
Example: scientific (or
generic) name of a
drug
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 41 ---
T
HESAURUS
• Sense 1: the familiar book we use to find
alternative words when writing
• Sense 2: used for vocabulary management in
information systems;
• Synonym management for an information
system
• Many kinds of relations (like ontologies have)

--- Page 42 ---
Semantic Relationships in a
Thesaurus
Correspondence
to lexical
relations?
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 43 ---
Semantic Relationships in a
Thesaurus
hypernym
synonym
sibling / meronym /
hyponym
other relationship
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 44 ---
Semantic Relationships in a
Thesaurus
Correspondence
to ontologies?
Rosenfeld et al., Information Architecture, O’Reilly, 2015

--- Page 45 ---
T (F V
HESAURUS OR OCABULARY
M )
ANAGEMENT
• A THESAURUS in indexing is a tool for finding the "right" or
"good" terms of a controlled vocabulary
• A collection of vocabulary terms annotated with lexical
relationships:
• Preferred (UF "used for")
• Broader (BT "broader term")
• Narrower (NT "narrower term")
• Related (RT "related term" or "see also")

--- Page 46 ---
N L
EXT ECTURE
• Semantic Similarity
• Taxonomic vs Thematic Relations


================================================================================
FILE: 13_semantic_similarity.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 12: Semantic Similarity

--- Page 2 ---
Today’s Outline
Lexical Relations Review
Taxonomic vs Thematic
Semantic Similarity
Card Sorting / Lumping vs Splitting
Automating Semantic Similarity

--- Page 3 ---
DIFFERENT LEXICAL FORM SAME LEXICAL FORM
hypernyms hypernyms
(superordinate) (superordinate)
---
fundamental measure
synonyms polysemes
time period (menstrual)
period
(amount of time)
sibling terms homographs
temperature, mass, length
period (punctuation)
hyponyms
hyponyms
week, past, eve… (subordinate)
(subordinate)
trial period, test period

--- Page 4 ---
R : A
ELATION NTONYMY
• Senses that are opposites with respect to only one feature
• Otherwise, they are very similar!
dark/light short/long fast/slow rise/fall
hot/cold up/down in/out
• More formally: antonyms can
• define a binary opposition or be at opposite ends of a scale
• long/short, fast/slow
• Be reversives:
• rise/fall, up/down
Slide from Jurafsky

--- Page 5 ---
C ( )
ONNOTATION SENTIMENT
• Words have affective meanings
• Positive connotations (happy)
• Negative connotations (sad)
• Connotations can be subtle:
• Positive connotation: copy, replica, reproduction
• Negative connotation: fake, knockoff, forgery
• Evaluation (sentiment!)
• Positive evaluation (great, love)
• Negative evaluation (terrible, hate)
Slide from Jurafsky

--- Page 6 ---
C
ONNOTATION
• Some words seem to vary along 3 affective dimensions:
• valence: the pleasantness of the stimulus
• arousal: the intensity of emotion provoked by the stimulus
• dominance: the degree of control exerted by the stimulus
Word Score Word Score
Valence love 1.000 toxic 0.008
happy 1.000 nightmare 0.005
Arousal elated 0.960 mellow 0.069
frenzy 0.965 napping 0.046
Dominance powerful 0.991 weak 0.045
leadership 0.983 empty 0.081
Osgood et al. 1957; Values from NRC VAD Lexicon (Mohammad 2018); Slide from Jurafsky

--- Page 7 ---
T T
AXONOMIC VS HEMATIC

--- Page 8 ---
T V T A
AXONOMIC S HEMATIC SSOCIATIONS
• Taxonomic: Hyponym/Hypernym, IS-A
• Eggs are a food, Toast is a food
• Toaster is a cooking implement; so is a Pan
• Thematic: Concepts linked by some other relationship
(used-for, made-from, etc.)
• Eggs are cooked in a Pan
• Bread is cooked in a Toaster
Icons by Gemini

--- Page 9 ---
Taxonomic Vs Thematic Associations
Evidence for Different Specialization in
Regions in the Brain
Intracranial EEG readings suggest:
ATL specialized for taxonomic relations
IPL specialized for thematic relations
Close coordination is also suggested
between the two regions.
Anterior Temporal Lobe (blue)
Inferior Parietal Lobule (red)
Thye et al, Intracranial EEG evidence of functional specialization for taxonomic and thematic relations, Cortex 140, 2021

--- Page 10 ---
A :
PPLICATION
T T F M
AXONOMIC VS HEMATIC ACETED ETADATA
• Fine arts collection:
• Taxonomic: Weapons > Swords; Occupation > Soldier, etc
• Thematic: Military Conflict (combines relevant subfacets)
• Sporting goods vendor:
• Taxonomic: Shoes > Boots > Ski Boots; Outerwear > Jackets
> Ski Jackets, etc
• Thematic: Sport > Skiiing (combines relevant subfacets)

--- Page 11 ---
E : A W -A S :
XAMPLE ORD SSOCIATION TUDY
T V T A
AXONOMIC S HEMATIC SSOCIATIONS
• Word pairs shown to crowd workers
• Rate the similarity from 1 -- 7
Landrigan & Mirman, Taxonomic and Thematic Relatedness Ratings for 659 Word Pairs, JOPD 4, 2016

--- Page 12 ---
A W -A S :
ORD SSOCIATION TUDY
T V T A
AXONOMIC S HEMATIC SSOCIATIONS
Taxonomic Instructions: Two words are similar if they look alike or belong to the same
category.
• For example, DOTS and STRIPES are similar (both are types of patterns or designs).
• However, SHIRT and STRIPES would not be similar.
Thematic Instructions: Two words are connected or related if they occur in the same
time or place; however, this does not mean they will share similar physical features.
• For example, HELMET and MOTORCYCLE are related
• But CHRISTMAS-TREES and PALM-TREES are not related.
Landrigan & Mirman, Taxonomic and Thematic Relatedness Ratings for 659 Word Pairs, JOPD 4, 2016

--- Page 13 ---
Taxonomic Properties
High Taxonomic, Low Thematic
Word 1 Word 2
Breakfast Dinner
Helmet Crown
Salt Sugar
Landrigan & Mirman, Taxonomic and Thematic Relatedness Ratings for 659 Word Pairs, JOPD 4, 2016

--- Page 14 ---
Thematic Relations
Low Taxonomic, High Thematic
Word 1 Word 2
Helicopter Pilot
Floss Teeth
Pillow Head
Landrigan & Mirman, Taxonomic and Thematic Relatedness Ratings for 659 Word Pairs, JOPD 4, 2016

--- Page 15 ---
Both Taxonomic and Thematic
Ring, Bracelet:
High Taxonomic, High Thematic
Very close taxonomically
Word 1 Word 2
Thematic:
Ring Bracelet
In this case, close taxonomically mirrors
Shingle Brick
closeness thematically (similar usage)
Tape Staple (not the case for, e.g., tent & battery)
Landrigan & Mirman, Taxonomic and Thematic Relatedness Ratings for 659 Word Pairs, JOPD 4, 2016

--- Page 16 ---
Neither Taxonomic Nor Thematic Similarity
Low Taxonomic, Low Thematic
Word 1 Word 2
Portrait Report
Prisoner Pupil
Bird Lamb
Landrigan & Mirman, Taxonomic and Thematic Relatedness Ratings for 659 Word Pairs, JOPD 4, 2016

--- Page 17 ---
Let’s Do Some Card Sorting!
Groups of 3
10 students use this link: 10 students use this link:
https://study.kardsort.com/cardsort2 https://tinyurl.com/3chr6ncr

--- Page 18 ---
D L S ?
ID YOU UMP OR PLIT

--- Page 19 ---
Gary Brookins/Richmond Times-Dispatch via NYTimes March 2015

--- Page 20 ---
L S
UMPERS AND PLITTERS
“A lumper takes things that seem disparate and
combines them because they have something
similar. A splitter tends to take two things that
are lumped together and separate them into
smaller categories.”
--Seth Maislin, American Society of Indexers, quoted by David Weinberger, Everything Is Miscellaneous, pg. 71, Times Books, 2007

--- Page 21 ---
W C
E ONSTANTLY HAVE TO MAKE
L S D
UMP VS PLIT ECISIONS
How we organize our clothes
• A big pile in the closet
• Old vs new
• Organized the Marie Kondo way
• How we write up reports of a usability study?
• Lots of detail, broken out into tables?
• Overall impressions, generalizations?

--- Page 22 ---
T M C
HE ISCELLANEOUS ATEGORY
There is almost always an “other” or
“miscellaneous” category
Our organizing systems do not handle these well
David Weinberger, Everything Is Miscellaneous, Times Books, 2007

--- Page 23 ---
Strategy: Taxonomic Grouping,
Coarse-Grained (lumped)

--- Page 24 ---
Strategy: Taxonomic,
Fine-Grained (split)

--- Page 25 ---
Strategy: Thematic Grouping
Coarse Grained (Lumped)

--- Page 26 ---
S S
EMANTIC IMILARITY
What makes two words or concepts similar?

--- Page 27 ---
R : S
ELATION IMILARITY
• Also called "word association"
• Words can be related in many ways
• coffee, tea: similar
• coffee, cup: related, not similar

--- Page 28 ---
R A P
ESULTS OF SKING EOPLE
2 ( 0-10)
HOW SIMILAR WORDS ARE FROM
word1 word2 similarity
vanish disappear 9.8
behave obey 7.3
belief impression 5.95
muscle bone 3.65
modest flexible 0.98
hole agreement 0.3
SimLex-999 dataset (Hill et al., 2015)

--- Page 29 ---
S R
EMANTIC ELATIONS AND
S S
EMANTIC IMILARITY

--- Page 30 ---
Different relation types may affect
“similarity”
Which
weaving pairs are
material
most
Is-a-kind-of
created-by similar?
cotton
fabric
rayon
clothing used-for
made-from
wool
curtains

--- Page 31 ---
Different relation types may affect
“similarity”
plant fiber
Which pairs
material
are most
Is-a-kind-of
similar? Is-a-kind-of Is-a-kind-of
cotton
fabric
made-fromanimal fiber
furnishing
used-for
Is-a-kind-of
wool
Is-a-kind-of
is-part-of
curtains
sheep

--- Page 32 ---
T A I S
HESE SSOCIATIONS NFLUENCE IMILARITY
J
UDGMENTS
W M L S ?
HICH ARE THE OST AND EAST IMILAR
• Is fabric similar to material?
• Is fabric similar to cotton?
• Is fabric similar to curtains?
• Is fabric similar to sheep?
• Is fabric similar to building?
• Is fabric similar to dream?

--- Page 33 ---
C S V :
OMPUTING IMILARITY ALUES
W E
ORD MBEDDINGS
• Represent words as vectors (arrays) of numbers
• Compare the values of the vectors to determine
similarity
• The vectors try to capture the various types of
relationships
• A closer similarity should correspond to having more
shared features, weighted by category centrality.

--- Page 34 ---
Computing Word Similarity
with Word Embeddings
Which relation types are scored as most similar?
Computed with Floret vector embeddings from Spacy (an extension of FastText, which is an extension of Word2Vec)

--- Page 35 ---
Code for Word Embedding-based
Similarity
Computed with Floret vector embeddings from Spacy (an extension of FastText, which is an extension of Word2Vec)

--- Page 36 ---
Computing Word Similarity
with Word Embeddings
Which relation types are scored as most similar?

--- Page 37 ---
Computing Word Similarity
with Word Embeddings

--- Page 38 ---
Example: Vehicle
More Prototypical is closer to the center
There is a gradience in category membership

--- Page 39 ---
Computing Word Similarity
with Word Embeddings

--- Page 40 ---
C S V :
OMPUTING IMILARITY ALUES
S E
ENTENCE MBEDDINGS
• Words in isolation can have many shades of meaning.
• The surrounding context of the word in a sentence clarifies the meaning.
• Sentence embeddings try to capture this meaning.
• Represents N words with N vectors (arrays) of numbers
• Average the N vectors to create one sentence embedding vector
• Compare the values of the 2 vectors to determine similarity as before

--- Page 41 ---
Computing Similarity
with Sentence Embeddings
The similarity scores capture both lexical and conceptual similarity.
Computed with all-MiniLM-L6-v2, a small, distilled version of a larger Transformer model from Hugging Face.

--- Page 42 ---
Computing Similarity
with Sentence Embeddings

--- Page 43 ---
M N W
IDTERM EXT EEK
• Open Book and Notes
• No internet, no AI, no talking with others
• 3 Hours, but expected to take 2
• You choose the time that you start the 3 hours
• Oct 16 through 20th
• Study guide at the start of next week

--- Page 44 ---
N W
EXT EEK
• How embeddings are built
• Basics of automated classification
• Midterm Review


================================================================================
FILE: 14_word_embeddings.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 14: Vector Representation; Word Embeddings

--- Page 2 ---
Today’s Outline
Word Meaning as
Distributional Context
Vector Representations
Word Embeddings
Word2Vec
Midterm Guide

--- Page 3 ---
W D W E ?
HY ISCUSS ORD MBEDDINGS
They are a key building block of Large Language Models
They are how we represent word meaning todauy

--- Page 4 ---
Representing Word Meaning
WordNet is meant for linguistic representation
It does not represent word similarity / distance well
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 5 ---
Representing Word Meaning
Instead, we now represent meaning by word distributions

--- Page 6 ---
D W D U
EFINING ORDS BY ISTRIBUTION OF SE
• Firth (1957):
• “You shall know a word by the company it keeps”
• Wittgenstein (1953):
"The meaning of a word is its use in the language"
• Zellig Harris (1954):
• “If A and B have almost identical environments, we say that they are
synonyms.”

--- Page 7 ---
D
EFINING MEANING AS A POINT IN SPACE
BASED ON DISTRIBUTION
• Each word is assigned a vector
• Similar words are "nearby in semantic space"
• We build this space automatically by seeing which words are
nearby in text
• These allow meaning to be represented as a point in a multi-
dimensional space

--- Page 8 ---
How do we know how to fill in the blank?
People fill this in based on their knowledge of the
world and of lexical usage; they can predict the fill

--- Page 9 ---
“fill in the blank”:
good data for machine learning

--- Page 10 ---
We call the surrounding words context

--- Page 11 ---
R W C
EPRESENTING ORDS AS ONTEXT
V
ECTORS

--- Page 12 ---
Intuition: Words with Similar Context
Neighborhoods Have Similar Meaning
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 13 ---
The ‘one hot’ vector representation
Every word has its own
(arbitrary) position in an
array
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 14 ---
But this representation has some limitations
1. The vectors are very long
(the vocabulary is huge)
2. Novel words missing
3. The representation does not
show which words are
semantically similar
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 15 ---
Computing Word Embeddings with
Distributions Makes a Richer Representation
Think of the colors as showing complex nuance about
which words have appeared in the same context
These are real numbers instead of frequency counts
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 16 ---
I B V R
NTUITION EHIND ECTOR EPRESENTATION
• Say each word can be understood according to it similarity to 3 categories or dimensions
• A vector is a list of numbers that represents the usage fingerprint pr profile of the word.
• The Categories (Dimensions): Instead of "length" or "width," the dimensions represent how often a word
appears near other specific types of words.
• Dimension 1 (e.g., "Food"): A high number means the word is often found near "eat," "cook,"
"delicious," etc.
• Dimension 2 (e.g., “Vehicle"): A high number means the word is often found near ”drive," “move”,
“go”, etc.
• Dimension N (e.g., "Technology"): ...near "app," "data," "internet," etc.
• Example Profiles:
• ”Truck": [0.0 (Food, 0.8 (Vehicle), 0.2 (Tech)]
• "Apple": [0.8 (Food), 0.0 (Vehicle), 0.2 (Tech)]
• "Google": [0.1 (Food), 0.0 (Vehicle), 0.9 (Tech)]

--- Page 17 ---
Vector Review
https://www.youtube.com/watch?v=_YkIivLaVJs&t=34s

--- Page 18 ---
P W V 2D
LOTTING ORD ECTORS IN
• X-axis: a scale from Large (+X) to Slow (-X)
• Y-axis: a scale from Fast (+Y) to Slow (-Y)
• Now we can create 2D vectors for words:

--- Page 19 ---
Plotting on a 2D Graph
fast
large
small A vector is not just a point on a graph;
it's the path from the center to that
point, defined by its components.
slow X-axis: a scale from Large (+X) to Slow (-X)
Y-axis: a scale from Fast (+Y) to Slow (-Y)
(0,0) is the “average” word

--- Page 20 ---
E : M W 2D S
XERCISE AP ORDS INTO A PACE
• x-axis: (Physicality)
• A scale from Tangible (+X) to Abstract (-X)
• y-axis: (Purpose)
• A scale from Fun (+Y) to Usefulness (-Y)
• Plot these words:
• Hammer, Joke, Game, Stress, Math

--- Page 21 ---
W E N 2D
ORD MBEDDINGS ARE OT
• The typical vector size is 300 dimensions
• We use 2D vector images for the intuition

--- Page 22 ---
W
E DEFINE MEANING OF A WORD AS A VECTOR
• Called an "embedding" because it's embedded into an
abstract, multi-dimensional space
• Is now the standard way to represent meaning in NLP
• Every modern NLP algorithm uses embeddings as the
representation of word meaning

--- Page 23 ---
C :
OMPUTING WORD SIMILARITY
T D
HE OT PRODUCT
• The dot product between two vectors is a scalar:
• The dot product tends to be high when the two vectors have large
values in the same dimensions
• Dot product can thus be a useful similarity metric between
vectors, but the drawback has to do with relative word frequency

--- Page 24 ---
We usually use cosine similarity instead of the
dot product for word vectors
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 25 ---
C :
OMPUTING WORD SIMILARITY
T C M
HE OSINE EASURE
• Instead of the dot product, we often compute the cosine between
vectors to correct for (normalize) the different vector lengths

--- Page 26 ---
W U C S ?
HY SE OSINE IMILARITY
• Imagine each word vector is a searchlight beam coming out of the origin (0,0).
• The Direction (Cosine Similarity):
• The angle between two beams tells you how related the words are.
• If two searchlights are pointed in nearly the same direction (a small angle), the
words are highly similar, even if one is brighter than the other. → High Cosine
Similarity (close to 1.0).
• If the beams are pointed in completely different directions (a large angle, close to
90∘), the words are unrelated. → Low Cosine Similarity (close to 0).

--- Page 27 ---
W U C S ?
HY SE OSINE IMILARITY
• The Brightness/Length (Vector Magnitude/Euclidean Distance):
• The length of the vector is its magnitude. In many word embedding models, the length is
not as important as the direction.
• Consider a common word like "run" and a rare word like "sprint". They mean almost the
same thing.
• If we used the straight-line Euclidean distance, "run" (long/bright vector) might be
considered very far from "sprint" (short/dim vector) just because of the difference in
length.
• But, because the words are so close in meaning, the two searchlights are pointed in
almost the exact same direction (small angle), so the Cosine Similarity is high, which
correctly captures their relationship.

--- Page 28 ---
This shows the cosine
similarity between pairs of
selected words based on
vectors trained from 100
billions words of news
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 29 ---
Distribution-based Word Similarity
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 30 ---
W 2V E
ORD EC MBEDDINGS

--- Page 31 ---
W 2
ORD VEC
• Popular embedding method
• Very fast to train
• Code available on the web
• Idea: predict if a word will appear near others (based on
word distributions)

--- Page 32 ---
W 2
ORD VEC
• Instead of counting how often each word w occurs near “ocean"
• Train a classifier on a binary prediction task:
• Is w likely to show up near “ocean"?
• We don’t actually care about this task
• But we'll take the learned classifier weights as the word embeddings
• Big idea: self-supervision:
• A word c that occurs near ocean in the corpus counts as the gold "correct
answer" for supervised learning
• No need for human labels
• Bengio et al. (2003); Collobert et al. (2011)

--- Page 33 ---
Word2Vec Algorithm (2 versions)
Skip-gram: Trying to predict the closest
CBOW: Trying to predict a middle word in a
2-4 neighbors of a specific word
window of 3-5 words
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 34 ---
W E V
ORD MBEDDINGS IDEO
https://www.youtube.com/watch?v=iErmK_sJtag

--- Page 35 ---
Code for Word Embedding-based Similarity
Computed with Floret vector embeddings from Spacy (an extension of FastText, which is an extension of Word2Vec)

--- Page 36 ---
Computing Word Similarity
with Word Embeddings
Which relation types are scored as most similar?

--- Page 37 ---
A
NALOGICAL RELATIONS
• The classic parallelogram model of analogical reasoning
(Rumelhart and Abrahamson
1973)
• We compute the analogy using the vector representation
• To solve: "apple is to tree as grape is to _____"
(tree – apple) + grape = vine
https://web.stanford.edu/~jurafsky/slp3/slides/vector25aug.pdf

--- Page 38 ---
A
NALOGICAL RELATIONS
• The classic parallelogram model of analogical reasoning
(Rumelhart and Abrahamson
1973)
• We compute the analogy using the vector representation
• To solve: "apple is to tree as grape is to _____"
(tree – apple) + grape = vine
https://web.stanford.edu/~jurafsky/slp3/slides/vector25aug.pdf

--- Page 39 ---
Try it with some code
Computed with Floret vector embeddings from Spacy (an extension of FastText, which is an extension of Word2Vec)

--- Page 40 ---
Code for computing
analogies with floret
embeddings

--- Page 41 ---
E : T U
XERCISE HINK P SOME
A
NALOGICAL RELATIONS
• To solve: "apple is to tree as grape is to _____"
(tree – apple) + grape = vine
https://web.stanford.edu/~jurafsky/slp3/slides/vector25aug.pdf

--- Page 42 ---
E E R C B
ARLY MBEDDINGS EFLECTED ULTURAL IAS
• Ask “Paris : France :: Tokyo : x”
• x = Japan
• Ask “father : doctor :: mother : x”
• x = nurse
• Ask “man : computer programmer :: woman : x”
• x = homemaker
Algorithms that use embeddings as part of e.g., hiring searches
for programmers, might lead to bias in hiring
However, this problem has been recognized and is usually fixed
Tolga et al.. "Man is to computer programmer as woman is to homemaker? debiasing word embeddings." In NeurIPS, pp. 4349-4357. 2016.

--- Page 43 ---
The model I used has been de-biased

--- Page 44 ---
E
MBEDDINGS AS A WINDOW ONTO HISTORICAL
SEMANTICS
Train embeddings on different decades of historical text to see meanings shift
~30 million books, 1850-1990, Google Books data
William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. Proceedings of ACL.

--- Page 45 ---
Extending Embeddings to Entire Document
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 46 ---
C S V :
OMPUTING IMILARITY ALUES
S E
ENTENCE MBEDDINGS
• Words in isolation can have many shades of meaning.
• The surrounding context of the word in a sentence clarifies the meaning.
• Sentence embeddings try to capture this meaning.
• Represents N words with N vectors (arrays) of numbers
• Average the N vectors to create one sentence embedding vector
• Compare the values of the 2 vectors to determine similarity as before

--- Page 47 ---
Computing Similarity
with Sentence Embeddings
The similarity scores capture both lexical and conceptual similarity.
Computed with all-MiniLM-L6-v2, a small, distilled version of a larger Transformer model from Hugging Face.

--- Page 48 ---
S : D
UMMARY ISTRIBUTED
R
EPRESENTATION
• Vector representation encodes information about the
distribution of contexts a word appears in
• Words that appear in similar contexts have similar
representations (and similar meanings, by the distributional
hypothesis).
• Word embeddings are the building blocks for modern NLP
algorithms including Large Language Models

--- Page 49 ---
M S G
IDTERM TUDY UIDE

--- Page 50 ---
C
OVERAGE
• Topics from week 1- 7
• Open notes, open class readings; CLOSED INTERNET
except as described in the exam.
• Topics will either:
• A topic that appeared both in class and in a reading
• A variation on an exercise we have done in class or
homework
• Be able to apply the knowledge you have learned


================================================================================
FILE: 15_automatic_classification.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 15: Text Classification

--- Page 2 ---
Today’s Outline
Automating Classification
Classification Process
Evaluation
Naïve Bayes Classification

--- Page 3 ---
W T C F ?
HAT IS EXT LASSIFICATION OR

--- Page 4 ---
Topic Classification
The CSE447 Web: © 1993-2025, Department of Computer Science and Engineering, University of Washington.

--- Page 5 ---
Sentiment Analysis
The CSE447 Web: © 1993-2025, Department of Computer Science and Engineering, University of Washington.

--- Page 6 ---
Spam Classification
The CSE447 Web: © 1993-2025, Department of Computer Science and Engineering, University of Washington.

--- Page 7 ---
Fact Verification
The CSE447 Web: © 1993-2025, Department of Computer Science and Engineering, University of Washington.

--- Page 8 ---
L ’ S S A
ET S DO OME ENTIMENT NALYSIS
Are these positive or negative movie reviews?
Still, this flick is fun and host to some truly excellent sequences.
Their computer-animated faces are very expressive.
It’s not life affirming -- it’s vulgar and mean, but I liked it.
You walk out of The Good Girl with mixed emotions – disapproval of
Justine with a tinge of understanding for her actions.

--- Page 9 ---
C S N
OPING WITH ENTIMENT UANCE
• Most sentiment prediction systems work just by looking at words
in isolation, giving positive points for positive words and negative
points for negative words and then summing up these points.
• Instead, we can build up a representation of whole sentences
based on the sentence structure.
• We compute the sentiment based on how words compose the
meaning of longer phrases.
Socher et al, EMNLP 2013, https://nlp.stanford.edu/sentiment/treebank.html

--- Page 10 ---
Socher et al, EMNLP 2013, https://nlp.stanford.edu/sentiment/treebank.html

--- Page 11 ---
Socher et al, EMNLP 2013, https://nlp.stanford.edu/sentiment/treebank.html

--- Page 12 ---
Socher et al, EMNLP 2013, https://nlp.stanford.edu/sentiment/treebank.html

--- Page 13 ---
Socher et al, EMNLP 2013, https://nlp.stanford.edu/sentiment/treebank.html

--- Page 14 ---
Why is Sentiment Analysis Difficult?
• Sentiment is a measure of a speaker’s private state, which is unobservable.
Sometimes words are a good indicator of sentiment (love, amazing, hate,
•
terrible); many times it requires deep world + contextual knowledge
“Valentine’s Day is being marketed as a Date Movie. I think it’s more of
a First-Date Movie. If your date likes it, do not date that person again.
And if you like it, there may not be a second date.”
RogerEbert,Valentine’sDay

--- Page 15 ---
W ?
HAT IS AUTOMATIC CLASSIFICATION
• Uses a dataset with labeled classes to learn which items belong to which
classes
• Learns f(x) = y, where x is features of an item and y is the class it belongs
to
• Ex: x = movie review, y = +/-

--- Page 16 ---
C
LASSIFICATION
A mapping h from input data x (drawn from
instance space 𝓧) to a label (or labels) y
from some enumerable output space 𝒴
𝓧 = set of all documents
𝒴 = {english, mandarin, greek, …}
x = a single document
y = ancient greek

--- Page 17 ---
T
EXT CATEGORIZATION PROBLEMS
task 𝓧 𝒴
language ID text {english, mandarin, greek, …}
spam classification email {spam, not spam}
authorship attribution text {jk rowling, james joyce, …}
genre classification novel {detective, romance, gothic, …}
sentiment analysis text {positive, negative, neutral, mixed}

--- Page 18 ---
C
LASSIFICATION
h(x) = y
h(μῆνιν ἄειδε θεὰ) = ancient grc

--- Page 19 ---
C
LASSIFICATION
Let h(x) be the “true” mapping.
We never know it. How do we
find the best ĥ(x) to
approximate it?
One option: rule based
if x has characters in
unicode point range 0370-03FF:
ĥ(x) = greek

--- Page 20 ---
C
LASSIFICATION
Supervised learning
Given training data in the form
of <x, y> pairs, learn ĥ(x)

--- Page 21 ---
P
ROCEDURE
collect
define represent train
training
categories data model
data
interpret evaluate predict

--- Page 22 ---
define
☞categorization
categories
Crucial step, since everything you learn to predict is only predictable
•
with respect to these categories.
Your categorization may reflect institutional, individual, or cultural
•
categories is always biased.
Important thing is to be aware of the source of that bias and its
•
consequences downstream

--- Page 23 ---
represent
☞resource description
data
Decide on what descriptions of the resource you want the algorithm to
•
have access to.
You’re critically not deciding what’s important at this stage (the
•
algorithms often determine this), but only rather what information is
allowed to be learned to be important or not

--- Page 24 ---
author: foer TRUE
author: austen FALSE
pub year 2016
height (inches) 9.2
weight (pounds) 2
contain: the TRUE
contains: zombies FALSE
amazon rank @1
159
month

--- Page 25 ---
collect
training
data
author=foer author=austen pubdate “zombies" fiction
TRUE FALSE 2016 FALSE TRUE
TRUE TRUE 1816 FALSE TRUE
FALSE FALSE 2016 FALSE FALSE
FALSE FALSE 2011 TRUE TRUE

--- Page 26 ---
collect
training ☞resource selection
data
• Resources paired with their categories, as a result of:
human classification
•
being found nature
•

--- Page 27 ---
Image Label
E :
XERCISE
Yes
• You want to train a model to predict whether
there is a dog in an Instagram image.
• 1. How would you collect data using code?
The dataset should include (Image, Label)
pairs.
No
• 2. If you could incorporate human-labeling,
how would your data collection process
change?

--- Page 28 ---
D
ATA COLLECTION
• We can collect data:
• (1) automatically via existing sources (e.g. via a web scraper + hashtags)
• (2) with the aid of human labeling
• Sometimes we can combine these two approaches, e.g. having humans confirm or correct
automatically-collected examples.
• How much data to collect depends on your classifier! Range from dozens to millions of examples.

--- Page 29 ---
U
SES
Computational classification serves two primary uses:
Prediction: automatically assign categories to new data points
Insight: understanding what aspects of resource description
are most informative for determining category membership

--- Page 30 ---
P
REDICTION
Replacement for expensive human
•
classification, especially for
repetitive tasks (e.g., mail sorting)
Like all classifications, provide
•
structure to support other
interactions (e.g., dewey decimal)

--- Page 31 ---
I
NSIGHT
• Understanding the strongest indicators for category membership.
For human classification, the principles for defining categories
•
(enumeration, properties, similarity, family resemblance, etc.) are
embodied in the classifications that use these principles
• Can lead to challenging the initial categorization system.

--- Page 32 ---
T
HERE ARE MANY METHODS IN
train
model
/
MACHINE LEARNING DATA SCIENCE
TO PERFORM CLASSIFICATION
•
Decision trees (e.g., random forests)
•
Probabilistic models (e.g., Naive Bayes)
•
Nearest neighbor similarity (KNN)
•
• Neural models (RNNs, LSTMs, Transformers, LLMs)
• All define a mapping from some input representation x to a category label y

--- Page 33 ---
predict ☞classification
Use that trained model to make predictions about the category
•
membership of new data points.

--- Page 34 ---
evaluate
Assess the accuracy of the trained model by comparing the predictions
•
it makes to some notion of “truth” for those same data points
Cross-validation: train a model on some fraction of the data, and
•
evaluate its performance on the remaining data.

--- Page 35 ---
interpret
• Understand what the model is learning about the data
Some methods are more amenable to interpretation than others (very
•
much method-dependent)

--- Page 36 ---
C
LASSIFICATION
Supervised learning
Given training data in the form
of <x, y> pairs, learn ĥ(x)
x y
loved it! positive
terrible movie negative
not too shabby positive

--- Page 37 ---
R S A
EPRESENTATION FOR ENTIMENT NALYSIS
• Only positive/negative words in sentiment dictionaries (e.g., MPQA)
• Only words in isolation (bag of words)
Conjunctions of words (sequential, skip ngrams, other non-linear
•
combinations)
• Higher-order linguistic structure (e.g., syntax)

--- Page 38 ---
Bag of Words Representation
Jurafsky & Martin slides

--- Page 39 ---
Apocalypse
B North
AG OF WORDS now
the 1 1
of 0 0
hate 0 9
Representation of text only as the
genius 1 0
counts of words that it contains
bravest 1 0
stupid 0 1
like 0 1
…

--- Page 40 ---
O T F
THER YPES OF EATURES
• Syntactic features
• - Part-of-speech information
• - Subject/Verb/Object information
• Length of Document
• Punctuation used

--- Page 41 ---
E
VALUATION
• For all supervised problems, it’s important to understand how well your model is
performing
What we try to estimate is how well you will perform in the future, on new data also
•
drawn from 𝓧
• Trouble arises when the training data <x, y> you have does not characterize the full
instance space.
• n is small
• sampling bias in the selection of <x, y>
• x is dependent on time
y is dependent on time (concept drift)
•

--- Page 42 ---
𝓧
INSTANCESPACE
DATA

--- Page 43 ---
𝓧
instancespace
train dev test

--- Page 44 ---
E
XPERIMENT DESIGN
training development testing
size 80% 10% 10%
evaluation; never
purpose training models model selection look at it until the
very end

--- Page 45 ---
M
AJORITY CLASS BASELINE
Pick the label that occurs the most frequently in the training data. (Don’t
•
count the test data!)
• Predict that label for every data point in the test data.

--- Page 46 ---
C M : N B
LASSIFICATION ODEL AIVE AYES
• Simple model, but works well in a lot of instances!
• Relies on the probability of a certain class given features.
• Based on probability and Bayes theorem

--- Page 47 ---
P C
ROBABILITY ONCEPTS
• Individual event’s probability: P(A)
P(coin=heads) = 0.5
• Conditional probability: P(A|B)
P(coin1=heads|coin2=tails)=0.5 P(I take the bus to work | it is raining) = 0.9
= P(coin1=heads), meaning these I’m more likely to take the bus
events are independent when it rains, so these events
are not independent

--- Page 48 ---
B T I
AYES HEOREM NTRODUCTION
• How can we calculate the
probability of Event A given that we
have observed Event B?
𝑃 𝐵 𝐴 ∗ 𝑃 𝐴
𝑃 𝐴 𝐵 =
𝑃 𝐵
• A lot of the time, we don’t have
direct access to P(A|B), so we can
use Bayes theorem to calculate
this!

--- Page 49 ---
B T
AYES HEOREM
It was foggy given
It will rain given that it has rained
It will rain
that it is foggy
• You observe that the sky is foggy
this morning. In Berkeley, it rains
𝑃 𝐵 𝐴 ∗ 𝑃 𝐴
𝑃 𝐴 𝐵 =
5% of all days. You know that rainy 𝑃 𝐵
days start off with foggy mornings
It is foggy
20% of the time. You also know
0.20 ∗ 0.05
that it is foggy 25% of the time.
𝑃 𝐴 𝐵 =
0.25
What is the probability that it will
rain today given that it is foggy? 𝑃 𝐴 𝐵 = 0.04 = 4.0%
Event A: It will rain
Event B: You observe that it is foggy

--- Page 50 ---
N B C
AÏVE AYES LASSIFIER
• Assumes we have a dataset with labeled features
• Generative classifier – assumes data is created by sampling class and
then generating text of document.
• Leverages Bayes Rule to calculate probability of a specific class given
the features present in the example.

--- Page 51 ---
N B C
AÏVE AYES LASSIFIER
Prior probability of each class
Probability of the
features given the
class
𝑃(𝑋|𝑦)∗ 𝑃 𝑦
𝑦 =𝑐𝑙𝑎𝑠𝑠 (+,−) 𝑃(𝑦|𝑋)=
𝑃 𝑋
Prior probability of the features
𝑋 =(𝑥 ,𝑥 ,…,𝑥 )
1 2 𝑛
We calculate the probability of a
class (+/-)
For each document we classify, X
given the features observed in the
is our featurized representation! example.
𝑥 ="good"
1
“Good movie” →
𝑥 ="𝑚𝑜𝑣𝑖𝑒"
2

--- Page 52 ---
N B C
AÏVE AYES LASSIFIER
𝑃(𝑋|𝑦)∗ 𝑃 𝑦
𝑃(𝑦|𝑋)=
𝑃 𝑋
𝑃 𝑥 𝑦 ∗𝑃 𝑥 𝑦 ∗⋯∗𝑃(𝑥 |𝑦)∗ 𝑃 𝑦
𝑦 =𝑐𝑙𝑎𝑠𝑠 (+,−) 𝑃(𝑦|𝑋)= 1 2 𝑛
𝑃 𝑥 ∗𝑃 𝑥 ∗⋯∗𝑃(𝑥 )
1 2 𝑛
𝑋 =(𝑥 ,𝑥 ,…,𝑥 )
1 2 𝑛
In practice, we drop the
𝑃 𝑦 𝑋 ∝𝑃 𝑥 𝑦 ∗𝑃 𝑥 𝑦 ∗⋯∗𝑃(𝑥 |𝑦)∗ 𝑃 𝑦
1 2 𝑛 denominator!
𝑥 ="good"
1
𝑛
𝑥 2 ="𝑚𝑜𝑣𝑖𝑒" 𝑃 𝑦 𝑋  𝑃 𝑦 ෑ𝑃 𝑥 𝑖 𝑦)
𝑖=1
𝑛
𝑦 =𝑎𝑟𝑔𝑚𝑎𝑥 𝑃 𝑦 ෑ𝑃 𝑥 𝑦)
𝑦 𝑖
𝑖=1 Want to find the most likely class
https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c?gi=1784c8032896

--- Page 53 ---
H N B C ?
OW DO WE TRAIN A AÏVE AYES LASSIFIER
• We can estimate these probabilities
P(“good”|+) = # 𝑜𝑓𝑡𝑖𝑚𝑒𝑠 “𝑔𝑜𝑜𝑑” appears in+docs
via our dataset!
# 𝑜𝑓𝑤𝑜𝑟𝑑𝑠 𝑖𝑛 𝑎𝑙𝑙+𝑑𝑜𝑐𝑠
• Let’s revisit sentiment analysis with a
215
= 0.39
550
standard bag-of-words featurization.
P(“good”|-) = # 𝑜𝑓𝑡𝑖𝑚𝑒𝑠 “𝑔𝑜𝑜𝑑” appears in−docs
For our movie review dataset, 350 reviews
# 𝑜𝑓𝑤𝑜𝑟𝑑𝑠 𝑖𝑛 𝑎𝑙𝑙−𝑑𝑜𝑐𝑠
are + and 650 reviews are -.
P(+) = 350 / 1000 = 0.35 𝑛 75
= 0.10
𝑦 = 𝑎𝑟𝑔𝑚𝑎𝑥 𝑃 𝑦 ෑ𝑃 𝑥 𝑦)
𝑦 𝑖 750
P(-) = 650 / 1000 = 0.65
𝑖=1

--- Page 54 ---
P C N B
REDICTING A LASS WITH AÏVE AYES
𝑛
𝑦 =𝑎𝑟𝑔𝑚𝑎𝑥 𝑃 𝑦 ෑ𝑃 𝑥 𝑦)
𝑦 𝑖
𝑖=1
y = + y = -
𝑛
𝑛
𝑃 𝑦 = + ෑ𝑃 𝑥 𝑦 = +)
𝑖
𝑃 𝑦 = − ෑ𝑃 𝑥 𝑦 = −)
𝑖=1 𝑖
Good movie
𝑖=1
0.35∗0.39 ∗0.3 0.65 ∗0.10 ∗0.3
0.041 0.020
• Our Naïve Bayes Classifier would predict this
document belongs to the + class

--- Page 55 ---
L N
IMITATIONS OF AÏVE BAYES
• Independence assumption – assumes each feature x is independent
of all other features.
• This is not always true!
• Likelihood of a document containing “good” increases once we’ve
observed this word
• Sparse data problem – what if the word “fantastic” never appears in
our training dataset?

--- Page 56 ---
M I
IDTERM NSTRUCTIONS
D M 8 ; 5 !
UE ONDAY AT PM START NO LATER THAN PM
• This is an open-assigned-readings, open-note exam. However, you may not talk to other people about it, either in person
or online. Since students are taking the exam a different times, please do not discuss it with others after you complete it
until the instructors say it is ok to do so.
• You will have a maximum of 3 hours from starting to complete and submit this midterm. We suggest you type your
answer for each question into a word processor and then copy-and-paste the answer into the quiz question field.
• Use of any kind of AI is not permitted. This includes grammar checkers. Please turn off any automated suggestions
before you start, if you use a word processor.
• In case of difficulties with bCourses, please keep a copy of your answers and, should you not be able to submit online,
send this copy to the instructional team in an email time-stamped within 3 hours of starting. Note that the due-date (8pm
on Monday) does not take your starting time into account, so if you start after 5pm Sunday you won't have the full 3
hours.
• If you have any questions or run into any issues, please email the instructional team. As you may start the midterm at any
time in the 3-hour period, we may not be continuously available to respond to your messages. Therefore, if you have
started, do not wait for a response but continue working on the exam so you can complete it in the allotted time


================================================================================
FILE: 16_image_classification.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 16: Image Classification

--- Page 2 ---
Today’s Outline
Image Classification
Data Creation & Augmentation
Image Convolutions
Convolutional Neural Nets

--- Page 3 ---
I P A
MAGE ROCESSING LGORITHMS
• Today: Image recognition brief intro, CNNs
• Later: Transformer-based image models
Slides modified from: Kraus, U Hawai’I, Manoa, Snavely CS 5670 Cornell; CS 4782 Cornell

--- Page 4 ---
I C
MAGE LASSIFICATION
Image classification is the task of taking an input image and outputting a
class or a probability of classes that best describes the image
Slide4

--- Page 5 ---
Image Classification Application:
Autonomous Driving

--- Page 6 ---
I A C :
MAGE NALYSIS HALLENGE
VARIABLE VIEWPOINT
Michelangelo 1475-1564

--- Page 7 ---
C :
HALLENGE
VARIABLE ILLUMINATION DIFFERENCES
image credit: J. Koenderink

--- Page 8 ---
C :
HALLENGE DEFORMATION

--- Page 9 ---
C :
HALLENGE
O
CCLUSION
Magritte, 1957

--- Page 10 ---
C :
HALLENGE BACKGROUND CLUTTER
Kilmeny Niland. 1995

--- Page 11 ---
Challenge: Intra-Class Variations
Svetlana Lazebnik

--- Page 12 ---
D G & A
ATA ATHERING UGMENTATION

--- Page 13 ---
I C D
MAGE ATEGORY ATASET
CIFAR-10 Dataset (https://www.cs.toronto.edu/~kriz/cifar.html)
•Consists of 60,000 32x32 color images in 10 classes, with 6,000 images
per class. There are 50,000 training images and 10,000 test images.
Slide8

--- Page 14 ---
I N : A D G B
MAGE ET ATA ATHERING REAKTHROUGH
> 14 M I 20,000 C !
MAGES AND ATEGORIES
https://www.image-net.org/about.php

--- Page 15 ---
I N A W N
MAGE ET ND ORD ET
WordNet inspired the creation of ImageNet and provides its structure
Deng, et al. "ImageNet: A large-scale hierarchical image database." IEEE CVPR 2009.

--- Page 16 ---
I N :
MAGE ET
A
BREAKTHROUGH IN DATA COLLECTION
“We went to the Internet, the biggest treasure trove of pictures that humans have
ever created. We downloaded nearly a billion images and used crowdsourcing
technology like the Amazon Mechanical Turk platform to help us to label these
images. At its peak, ImageNet was one of the biggest employers of the Amazon
Mechanical Turk workers: together, almost 50,000 workers from 167 countries
around the world helped us to clean, sort and label nearly a billion candidate
images. That was how much effort it took to capture even a fraction of the imagery
a child's mind takes in in the early developmental years”. – Fei Fei Li
https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures/transcript

--- Page 17 ---
Image Data Gathering Tasks
Single object, plain background
Multiple objects
Choose a more general category when uncertain
https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures/transcript

--- Page 18 ---
Image Data Gathering Task: Data
Augmentation
https://www.ibm.com/think/topics/data-augmentation

--- Page 19 ---
Image Data Gathering Task: Data
Augmentation
https://www.ibm.com/think/topics/data-augmentation

--- Page 20 ---
A : D G I R
PPLICATION ATA ATHERING VIA MAGE ECOGNITION
Got millions of Google street view images, and classified the makes and models of cars
https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures/transcript

--- Page 21 ---
A : D G I R
PPLICATION ATA ATHERING VIA MAGE ECOGNITION
Then showed the expected correlation between car model and average income of a region
https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures/transcript

--- Page 22 ---
A : D G I R
PPLICATION ATA ATHERING VIA MAGE ECOGNITION
But also found unexpected correlation between car model and crime rate
https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures/transcript

--- Page 23 ---
E C L
NSURING THE ATEGORY ABELS
VERY A
ARE CCURATE
• “I ended up training on 500 validation images and then switched to the test
set of 1500 images. The labeling happened at a rate of about 1 per minute,
but this decreased over time. I only enjoyed the first ~200, and the rest I
only did #forscience. ….
• The labeling time distribution was strongly bimodal: Some images are
easily recognized, while some images (such as those of fine-grained
breeds of dogs, birds, or monkeys) can require multiple minutes of
concentrated effort. I became very good at identifying breeds of dogs.”
https://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet

--- Page 24 ---
E C L
NSURING THE ATEGORY ABELS
VERY A
ARE CCURATE
https://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet

--- Page 25 ---
E : T L I
XERCISE RY THE ABELING NTERFACE
S D S C C
CROLL OWN TO EE THE ATEGORY HOICES
https://cs.stanford.edu/people/karpathy/ilsvrc/

--- Page 26 ---
I R &
MAGE EPRESENTATION
C
ONVOLUTIONS

--- Page 27 ---
MNIST: A Classic Image Dataset
MNIST Dataset
•60,000 training examples
•10,000 test examples
•Rank of best Classifiers and Errors

--- Page 28 ---
How Computers Represent Images
What we see vs. What computers see

--- Page 29 ---
MNIST Dataset

--- Page 30 ---
Color Images

--- Page 31 ---
Color Images

--- Page 32 ---
C “ ”
ONVOLUTIONS TO AVERAGE
I
OVER AN MAGE
But What is a Convolution? 3Blue1Brown https://www.youtube.com/watch?v=KuXjwB4LzSA

--- Page 33 ---
C I
ONVOLUTIONS OVER AN MAGE
Create a grid of weights
In this case, 3x3 grid
Equally weighted with 1/9
Move the grid along the image
At each position in the image, multiply
the grid values by the value of the color
of the underlying pixel
Place that value in a new image
That value is the average color of the 9
pixels
But What is a Convolution? 3Blue1Brown https://www.youtube.com/watch?v=KuXjwB4LzSA

--- Page 34 ---
Convolutions Filters to
“Average” over an Image

--- Page 35 ---
Convolutions Filters to
“Average” over an Image

--- Page 36 ---
Convolutions Filters to
“Average” over an Image

--- Page 37 ---
But What is a Convolution? 3Blue1Brown https://www.youtube.com/watch?v=KuXjwB4LzSA

--- Page 38 ---
C F E D
ONVOLUTIONS OR DGE ETECTION
But What is a Convolution? 3Blue1Brown https://www.youtube.com/watch?v=KuXjwB4LzSA

--- Page 39 ---
C F E D
ONVOLUTIONS OR DGE ETECTION
Here, black pixels are 0 If we color the new image:
So left & middle column are 0 - negative as red,
(multiply by 0 = 0) - positive as blue
- zero as black
Here, white columns are 1 Then we start to see red edges
So we will get negative results in the on the left and blue enges on
right column (-.25, -.50, -.25) the right
But What is a Convolution? 3Blue1Brown https://www.youtube.com/watch?v=KuXjwB4LzSA

--- Page 40 ---
C F E D
ONVOLUTIONS OR DGE ETECTION
But What is a Convolution? 3Blue1Brown https://www.youtube.com/watch?v=KuXjwB4LzSA

--- Page 41 ---
Varying Convolution Filters
Different filters will produce
different feature maps for the
same input image
Input image

--- Page 42 ---
Exercise: Match the Convolutions with the
Output They Produce

--- Page 43 ---
C
ONVOLUTIONAL
N N (CNN )
EURAL ETWORKS S

--- Page 44 ---
P -D L I C
RE EEP EARNING MAGE LASSIFICATION
•Hand-Crafted Features
•Texture Features: Histogram based, Entropy, Haralick features (Co-
occurrence matrix), Gray-level run length metrics, Local Binary Pattern,
Fractal, etc.
•Morphological Features: Hu's moments, Shape features,
Granulometry, Bending Energy, Roundness ratio, etc

--- Page 45 ---
Standard vs Convolutional
Neural Nets
A ConvNet arranges its neurons in three dimensions
A standard 3-layer Neural Network.
(width, height, depth), as visualized in one of the layers.
Every layer of a ConvNet transforms the 3D input
volume to a 3D output volume of neuron activations.
https://cs231n.github.io/convolutional-networks/

--- Page 46 ---
Convolutional Neural Networks (CNNs)
Network Architecture
Convolutional Layer, Pooling Layer, Fully Connected Layer

--- Page 47 ---
CNN T
ERMINOLOGY
Convolution Operator
x =
Input Image(I) Filter(K)
•The 3×3 matrix (K) is called a ‘filter‘ or ‘kernel’ or ‘feature detector’
and the matrix formed by sliding the filter over the image and
computing the dot product is called the ‘Convolved Feature’ or
‘Activation Map’ or the ‘Feature Map‘.
Slide17

--- Page 48 ---
Make a convolution filter for each RGB
color channel

--- Page 49 ---
C N N (CNN )
ONVOLUTIONAL EURAL ETWORKS S
•In practice, a CNN learns the values of these filters on its
own during the training process
•Although we still need to specify parameters such as
number of filters, filter size, padding, and stride before
the training process
Slide23

--- Page 50 ---
CNN A L
CTIVATION AYERS
Activation Layer (ReLU)
•An additional operation called Rectified Linear Unit (ReLU) which
can be used after every Convolution operation
•Basically, ReLU is an element wise operation (applied per pixel) and
replaces all negative pixel values in the feature map by zero
•The purpose of ReLU is to introduce non-linearity to the network
Slide24

--- Page 51 ---
CNN A L
CTIVATION AYERS
Activation Layer (ReLU)
•Other nonlinear functions such as tanh or sigmoid can also be
used instead of ReLU, but ReLU has been found to perform better in
most situations.
Slide25

--- Page 52 ---
CNN M P
AX OOLING
Slide26

--- Page 53 ---
CNN P L
OOLING AYER
Pooling Layer
Pooling layer downsamples the volume spatially, independently in
each depth slice of the input
The most common downsampling operation is max, giving rise to max
pooling, here shown with a stride of 2
Slide26

--- Page 54 ---
CNN F C L
ULLY ONNECTED AYER FOR
C
LASSIFICATION
Fully Connected Layer
•Neurons in a fully connected layer have full connections to all
activations in the previous layer

--- Page 55 ---
CNN Output at Each Layer
Example: Input >> [ [ Conv >> ReLU ] * 2 >> Pool ] * 3 >> FC
The leftmost column stores the raw image pixels and the rightmost stores
the class scores (right). Since it's difficult to visualize 3D volumes, we show
image slices in rows. The last layer volume holds the scores for each class.

--- Page 56 ---
https://www.youtube.com/watch?v=YRhxdVk_sIs

--- Page 57 ---
https://distill.pub/2018/building-blocks/

--- Page 58 ---
Explore CNN Features
https://distill.pub/2018/building-blocks/

--- Page 59 ---
Advantages of CNNs for Image Classification

--- Page 60 ---
CNN S
S IN UMMARY
• CNNs are primarily designed to process and analyze visual data, such as images and videos.
• Key components: convolution layers, pooling layers, activation functions, normalization layers
• Advantages:
• Translational Invariance
• Parameter sharing
• Feature learning
• Can be trained with backprop
• Used for tasks such as segmentation, classification, object detection, etc.
• Achieved results around 86% on ImageNet
• Have been superseded today by Vision Transformers

--- Page 61 ---
To Learn More…
To learn about computer vision in detail:
Prof Hany Farid:


================================================================================
FILE: 17_grounded_coding.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 17: Grounded Coding

--- Page 2 ---
F P C : E C D
ROM REVIOUS LASS THICAL ONCERNS OF ATA
C & I C
OLLECTION MAGE LASSIFICATION
• The main issues:
• Intellectual property concerns (week 15)
• Bias in representativeness of images (week 15)
• Labor practices on crowdwork platforms (INFO
203)
• Privacy concerns for applications of image
recognition (I 203, I 205)

--- Page 3 ---
Today’s Outline
Upcoming Assignments & Lectures
Revisit Card Sorting
Grounded Coding
Inter-annotator Agreement

--- Page 4 ---
U L T
PCOMING ECTURE OPICS
• Week 10: Information seeking, information foraging
• Week 11: Guest lectures:
• Federal Datasets
• Rulemaking Standards
• Week 12: How search engines work; search evaluation
• Week 13: Generative AI; LLMs and search
• Week 14: Misinformation (Thanksgiving Week)
• Week 15: Fairness & Bias in Search; Intellectual Property

--- Page 5 ---
Topic: A Federal Data Field Guide
Good source for class projects! Ask her!

--- Page 6 ---
Topic: Data Standards Creation;
Digital Accessibility

--- Page 7 ---
U A
PCOMING SSIGNMENTS
• This week: Grounded coding individually
• Week 10: Grounded coding with a partner
• Week 11-12: Final project proposal
• Week 13: Search assignment; may include questions about the gues
lectures
• Week 14-15: Work on final project

--- Page 8 ---
F P (I )
INAL ROJECTS NDIVIDUAL
• Goal: synthesize concepts from throughout the semester and apply them to a real-world topic.
• Sizable: (we expect it to take approximately 4 weeks to complete, including proposal writing),
but we are not expecting you to perform original research or propose new methods.
• Choice of:
• Paper (3200-6400 words, cite at least 3 sources)
• Implementation project (hosted online; also requires a writeup)
• Design project (includes creating information architectures and other category systems,
and should substantially engage with concepts from the class, and requires an evaluation)

--- Page 9 ---
Today’s Outline
Upcoming Assignments & Lectures
Revisit Card Sorting
Grounded Coding
Inter-annotator Agreement

--- Page 10 ---
Previously: Card Sorting

--- Page 11 ---
W D W D C S ?
HY O E O ARD ORTING
• Uncover people’s underlying assumptions/intuitions about how
concepts organize
• Open sort: given a set of terms, group terms & name the groups
• See how much agreement there is for categories for a set of
items
• Closed sort: given a set of terms and some pre-defined categories,
place the terms into the categories
• See how well people’s intuitions agree with a set of categories

--- Page 12 ---
G C
ROUNDED ODING
A technique for creating or analyzing surveys, interviews, and other qualitative info

--- Page 13 ---
Grounded Coding
What: A method for assigning categories to qualitative data
Why: Make sense of qualitative data (surveys, tweets, interviews)
How:
• Iterative
• Build consensus from a team of categorizers (“coders”)
• Categories based on data (grounded) instead of theory
• After rounds of data-driven category formation, use theories to
motivate which categories to make prominent

--- Page 14 ---
Example: Survey of Data Analysts
Alspaugh et al., Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices, IEEE TVCG 2019

--- Page 15 ---
Example: Survey of Data Analysts
Alspaugh et al., Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices, IEEE TVCG 2019

--- Page 16 ---
Example: Survey of Data Analysts
Alspaugh et al., Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices, IEEE TVCG 2019

--- Page 17 ---
Example: Survey of Data Analysts
Alspaugh et al., Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices, IEEE TVCG 2019

--- Page 18 ---
Example: Survey of Data Analysts
Alspaugh et al., Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices, IEEE TVCG 2019

--- Page 19 ---
Example: Grounded Coding of Tweets
Goal: better understand how students are writing outside
the classroom
Approach: use tweets to analyze the writing practices of
fans of Bruce Springsteen
Data: tweets before, during, and after a concert in 2012
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015
https://web.archive.org/web/20180212012518/http://kairos.technorhetoric.net/19.3/topoi/wolff/index.html
wikipedia

--- Page 20 ---
First Stage: Open Coding
Notice small details
You can use any code naming scheme you like
Make brief definitions for each code
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3,

--- Page 21 ---
Open Coding a Tweet
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 22 ---
Open Codes
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 23 ---
A C
XIAL ODING
• Goal: generate categories related to the focus; capture higher-
level phenomena
• In this case study, the author decided to focus on pre, during, and
post concert tweets because these were the most complex and
embodied many of the other categories
• Example:
• LYRIC: mention lyrics: ALB: mentioned album: SOT: song title
• Intertextual: Tweets containing LYRIC, ALB, and/or SOT
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 24 ---
s
e
d
o
C
l
a
i
x
A
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 25 ---
s
e
d
o
C
l
a
i
x
A
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 26 ---
S C
ELECTIVE ODING
• Goal: generate theories about the phenomena
• Method: make connections between the categories
defined during axial coding
• Example:
• Practicing: “fandom involves a particular set of critical
and interpretive practices”
• Composed of 5 out of the 18 axial codes
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 27 ---
S R
UMMARY ESULTS
“Category percentages show significant narrating of events, little
conversing, some notifying of others, and lots of retweeting. “
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 28 ---
S R
UMMARY ESULTS
Some nuances according to pre, during, and post concert tweets
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015

--- Page 29 ---
A : P M O C
SSIGNMENT RACTICE AKING PEN ODES
S C C
ENTENCES ABOUT LIMATE HANGE
Global sea level rose about 8 inches in
1 supported the last century.
Extreme melting and changes to the climate like this has released
2 supported pressure on to the continent, allowing the ground to rise up.
The Great Barrier Reef is experiencing the most
3 supported widespread bleaching ever recorded
Human additions of CO2 are in the margin of error of current
measurements and the gradual increase in CO2 is mainly from
oceans degassing as the planet slowly emerges from the last ice
4 refuted age.
The rate of warming according to the data is much slower
5 refuted than the models used by the IPCC

--- Page 30 ---
I -A A
NTER NNOTATOR GREEMENT
Calculate this to see if you coding strategy is reliable
What to do when your coders don’t agree?
Try, try again!
Refine the categories until you get decent inter-
annotator agreement

--- Page 31 ---
C ’ K
OHEN S APPA
• Purpose: Measures the agreement between two raters for categorical data, correcting for
chance.
• Application: Ideal for situations where two human annotators classify a set of items into
predefined categories, such as rating movie reviews as "positive" or "negative".
• Weighted Kappa: A variation used for ordinal data (ranked categories). It accounts for the
magnitude of disagreement, giving partial credit for "close" ratings.
• Limitations:
• Limited to two annotators.
• Can produce counterintuitive results (the "Kappa paradox") when data is highly
skewed toward one category
Advantage: easiest to understand & compute

--- Page 32 ---
F ’ K
LEISH S APPA
• Purpose: An extension of Cohen's Kappa that measures the reliability of agreement for three or
more raters with categorical data.
• Application: Suitable for larger annotation projects where multiple annotators are involved, such
as assessing the severity of a medical condition or categorizing social media posts.
• Assumption: Assumes that the raters are randomly sampled from a population of raters. This
means the same set of raters do not need to evaluate all items.
• Key difference from Cohen's: While Cohen's Kappa assumes the same two raters, Fleiss' Kappa
can be used even if different raters evaluate different items.

--- Page 33 ---
K ’ A
RIPPENDORFF S LPHA
• Purpose: A versatile reliability coefficient that can handle virtually any situation, including:
• Any number of raters
• Missing data
• Any level of measurement (nominal, ordinal, interval, or ratio)
• Application: Often used in complex annotation tasks, such as coding open-ended text in content
analysis or evaluating machine learning outputs.
• Flexibility: It is arguably one of the most robust and flexible inter-rater reliability measures
available
• Limitation: More complex to understand (but supported by software packages)

--- Page 34 ---
I -A A
NTER NNOTATOR GREEMENT
2 ; 8 18
CODERS FIRST OF CATEGORIES
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015 kairos.technorhetoric.net/19.3/topoi/wolff/

--- Page 35 ---
I -A A
NTER NNOTATOR GREEMENT
2 ; 8 18
CODERS FIRST OF CATEGORIES
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015 kairos.technorhetoric.net/19.3/topoi/wolff/

--- Page 36 ---
M I -A A :
EASURING NTER NNOTATOR GREEMENT
S P
IMPLE ROPORTIONS
Simple method: Simply compute the proportion of times the two
annotators agree.
In the table below, the two Profs agree 2 times out of 6, or .33
(WL is waitlist)

--- Page 37 ---
M I -A A :
EASURING NTER NNOTATOR GREEMENT
S P
IMPLE ROPORTIONS
This method has drawbacks Coder A Coder B Agree?
cat cat agree
Example: Labeling photos;
cat cat agree
cat cat agree
Some distinctions are easier than others
cat cat agree
• cat vs porpoise is easy
porpoise dolphin disagree
• dolphin vs porpoise is difficult porpoise dolphin disagree
dolphin dolphin agree
• Therefore, two annotators are more likely to
dolphin porpoise disagree
agree on cat/porpoise than dolphin/porpoise
dog dog agree
dog cat disagree

--- Page 38 ---
C ’ K O W M I -
OHEN S APPA IS NE AY TO EASURE NTER
A A
NNOTATOR GREEMENT
• It is a widely used measure.
• Krippendorff’s alpha is preferred now, but tricker to compute
• It takes into account that agreement can happen by chance.
• Ranges between 1 and -1
• Values closer to 1 indicate high agreement.
• Negative values indicate strong disagreement
• Ideally you achieve a score of around .8 or higher
• However, scores are often lower if you have a lot of categories
• Some categories are crisper than others

--- Page 39 ---
Example: Using Simple Proportions
But: agreement can happen
by chance!
Cohen’s Kappa gives you a
measure of how good the
agreement is taking chance
agreement into account.
https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c

--- Page 40 ---
Example: Computing Cohen’s Kappa
https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c

--- Page 41 ---
Cohen’s Kappa:
Compute Probability of Observed Agreement
Compute the observed
probability of agreement for
each label and coder
ProbA(Accept) = 6/25
ProbA(WL) = 10/25
ProbA(Reject) = 9/25
ProbB(Accept) = 13/25
ProbB(WL) = 3/25
ProbB(Reject) = 9/25
https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c

--- Page 42 ---
Cohen’s Kappa:
Compute Probability of Chance Agreement
The probability of the chance of
agreement for each label
Multiply the previous values for each label
https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c

--- Page 43 ---
Put the Cohen’s Kappa Formula Together
The Kappa Score is:
(ProbObservedAgreement – ChanceAgreement)
Divided by (1 – ChanceAgreement)
Kappa should be above about .67
This shows very low agreement
between the professors
https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c

--- Page 44 ---
W ?
HAT ABOUT MULTIPLE LABELS PER ITEM
• Situation: 2 people assigning codes to 3 sentences from interviews
• The coders want to assign more than one code per sentence
sentence Coder A Coder B
1 {Happy, Sad} {Happy}
2 {Sad} {Sad, Confused}
3 {Confused} {Confused}

--- Page 45 ---
W ?
HAT ABOUT MULTIPLE LABELS PER ITEM
• We expand this table out to assign a binary (0 or 1) to each combination
sentence Coder A Coder B sentence Code A B
1 {Happy, Sad} {Happy}
1 Happy 1 1
2 {Sad} {Sad, Confused}
1 Sad 1 0
3 {Confused} {Confused}
1 Confused 0 0
2 Happy 0 0
2 Sad 1 1
2 Confused 0 1
3 Happy 0 0
3 Sad 0 0
3 Confused 1 1

--- Page 46 ---
Cohen’s Kappa: Multiple Labels Per Item
Compute Observed Probability of Agreement
sentence Code A B Agree = 1
Agree 7/9 times = .778
1 Happy 1 1 1
1 Sad 1 0 0
1 Confused 0 0 1
2 Happy 0 0 1
2 Sad 1 1 1
2 Confused 0 1 0
3 Happy 0 0 1
3 Sad 0 0 1
3 Confused 1 1 1

--- Page 47 ---
Cohen’s Kappa: Multiple Labels Per Item
Compute Chance Probability of Agreement
sentence Code A B Agree = 1
Compute Chance Agreement
1 Happy 1 1 1
(also called Expected Agreement)
1 Sad 1 0 0
1 Confused 0 0 1
Compute probability that coder A ever said 1 (4/9)
2 Happy 0 0 1
Compute probability that coder B ever said 1 (4/9)
2 Sad 1 1 1 Compute probability that coder A ever said 0 (5/9)
Compute probability that coder B ever said 0 (5/9)
2 Confused 0 1 0
3 Happy 0 0 1 Multiply the chance for label 1 (4/9 * 4/9)
Multiply the chance for label 0 (5/9 * 5/9)
3 Sad 0 0 1
Add these together: .1975 + .308 = .506
3 Confused 1 1 1

--- Page 48 ---
Cohen’s Kappa: Multiple Labels Per Item
Compute Chance Probability of Agreement
sentence Code A B Agree = 1
Compute Chance Agreement
1 Happy 1 1 1
(also called Expected Agreement)
1 Sad 1 0 0
1 Confused 0 0 1
Compute probability that coder A ever said 1 (4/9)
2 Happy 0 0 1
Compute probability that coder B ever said 1 (4/9)
Compute probability that coder A ever said 0 (5/9)
2 Sad 1 1 1
Compute probability that coder B ever said 0 (5/9)
2 Confused 0 1 0
Multiply the chance for label 1 (4/9 * 4/9)
3 Happy 0 0 1 Multiply the chance for label 0 (5/9 * 5/9)
Add these together:
3 Sad 0 0 1
ChanceAgree = .1975 + .308 = .506
3 Confused 1 1 1

--- Page 49 ---
Multiple Labels Per Item
Cohen’s Kappa
sentence Code A B Agree = 1
1 Happy 1 1 1
1 Sad 1 0 0
Agree = .778
ChanceAgree = .506
1 Confused 0 0 1
2 Happy 0 0 1
2 Sad 1 1 1
2 Confused 0 1 0
3 Happy 0 0 1
This is a middling score
3 Sad 0 0 1
The coders probably need to
3 Confused 1 1 1
improve their codebook

--- Page 50 ---
Let’s Practice
Download spreadsheet on course website (next to lecture notes)

--- Page 51 ---
O M
THER EASURES
• Cohen’s Kappa is controversial today due to some
limitations.
• Other measures can be better for more than 2 coders
• Other measures take into account that some categories
are more important than others
See Antoine et al., Weighted Krippendorff’s alpha is a more reliable metrics for multi-coders ordinal
annotations: experimental studies on emotion, opinion and coreference annotation, EACL 2014
https://www.aclweb.org/anthology/E14-1058/

--- Page 52 ---
S : G C
UMMARY ROUNDED ODING
• Is a method for creating systematic and consistent
categories for qualitative data
• Is usually needed even if you are going to use
machine learning, since you have to understand
your target.
• Requires multiple annotators to be confident in
results
• A very useful skill for information scientists!


================================================================================
FILE: 19_llm_search_sensemaking_foraging (1).pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 19: LLM Search Behavior, Foraging, Sensemaking

--- Page 2 ---
L T
AST IME
• Information Needs
• Search Tactics and Strategies
• Information Seeking Models

--- Page 3 ---
Today’s Outline
LLMs and Changing Search Behavior
Emotions and Search
Sensemaking
Foraging Theory

--- Page 4 ---
Natural language interfaces have long been a goal
Early Natural Language Database (1973)
W. A. Woods: Progress in natural language understanding—An application to lunar geology, AFIPs 1973
4

--- Page 5 ---
Natural language interfaces have long been a goal
Keyword search was never the goal, but NL did not work
People loved Ask Jeeves even though it didn’t work!
5

--- Page 6 ---
Computer scientists have long pursued natural language interfaces
But the obstacles were overwhelming
Notice the subtle differences of these two NL queries
9
https://northeastern-datalab.github.io/diagrammatic-representation-tutorial/

--- Page 7 ---
NLP has to work really well to be useful
Studies showed that people on average
experimented with Alexa for about a week and
then settled on a small fixed set of commands
This was due to its limited NLP abilities
10

--- Page 8 ---
https://www.nngroup.com/articles/ai-paradigm/ 11

--- Page 9 ---
Language-based Chat is the New User Interface
But only because it works very well
Many problems are now solved, but new ones arise! 12

--- Page 10 ---
Language-based Chat is the New User Interface
Pro: Can allow for great flexibility in expression
Easier to write with language than code
“Write code in python for jupyter notebook to extract paper
titles from the enclosed pdf.”
13

--- Page 11 ---
Language-based Chat is the New User Interface!
Pro: Can allow for great flexibility in expression
Con: it often requires (lots of) iteration
“Write code in python for jupyter notebook to extract paper
titles from the enclosed pdf. Paper titles are in boldface, author
names and affiliations are in normal text. Extract only the titles
even if they wrap to a second line, convert unicode to ascii, and
output to a csv. Assume the files are in the same directory as
the code.”
14

--- Page 12 ---
H LLM C S
OW ARE S HANGING EARCH
B ?
EHAVIOR
A summary of some recent research papers

--- Page 13 ---
S : C B S C -P
TUDY OMPARE ING EARCH TO O ILOT
• Used real query logs
• 80,000 conversations using Co-Pilot (an LLM), 80,000 using Bing Search
• Used GPT-4 to classify the main task in each conversation and session
according to Anderson and Krathwohl’s Taxonomy:
• From lowest complexity to highest:
• Remember, Understand, Apply, Analyze, Evaluate, and Create.
• Question: what percent of sessions fall into the 4 more complex types?
• Bing: 13.4%
• Copilot: 37.0%
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 14 ---
A much higher percentage (37%)
of Bing Copilot conversations
were categorized into the higher
complexity tasks compared to
only 13.4% for Bing Search
queries.
For both partial and fully
completed tasks, user
satisfaction increased more as
the level of task complexity
increases.
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 15 ---
S : C B S C -P
TUDY OMPARE ING EARCH TO O ILOT
• Classified conversations / sessions into 25 topical domains
• Also classified conversations as consisting of knowledge work
“Work that concerns the creation, handling, and distribution of
information and knowledge products, and that involved non-
routine tasks and uses creative and analytical thinking in
convergent and divergent ways”
• Search sessions in Bing Copilot contain user tasks that skew more
towards knowledge work than Bing Search.
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 16 ---
Knowledge Work Comparison
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 17 ---
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 18 ---
S : C GPT-3.5 (LLM) S
TUDY OMPARED TO TANDARD
S (B )
EARCH ING
Controlled experiment to facilitate direct
comparisons
Focused on car product comparison tasks
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 19 ---
Compared Interfaces
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 20 ---
S : C C (LLM) S
TUDY OMPARED OPILOT TO TANDARD
S (B )
EARCH ING
• First study: participants using the LLM-based tool were able to
complete their tasks more quickly, using fewer but more complex
queries than those who used traditional search
• Comparable results between search and LLM on “routine” tasks
• However, an overreliance on incorrect information when the LLM
erred; certain tasks were more likely to generate LLM errors
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 21 ---
Complexity of Queries
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 22 ---
S : C GPT 3.5 S S
TUDY OMPARED TO TANDARD EARCH
(B )
ING
• First study: participants using the LLM-based tool were able to complete their tasks
more quickly, using fewer but more complex queries than those who used
traditional search
• However, an overreliance on incorrect information when the LLM erred
• Second study: some users see a color-coded highlighting scheme to alert them to
potentially incorrect or misleading information in the LLM responses.
• This substantially increases the rate at which users spot incorrect information,
improving the accuracy of their overall decisions while leaving most other measures
unaffected
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 23 ---
Highlighting High and Low-Confidence Info
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 24 ---
I G C
MAGE EOLOCATION OMPARISON
• Task: determine the location where an image was captured
• 60 Participants using traditional search (Bing) more accurately predicted the
location of the image compared to those using the LLM-based search (Bing Chat).
• Participants using the LLM-based search issued longer, more natural language
queries, but had shorter search sessions.
• When reformulating their search queries, traditional search participants tended
to add more terms to their initial queries, whereas participants using the LLM-
based search consistently rephrased their initial queries.
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on Human
Information Interaction and Retrieval, 2024

--- Page 25 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on Human
Information Interaction and Retrieval, 2024

--- Page 26 ---
I G C
MAGE EOLOCATION OMPARISON
• Task: determine the location where an image was captured
• 60 Participants using traditional search (Bing) more accurately predicted the
location of the image compared to those using the LLM-based search (Bing Chat).
• Participants using the LLM-based search issued longer, more natural language
queries, but had shorter search sessions.
• When reformulating their search queries, traditional search participants tended
to add more terms to their initial queries, whereas participants using the LLM-
based search rephrased their initial queries.
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on Human
Information Interaction and Retrieval, 2024

--- Page 27 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIRl, 2024

--- Page 28 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIRl, 2024

--- Page 29 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIR 2024

--- Page 30 ---
E U G C
XAMPLE SE OF ROUNDED ODING
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIR, 2024

--- Page 31 ---
E S
MOTIONS AND EARCH

--- Page 32 ---
E S
MOTIONS AND EARCH
Kuhlthau 1991 on informational and emotional stages in search
Initiation Uncertainty and apprehension
Selection Confusion, uncertainty, doubt, frustration
Exploration Optimism (after deciding)
Formulation Confidence dawning *
Collection Confidence growing
Presentation Relief and satisfaction (or disappointment)
(Assuming novice researchers engaged in challenging tasks)

--- Page 33 ---
E : T I A
MOTIONS HE MPORTANCE OF ESTHETICS
With an aesthetically pleasing design:
▪ People will enjoy working with it more
▪ People will persist searching longer
▪ People will (sometimes) choose it even if it is less
efficient
Nakarada-Kordic & Lobb, 2005, Ben-Basset et al. 2006, Parush et al. 1998, van der Heijden 2003

--- Page 34 ---
E : T I A
MOTIONS HE MPORTANCE OF ESTHETICS
Small details matter
Example:
A lefthand side line vs. a box for ads
The line integrates the results into the page
Balancing white space with content
Balancing font color, shape, and weight
Hotchkiss, Search Engine Land Report 2007

--- Page 35 ---
F
ORAGING
T
HEORY

--- Page 36 ---
Information Foraging Theory
A theory that tries to quantify people’s decision-making
processes during information seeking.
Tries to answer questions like: How do people decide to click
on a link? When do they leave a webpage? When do they
prefer to search and when do they browse?
When users have an information goal, they try to maximize:

--- Page 37 ---
Foraging Theory
https://www.nngroup.com/articles/information-foraging/

--- Page 38 ---
Foraging Theory
https://www.nngroup.com/articles/information-foraging/

--- Page 39 ---
https://www.nngroup.com/articles/information-foraging/

--- Page 40 ---
Information Foraging Theory:
Essentially, A Cost/Benefit Analysis
Information Value:
Do you want a varied diet, or fill up on one thing?
Information “Scent” and Structure:
How to find a good direction to travel to a new info “patch”?
Want to Maximize
Rate of gain = Information value / Cost of obtaining that information

--- Page 41 ---
An Example of “Information Scent”

--- Page 42 ---
An Example of “Information Scent”
The text beneath the
link provides a concise
summary of what can
be found there.

--- Page 43 ---
S M
ENSE AKING

--- Page 44 ---
Two parts of a process:
I
NFORMATION
analysis and synthesis
search and retrieval
S
EEKING of search results
B
EHAVIOR IS
P
ART OF
S M
ENSE AKING The whole process is called
SenseMaking

--- Page 45 ---
Follow Links
Search
The Sensemaking Model
Ask Colleagues
Triage
Navigate Resources
“The process of searching for a representation and
encoding data in that representation to answer task-
Read Overviews
specific questions.” – Russell et al. CHI 1993
Take Notes
Search
Categorize Notes
Encoding
Write Summaries
Create Spreadsheets
Make Database Entries
Talk with Collaborators

--- Page 46 ---
The Encoding Portion
Encoding
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 47 ---
Intelligence Analysts’ SenseMaking Loop
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as
Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 48 ---
Intelligence Analysts’ SenseMaking Loop
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as
Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 49 ---
Intelligence Analysts’ SenseMaking Loop
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as
Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 51 ---
Search Portion of the Sensemaking Loop

--- Page 52 ---
Encoding Portion of the Sensemaking Loop

--- Page 53 ---
Spatial Organizations of Search Results
There have been many attempts
They haven’t caught on except for citation graphs
Organizing with tags / facets seems to work better

--- Page 54 ---
Many, Many Spatial Search Results Layout Attempts

--- Page 55 ---
Many Visual Browsing History Attempts

--- Page 56 ---
The Data
Mountain
Robertson et al. "Data mountain: using spatial memory for document
management."UIST 1998.
https://uist.acm.org/archive/html/videos.html#1998

--- Page 57 ---
Data Mountain Video

--- Page 58 ---
Research Rabbit

--- Page 59 ---
Research paper citation graphs are an
exception. Recent example: Connected Papers

--- Page 60 ---
Research paper citation graphs are an
exception. Recent example: Connected Papers

--- Page 61 ---
Spatial Organizations of Search Results
There have been many attempts
They haven’t caught on except for citation graphs
Organizing with tags / facets seems to work better

--- Page 62 ---
Our Reserach: The Problem
Time-constrained analysts Millions of documents Want to:
Search quickly, triage best docs
Organize quickly as they go
Later, organize in more detail
Hearst & Degler. "Sewing the seams of sensemaking: A practical interface for tagging and organizing saved search results.” HCIR 2013.

--- Page 63 ---
The Solution: Focus on Triage
Triage: “The practice of quickly determining the usefulness
and relevance of documents in a collection of documents.”
Badi et al., IUI’06
• Keyboard Letter Commands Create Groups
• Groups Are Immediately Visible, Editable
• Groups Tightly Coupled to Search
• Spatial Organizing Available, but with Automated Arranging
Hearst & Degler. "Sewing the seams of sensemaking: A practical interface for tagging and organizing saved search results.” HCIR 2013.

--- Page 65 ---
Search Triage User Interface
Hearst & Degler. "Sewing the seams of sensemaking: A practical interface for tagging and organizing saved search results.” HCIR 2013.

--- Page 66 ---
.3
1
0
2
R
IC
H
r .
e
lg
e
D
&
t
s
r
a
e
H

--- Page 67 ---
S
UMMARY
• Search is part of a larger sensemaking process
• Consists of a cycle of searching, assimilating info, and creating
knowledge
• Models of the search process help us design novel search user interfaces
• Information foraging theory
• Berry picking model
• Orienteering
• Natural language dialogue
• LLM-based dialog interfaces are automating big chunks of what used to
be manual steps in search and encoding


================================================================================
FILE: 19_llm_search_sensemaking_foraging.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 19: LLM Search Behavior, Foraging, Sensemaking

--- Page 2 ---
L T
AST IME
• Information Needs
• Search Tactics and Strategies
• Information Seeking Models

--- Page 3 ---
Today’s Outline
LLMs and Changing Search Behavior
Emotions and Search
Sensemaking
Foraging Theory

--- Page 4 ---
Natural language interfaces have long been a goal
Early Natural Language Database (1973)
W. A. Woods: Progress in natural language understanding—An application to lunar geology, AFIPs 1973
4

--- Page 5 ---
Natural language interfaces have long been a goal
Keyword search was never the goal, but NL did not work
People loved Ask Jeeves even though it didn’t work!
5

--- Page 6 ---
Computer scientists have long pursued natural language interfaces
But the obstacles were overwhelming
Notice the subtle differences of these two NL queries
9
https://northeastern-datalab.github.io/diagrammatic-representation-tutorial/

--- Page 7 ---
NLP has to work really well to be useful
Studies showed that people on average
experimented with Alexa for about a week and
then settled on a small fixed set of commands
This was due to its limited NLP abilities
10

--- Page 8 ---
https://www.nngroup.com/articles/ai-paradigm/ 11

--- Page 9 ---
Language-based Chat is the New User Interface
But only because it works very well
Many problems are now solved, but new ones arise! 12

--- Page 10 ---
Language-based Chat is the New User Interface
Pro: Can allow for great flexibility in expression
Easier to write with language than code
“Write code in python for jupyter notebook to extract paper
titles from the enclosed pdf.”
13

--- Page 11 ---
Language-based Chat is the New User Interface!
Pro: Can allow for great flexibility in expression
Con: it often requires (lots of) iteration
“Write code in python for jupyter notebook to extract paper
titles from the enclosed pdf. Paper titles are in boldface, author
names and affiliations are in normal text. Extract only the titles
even if they wrap to a second line, convert unicode to ascii, and
output to a csv. Assume the files are in the same directory as
the code.”
14

--- Page 12 ---
H LLM C S
OW ARE S HANGING EARCH
B ?
EHAVIOR
A summary of some recent research papers

--- Page 13 ---
S : C B S C -P
TUDY OMPARE ING EARCH TO O ILOT
• Used real query logs
• 80,000 conversations using Co-Pilot (an LLM), 80,000 using Bing Search
• Used GPT-4 to classify the main task in each conversation and session
according to Anderson and Krathwohl’s Taxonomy:
• From lowest complexity to highest:
• Remember, Understand, Apply, Analyze, Evaluate, and Create.
• Question: what percent of sessions fall into the 4 more complex types?
• Bing: 13.4%
• Copilot: 37.0%
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 14 ---
A much higher percentage (37%)
of Bing Copilot conversations
were categorized into the higher
complexity tasks compared to
only 13.4% for Bing Search
queries.
For both partial and fully
completed tasks, user
satisfaction increased more as
the level of task complexity
increases.
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 15 ---
S : C B S C -P
TUDY OMPARE ING EARCH TO O ILOT
• Classified conversations / sessions into 25 topical domains
• Also classified conversations as consisting of knowledge work
“Work that concerns the creation, handling, and distribution of
information and knowledge products, and that involved non-
routine tasks and uses creative and analytical thinking in
convergent and divergent ways”
• Search sessions in Bing Copilot contain user tasks that skew more
towards knowledge work than Bing Search.
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 16 ---
Knowledge Work Comparison
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 17 ---
Suri et al., The Use of Generative Search Engines for Knowledge Work and Complex Tasks, arXiv:2404.04268, March 2024

--- Page 18 ---
S : C GPT-3.5 (LLM) S
TUDY OMPARED TO TANDARD
S (B )
EARCH ING
Controlled experiment to facilitate direct
comparisons
Focused on car product comparison tasks
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 19 ---
Compared Interfaces
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 20 ---
S : C C (LLM) S
TUDY OMPARED OPILOT TO TANDARD
S (B )
EARCH ING
• First study: participants using the LLM-based tool were able to
complete their tasks more quickly, using fewer but more complex
queries than those who used traditional search
• Comparable results between search and LLM on “routine” tasks
• However, an overreliance on incorrect information when the LLM
erred; certain tasks were more likely to generate LLM errors
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 21 ---
Complexity of Queries
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 22 ---
S : C GPT 3.5 S S
TUDY OMPARED TO TANDARD EARCH
(B )
ING
• First study: participants using the LLM-based tool were able to complete their tasks
more quickly, using fewer but more complex queries than those who used
traditional search
• However, an overreliance on incorrect information when the LLM erred
• Second study: some users see a color-coded highlighting scheme to alert them to
potentially incorrect or misleading information in the LLM responses.
• This substantially increases the rate at which users spot incorrect information,
improving the accuracy of their overall decisions while leaving most other measures
unaffected
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 23 ---
Highlighting High and Low-Confidence Info
Spatharioti et al, Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, 2023

--- Page 24 ---
I G C
MAGE EOLOCATION OMPARISON
• Task: determine the location where an image was captured
• 60 Participants using traditional search (Bing) more accurately predicted the
location of the image compared to those using the LLM-based search (Bing Chat).
• Participants using the LLM-based search issued longer, more natural language
queries, but had shorter search sessions.
• When reformulating their search queries, traditional search participants tended
to add more terms to their initial queries, whereas participants using the LLM-
based search consistently rephrased their initial queries.
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on Human
Information Interaction and Retrieval, 2024

--- Page 25 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on Human
Information Interaction and Retrieval, 2024

--- Page 26 ---
I G C
MAGE EOLOCATION OMPARISON
• Task: determine the location where an image was captured
• 60 Participants using traditional search (Bing) more accurately predicted the
location of the image compared to those using the LLM-based search (Bing Chat).
• Participants using the LLM-based search issued longer, more natural language
queries, but had shorter search sessions.
• When reformulating their search queries, traditional search participants tended
to add more terms to their initial queries, whereas participants using the LLM-
based search rephrased their initial queries.
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on Human
Information Interaction and Retrieval, 2024

--- Page 27 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIRl, 2024

--- Page 28 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIRl, 2024

--- Page 29 ---
I G C
MAGE EOLOCATION OMPARISON
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIR 2024

--- Page 30 ---
E U G C
XAMPLE SE OF ROUNDED ODING
Wazzan et al., Comparing Traditional and LLM-based Search for Image Geolocation, Conference on HIIR, 2024

--- Page 31 ---
E S
MOTIONS AND EARCH

--- Page 32 ---
E S
MOTIONS AND EARCH
Kuhlthau 1991 on informational and emotional stages in search
Initiation Uncertainty and apprehension
Selection Confusion, uncertainty, doubt, frustration
Exploration Optimism (after deciding)
Formulation Confidence dawning *
Collection Confidence growing
Presentation Relief and satisfaction (or disappointment)
(Assuming novice researchers engaged in challenging tasks)

--- Page 33 ---
E : T I A
MOTIONS HE MPORTANCE OF ESTHETICS
With an aesthetically pleasing design:
▪ People will enjoy working with it more
▪ People will persist searching longer
▪ People will (sometimes) choose it even if it is less
efficient
Nakarada-Kordic & Lobb, 2005, Ben-Basset et al. 2006, Parush et al. 1998, van der Heijden 2003

--- Page 34 ---
E : T I A
MOTIONS HE MPORTANCE OF ESTHETICS
Small details matter
Example:
A lefthand side line vs. a box for ads
The line integrates the results into the page
Balancing white space with content
Balancing font color, shape, and weight
Hotchkiss, Search Engine Land Report 2007

--- Page 35 ---
F
ORAGING
T
HEORY

--- Page 36 ---
Information Foraging Theory
A theory that tries to quantify people’s decision-making
processes during information seeking.
Tries to answer questions like: How do people decide to click
on a link? When do they leave a webpage? When do they
prefer to search and when do they browse?
When users have an information goal, they try to maximize:

--- Page 37 ---
Foraging Theory
https://www.nngroup.com/articles/information-foraging/

--- Page 38 ---
Foraging Theory
https://www.nngroup.com/articles/information-foraging/

--- Page 39 ---
https://www.nngroup.com/articles/information-foraging/

--- Page 40 ---
Information Foraging Theory:
Essentially, A Cost/Benefit Analysis
Information Value:
Do you want a varied diet, or fill up on one thing?
Information “Scent” and Structure:
How to find a good direction to travel to a new info “patch”?
Want to Maximize
Rate of gain = Information value / Cost of obtaining that information

--- Page 41 ---
An Example of “Information Scent”

--- Page 42 ---
An Example of “Information Scent”
The text beneath the
link provides a concise
summary of what can
be found there.

--- Page 43 ---
S M
ENSE AKING

--- Page 44 ---
Two parts of a process:
I
NFORMATION
analysis and synthesis
search and retrieval
S
EEKING of search results
B
EHAVIOR IS
P
ART OF
S M
ENSE AKING The whole process is called
SenseMaking

--- Page 45 ---
Follow Links
Search
The Sensemaking Model
Ask Colleagues
Triage
Navigate Resources
“The process of searching for a representation and
encoding data in that representation to answer task-
Read Overviews
specific questions.” – Russell et al. CHI 1993
Take Notes
Search
Categorize Notes
Encoding
Write Summaries
Create Spreadsheets
Make Database Entries
Talk with Collaborators

--- Page 46 ---
The Encoding Portion
Encoding
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 47 ---
Intelligence Analysts’ SenseMaking Loop
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as
Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 48 ---
Intelligence Analysts’ SenseMaking Loop
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as
Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 49 ---
Intelligence Analysts’ SenseMaking Loop
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as
Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 51 ---
Search Portion of the Sensemaking Loop

--- Page 52 ---
Encoding Portion of the Sensemaking Loop

--- Page 53 ---
Spatial Organizations of Search Results
There have been many attempts
They haven’t caught on except for citation graphs
Organizing with tags / facets seems to work better

--- Page 54 ---
Many, Many Spatial Search Results Layout Attempts

--- Page 55 ---
Many Visual Browsing History Attempts

--- Page 56 ---
The Data
Mountain
Robertson et al. "Data mountain: using spatial memory for document
management."UIST 1998.
https://uist.acm.org/archive/html/videos.html#1998

--- Page 57 ---
Data Mountain Video

--- Page 58 ---
Research Rabbit

--- Page 59 ---
Research paper citation graphs are an
exception. Recent example: Connected Papers

--- Page 60 ---
Research paper citation graphs are an
exception. Recent example: Connected Papers

--- Page 61 ---
Spatial Organizations of Search Results
There have been many attempts
They haven’t caught on except for citation graphs
Organizing with tags / facets seems to work better

--- Page 62 ---
Our Reserach: The Problem
Time-constrained analysts Millions of documents Want to:
Search quickly, triage best docs
Organize quickly as they go
Later, organize in more detail
Hearst & Degler. "Sewing the seams of sensemaking: A practical interface for tagging and organizing saved search results.” HCIR 2013.

--- Page 63 ---
The Solution: Focus on Triage
Triage: “The practice of quickly determining the usefulness
and relevance of documents in a collection of documents.”
Badi et al., IUI’06
• Keyboard Letter Commands Create Groups
• Groups Are Immediately Visible, Editable
• Groups Tightly Coupled to Search
• Spatial Organizing Available, but with Automated Arranging
Hearst & Degler. "Sewing the seams of sensemaking: A practical interface for tagging and organizing saved search results.” HCIR 2013.

--- Page 65 ---
Search Triage User Interface
Hearst & Degler. "Sewing the seams of sensemaking: A practical interface for tagging and organizing saved search results.” HCIR 2013.

--- Page 66 ---
.3
1
0
2
R
IC
H
r .
e
lg
e
D
&
t
s
r
a
e
H

--- Page 67 ---
S
UMMARY
• Search is part of a larger sensemaking process
• Consists of a cycle of searching, assimilating info, and creating
knowledge
• Models of the search process help us design novel search user interfaces
• Information foraging theory
• Berry picking model
• Orienteering
• Natural language dialogue
• LLM-based dialog interfaces are automating big chunks of what used to
be manual steps in search and encoding


================================================================================
FILE: 1_information.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 1: Introduction

--- Page 2 ---
Today’s Outline
Instructor Introductions
Student Introductions
Information vs Data
Course Structure

--- Page 3 ---
I : M H
NSTRUCTOR ARTI EARST
I School and CS Professor
Research: Information Visualization, Natural
Language Processing, HCI
Fav Berkeley Restaurant: Zachary’s Pizza, Great China, Millennium

--- Page 4 ---
TA: S L
UNNY EE
MIMS ’26
Bachelors in Neuroscience & Cognitive Science
Work experience in healthcare & academic research
Current interests: UX research, data science, product mgmt

--- Page 5 ---
R : S A
EADER ARAH LGASHGARI
MIMS ’26
IMSA President
Information Visualization, Consulting

--- Page 6 ---
S I
TUDENT NTRODUCTIONS
Introduce yourself to your neighbor

--- Page 7 ---
S B : R
TUDENT ACKGROUND ESULTS OF
S
URVEY

--- Page 8 ---
Where Did We Arrive From?

--- Page 9 ---
Favorite Berkeley Restaurants
Mezzo
Saigon Express
Imm Thai
Saul’s Deli
Noodle Dynasty
The Smokehouse
Jot Mahal
HanKki
Kura Sushi
Masa Raman Lulu

--- Page 10 ---
What Excites You about the I School

--- Page 11 ---
What You Hope to Achieve Here
Make friends and network
Enhance both technical and interpersonal skills
Broaden my skillset within information science
Leave with a UX research position
Get more technical in building software & upskill in design
Learn new skills in different areas & have fun
Be a sponge

--- Page 13 ---
Tools for Class Interaction
bcourses #info-202-2025-fall

--- Page 14 ---
What is Data? What is Information?

--- Page 15 ---
Imagine you are viewing this boat from the dock.
What kinds of information might you want to find out about it?
What kinds of data might you want to find out about it?

--- Page 16 ---
Original Image by Hugh MacLeod. https://informationversusknowledge-blog.tumblr.com/

--- Page 17 ---
Original Image by Hugh MacLeod. https://informationversusknowledge-blog.tumblr.com/

--- Page 18 ---
Data, Information, Knowledge, Wisdom
DIKW Pyramid
Data
The raw material of information
Information
Data organized and presented by someone
Knowledge
Information read, heard or seen and understood
Wisdom
Distilled and integrated knowledge and understanding
wikipedia
People debate this representation; see the DIKW Wikipedia article

--- Page 19 ---
B ’ L B
ORGES IBRARY OF ABEL
Imagine a universal library, containing books with every possible
combination of 410 pages of letters, thus containing every book that
ever has been and every book that ever could be written
Image from Andrew DeGraff’s Plotted: A Literary Atlas

--- Page 20 ---
The books are organized into 3D hexagons of
bookshelves that the library patrons live in.
Twenty bookshelves, five to each side, line four of the
hexagon's six sides, each bookshelf holds thirty-two books
identical in format
Each book contains four hundred ten pages; each page, forty
lines; each line, approximately eighty black letters.
Image by Jean-Francois Rauzier

--- Page 21 ---
T S B L
HE TRUCTURE OF THE ORGES IBRARY
• The books are all 410 pages long, 40 lines per page, 80 characters
per line, so 1,312,000 characters per book
• The alphabet has 25 characters
• The library contains every possible 410-page permutation of those
characters, so 25 raised to 1,312,000 books.
• The books are organized into 3D hexagons of bookshelves that the
library patrons live in.
• The entire library is a sphere; each hexagon is a center.
Vintage Español

--- Page 22 ---
T D B L
HE IGITAL ORGES IBRARY
libraryofbabel.info by Jonathan Basile
A digital library of every possible 3200 character “book”

--- Page 23 ---
An Abstract from a New Scientific Paper

--- Page 24 ---
An Abstract from a New Scientific Paper

--- Page 25 ---
T D B L
HE IGITAL ORGES IBRARY
libraryofbabel.info by Jonathan Basile
A digital library of every possible 3200 character “book”
Exercise: Take a paragraph that you have written in the
past. Will you be able to find it in the library? Why or
why not? Try it out! What does it all mean?

--- Page 26 ---
A T C
BOUT HIS OURSE

--- Page 27 ---
This course studies both the human and the
technical sides of Info Org and Retrieval
and their intersection…

--- Page 28 ---
This course also
introduces you to many
of the instructors and
courses at the I School

--- Page 29 ---
A , M , P
SSIGNMENTS IDTERM ROJECT
Assignments help you practice concepts
Both during and after class
Some will build towards a larger assignment
We will teach you the concepts you need to use
Midterm will help you practice concepts; open book / take-home
Final project will let you explore a topic of your choice in depth

--- Page 30 ---
R
EADINGS
When to read them?
People’s preferences differ
I’ve posted them in advance
Assignments will usually be due the following Monday

--- Page 31 ---
R R A
EADING EFLECTION SSIGNMENTS
By the first deadline: By the second Most interesting posts
You’ll be organized into
deadline: may be shared with full
groups of students
class
Read the readings Respond to other students’
posts in a threaded
Post a thoughtful response to
conversation
the prompt

--- Page 32 ---
O A
THER SSIGNMENTS
• Create metadata structures (HTML, XML, JSON)
• Analyze a dataset
• Experiment with ranking & classification algorithms

--- Page 33 ---
A M -P
INI ROJECT
• Do grounded coding, individually and in pairs
• Build faceted category structures; assign to data
• Create faceted search interfaces from this data

--- Page 34 ---
Marti Co-Taught I 202 from 1997-2001,
Solo in 2020-2021
Prof. Ray Larson

--- Page 35 ---
Other Past Course Instructors
Prof. Bob Glushko Prof. David Bamman

--- Page 36 ---
A
NY
Q ?
UESTIONS


================================================================================
FILE: 22_ir_intro_crawling_indexing.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 22: How Web Search Works (IR intro, crawling, indexing)

--- Page 2 ---
Today’s Outline
What is IR?
Web Crawling
Inverted Index
Long Tail / Zipf’s Law

--- Page 3 ---
W I R
HAT IS NFORMATION ETRIEVAL
(IR)?
“Information retrieval deals with the representation, storage,
organization of, and access to information items.”
Chapter 1, Modern Information Retrieval, Baeza-Yates and Ribeiro-Neto 1999

--- Page 4 ---
IR H
ISTORY
• The “information overload” problem is not new!
• Origins in period immediately after World War II
• Tremendous scientific progress during the war
• Rapid growth in amount of scientific publications available
• The “Memex Machine”
• Conceived by Vannevar Bush, President Roosevelt's science
advisor
• Outlined in 1945 Atlantic Monthly article titled “As We May
Think”
• Foreshadows the development of hypertext (the Web) and
information retrieval system

--- Page 5 ---
T M M
HE EMEX ACHINE
V B , 1945
ANNEVAR USH

--- Page 6 ---
Information Retrieval Textbooks
2010 2011
2009

--- Page 7 ---
Search Interface Textbooks (academic)
2009 2016

--- Page 8 ---
Retrieval Augmented Generation
(I haven’t read these … might not be good!)
2025 2024

--- Page 9 ---
DBMS vs. Information Retrieval
Databases IR
What we’re Structured data. Clear Mostly unstructured.
retrieving semantics based on a Free text with some
formal model. metadata.
Queries we’re Formally Vague, imprecise
posing (mathematically) information needs
defined queries. (often expressed in
Unambiguous. natural language).
Results we get Exact. Always correct Sometimes relevant,
in a formal sense. sometimes not.
Interaction with One-shot queries. Interaction is important.
system
Other issues Concurrency, recovery, Not usually relevant.
atomicity are all critical.

--- Page 10 ---
D : H W S
ISCUSS OW DO EB EARCH
E W ?
NGINES ORK

--- Page 11 ---
H S E W
OW EARCH NGINES ORK
Three main parts:
i. Gather the contents of all web pages (using a
program called a crawler or spider)
ii. Organize the contents of the pages in a way that
allows efficient retrieval (indexing)
iii. Take in a query, determine which pages match, and
show the results (ranking and display of results)

--- Page 12 ---
S W S E A
TANDARD EB EARCH NGINE RCHITECTURE
Check for duplicates,
store the
crawl the
documents
Web web
Crawlers DocIds
Create an
user
inverted
query index
Search
Inverted
engine
Show results
index
To user servers

--- Page 13 ---
The Web Is
Enormous
https://www.worldwidewebsize.com/

--- Page 14 ---
“The Web is a Bow-Tie”,
2000
Broder, Andrei, et al. "Graph structure in the web."Computer networks33.1-6
(2000): 309-320.
Naturevolume405,page113 (2000)

--- Page 15 ---
The
Web
Is
Enor
mous
Internet-map.net

--- Page 16 ---
i. W C / S
EB RAWLERS PIDERS
How to find web pages to visit and copy?
• Can start with a list of domain names, visit the home pages there.
• Look at the hyperlink on the home page, and follow those links to
more pages.
• Keep a list of urls visited, and those still to be visited.
• Each time the program loads in a new HTML page, add the links in that
page to the list to be crawled.
Slide adapted from Lew & Davis

--- Page 17 ---
H W W W W ?
OW DOES THE ORLD IDE EB ORK
• Internet vs WWW
• Server vs Router
• IP address vs Domain Name
• URL Structure and Network Protocols

--- Page 18 ---
How Does the World Wide Web Work?
Say a user named Oski using his computer at home (or
in, say, Seoul) wants to find information about i202?
What happens when he:
Brings up a search engine home page?
Types his query?
First, we have to understand how the WWW works!
sylvain kalache at wikicommons
Then we can understand search engines.

--- Page 19 ---
I . WWW
NTERNET VS
• Internet and Web are not synonymous
• Internet is a global communication network
connecting millions (billions?) of computers.
• World Wide Web (WWW) is one component of
the Internet, along with e-mail, chat, etc.
• Now we’ll talk about both.
Slide adapted from Lew & Davis

--- Page 20 ---
How Does the WWW Work?
(simplified explanation)
• Let’s say Oski received email with the
address for i202 (assume it is at
ischool.berkeley.edu)
• He goes to a networked computer, and
launches a web browser.
• He then types the address, known as a
sylvain kalache at wikicommons
URL, into the address bar of the browser.
• What happens next?
(URL stands for Uniform Resource Locator)

--- Page 21 ---
Uploading a Page
• Say Oski’s instructor has written a web
page on her laptop.
• She copies the page to a directory on a
computer called herald, which is on the
ischool local network.
• This computer is connected to the
Internet and runs a program called
Apache. This allows herald to act as a
Web server
web server.

--- Page 22 ---
Routing Between Computers
• How does the computer at Oski’s desk
figure out where the i202 web pages are?
• In order for him to use the WWW, Oski’s
computer must be connected to another
machine acting as a web server (via his
ISP).
• This machine is in turn connected to
other computers, some of which are
routers.
iSchool
Network
Routers figure out how to move information from
one part of the network to another.
There are many different possible routes.

--- Page 23 ---
IP Address to Domain Name
• How do Oski’s server and the routers know how to find the right
server?
• First, the url has to be translated into a number known as an IP
address.
• Oski’s server connects to a Domain Names Server (DNS) that knows
how to do the translation.
DNS server

--- Page 24 ---
C D N
ONVERTING OMAIN AMES
• Domain names are for humans to read.
• The Internet actually uses numbers called IP addresses to
describe network addresses.
• The Domain Name System (DNS) – resolves IP addresses
into easily recognizable names
• For example:
• 12.42.192.73 = www.xyz.com
• A domain name and its IP address refer to the same Web
server.
Slide adapted from CIW foundations

--- Page 25 ---
T D N
YPICAL OMAIN AME
www.xyz.com
Server (host) Registered Domain
name company category
domain name (top-level
domain)
Domain names are part of URLs, used in web pages.
Slide adapted from CIW foundations

--- Page 26 ---
T -L D
OP EVEL OMAINS
• com, biz, cc — commercial or company sites
• edu — educational institutions, typically universities
• org — organizations; originally meant for clubs,
associations and nonprofit groups
• mil — U.S. military
• gov — U.S. civilian government
• net — network sites, including ISPs
• int — international organizations (rarely used)
Many other top-level domains are available
There is an interesting standards story about this!
Slide adapted from CIW foundations

--- Page 27 ---
I A
NTERNET DDRESSES
• The internet is a network on which each computer must have a unique
address.
• The Internet uses IP addresses; for example, herald’s IP address is
128.32.78.23
• Internet Protocol version 4 (IPv4) – supports 32-bit dotted quad IP address
format
• Four sets of numbers, each set ranging from 0 to 255
• UC Berkeley’s LAN addresses range from 128.32.0.0 to 128.32.255.255
• Other addresses in the iSchool LAN include 128.32.78.19
• Using this setup, there are approximately 4 billion possible unique IP
addresses
• Router software knows how to use the IP addresses to find the target
computer.
• The newer version is IPv6; it has 128-bit addresses; about 25% of the internet
is using this version as of 2018, according to Vint Cerf

--- Page 28 ---
IPV6 A
DOPTION

--- Page 29 ---
N P P
ETWORK ROTOCOLS AND ACKETS
• Network Protocols:
• Protocol – an agreed-upon format for
transmitting data between two devices
• The Internet protocol is TCP/IP
• The WWW protocol is HTTP
• Network Packets:
• Typically, a message is broken up into
smaller pieces and re-assembled at the
receiving end.
• These pieces of information, surrounded
by address information, are called packets
Slide adapted from CIW foundations
https://www.geeksforgeeks.org/computer-networks/difference-between-message-and-packet-switching/

--- Page 30 ---
IP P F ( 4)
ACKET ORMAT V
Field length in bits
Bit 0 Bit 31
Version Hdr Len
TOS (8) Total Length in bytes (16)
(4) (4)
Identification (16 bits) Flags (3) Fragment Offset (13)
r
e
Time to Live (8) Protocol (8) Header Checksum (16)
d
a
e Source IP Address (32)
H
Destination IP Address (32)
Options (if any)
a
t
a Data (variable length)
D

--- Page 31 ---
Using the URL
• What happens now that the request for
information from Oski’s browser has been
received by the web server herald at
www.ischool.berkeley.edu?
• The web server processes the url to figure
out which page on the server is requested.
• It then sends all the information from that
iSchool
page back to the requesting address.
Network

--- Page 32 ---
R URL
EADING A
http://courses.ischool.berkeley.edu/i202/f21/index.html
http:// = HyperText Transfer Protocol
courses = service name (often is www)
.ischool = host name
.berkeley = primary domain name
.edu/ = top level domain
i202/ = directory name
f21/ = directory names
index.html = file name of web page
Slide adapted from Lew & Davis

--- Page 33 ---
HTTP Request: Example
This information is received by the web server at
www.ischool.berkeley.edu :
Request line GET i202/f21/index.html HTTP/1.1<CRLF>
Request header Host: courses.ischool.berkeley.edu <CRLF>
Blank line <CRLF>
Because HTTP is built on TCP/IP, the web server
knows which IP address to send the contents of
the web page back to
.

--- Page 34 ---
Serving a Web Page
• When Oski typed in the url for the i202 home
page, this was turned into an HTTP request
and routed to the web server in Berkeley.
• The web server then decomposed the url and
figured out which web page in its directories
was being asked for.
• The server then sends the HTML contents of iSchool
Network
the page back to Oski’s IP address.
Oski’s browser receives these HTML contents
and renders the page in graphical form.
If he clicks on the hyperlink to the syllabus, a
similar sequence of events will happen.

--- Page 35 ---
H D Y
OW O OU
T W
HINK A EB
C
RAWLER
W ?
ORKS
Yogesh Gosavi, Unsplash

--- Page 36 ---
i. W C /
EB RAWLERS
S
PIDERS
How to find web pages to visit and copy?
• Can start with a list of domain names, visit the home pages
there.
• Look at the hyperlink on the home page, and follow those links to
more pages.
• Keep a list of urls visited, and those still to be visited.
• Each time the program loads in a new HTML page, add the links
in that page to the list to be crawled.
Slide adapted from Lew & Davis

--- Page 37 ---
R W P
ETRIEVING EB AGES
• Web crawler client program connects to a domain name system
(DNS) server
• DNS server translates the hostname into an internet protocol (IP)
address
• Crawler then attempts to connect to server host using specific port
• After connection, crawler sends an HTTP request to the web server
to request a page (usually a GET request)
Slide from Croft et al., Information Retrieval in Practice

--- Page 38 ---
Sec. 20.2
Crawling picture
URLs crawled
and parsed
Unseen Web
URLs frontier
Seed
pages
Web
Slide from Croft et al., Information Retrieval in Practice

--- Page 39 ---
W C
EB RAWLING
• Web crawlers spend a lot of time waiting for responses to
requests
• To reduce this inefficiency, web crawlers use threads and fetch
hundreds of pages at once
• Crawlers could potentially flood sites with requests for pages
• To avoid this problem, web crawlers use politeness policies
• e.g., delay between requests to same web server
Slide from Croft et al., Information Retrieval in Practice

--- Page 40 ---
F “L ” C
OUR AWS OF RAWLING
• A Crawler must show identification
• A Crawler must obey the robots exclusion standard
Use the robots.txt file to give instructions to the crawler
http://www.robotstxt.org/wc/norobots.html
• A Crawler must not hog resources
• A Crawler must report errors

--- Page 41 ---
S
PIDER BEHAVIOUR VARIES
• Parts of a web page that are indexed
• How deeply a site is indexed
• Types of files indexed
• How frequently the site is spidered
Slide adapted from Lew & Davis

--- Page 42 ---
L
OTS OF TRICKY ASPECTS
• Servers are often down or slow
• Hyperlinks can get the crawler into cycles
• Some websites have junk in the web pages
• Many pages have dynamic content
HUGE
• The web is

--- Page 43 ---
“F ”
RESHNESS
• Need to keep checking pages
• Pages change
• At different frequencies
• Who is the fastest changing?
• Pages are removed
• Many search engines cache the pages (store
a copy on their own servers)

--- Page 44 ---
D W
EEP EB
• Sites that are difficult for a crawler to find are collectively
referred to as the Deep (or hidden) Web
• much larger than conventional Web
• Three broad categories:
• private sites
• no incoming links, or may require log in with a valid account
• form results
• sites that can be reached only after entering some data into a
form
• scripted pages
• pages that use JavaScript, Flash, or another client-side
language to generate links
Slide from Croft et al., Information Retrieval in Practice

--- Page 45 ---
S
ITEMAPS
• Sitemaps contain lists of URLs and data about those
URLs, such as modification time and modification
frequency
• Generated by web server administrators
• Tells crawler about pages it might not otherwise find
• Gives crawler a hint about when to check a page for
changes
Slide from Croft et al., Information Retrieval in Practice

--- Page 46 ---
Sitemap Example
d
e
t
a
r
e
n
e gs
e
yg
lla a
p
c
im
a
n
y
D
Slide from Croft et al., Information Retrieval in Practice

--- Page 47 ---
D F
OCUMENT EEDS
• Many documents are published
• created at a fixed time and rarely updated again
• e.g., news articles, blog posts, press releases, email
• Published documents from a single source can
be ordered in a sequence called a document feed
• new documents found by examining the end of the
feed
• RSS used to be popular
• Social media (seems to) have replaced it

--- Page 48 ---
D C
ISTRIBUTED RAWLING
• Useful to use multiple computers for crawling
• Helps to put the crawler closer to the sites it crawls
• Reduces the number of sites the crawler has to keep track of
• Reduces computing resources required
• Distributed crawler uses a hash function to assign
URLs to crawling computers
• hash function should be computed on the host part of each
URL

--- Page 49 ---
T I HTML A
HE MPORTANCE OF NCHOR
T
EXT
<a href=“http://ischool.berkeley.edu”>
UCB School of Information </a>
<a href=“http://ischool.berkeley.edu”>
A terrific place to get a
masters degree
</a>
The anchor text (in green) summarizes
what the website is about.

--- Page 50 ---
H S E W
OW EARCH NGINES ORK
Three main parts:
i. Gather the contents of all web pages (using a
program called a crawler or spider)
ii. Organize the contents of the pages in a way
that allows efficient retrieval (indexing)
iii. Take in a query, determine which pages match,
and show the results (ranking and display of
results)

--- Page 51 ---
I I
NVERTED NDEX

--- Page 52 ---
S W S E A
TANDARD EB EARCH NGINE RCHITECTURE
Check for duplicates,
store the
crawl the
documents
web
Crawler
DocIds
machines
Create an
user
inverted
query index
Search
Inverted
engine
Show results
index
To user servers

--- Page 53 ---
. I
II NDEX
Record information about each page
• List of words
• In the title?
• How far down in the page?
• Was the word in boldface?
• URLs of pages pointing to this one
• Anchor text on pages pointing to this one
Slide adapted from Lew & Davis

--- Page 54 ---
Bag of Words Representation
Jurafsky & Martin slides

--- Page 55 ---
A S R T
IMPLE EPRESENTATION OF EXT
• How do we represent the complexities of language?
• Keeping in mind that computers don’t “understand”
documents or queries
• A simple, yet effective approach: create a“bag of words”
• Treat all the words in a document as index terms for
that document
• Assign a “weight” to each term based on its
“importance”
• Disregard order, structure, meaning, etc. of the words

--- Page 56 ---
Sample Document
McDonald's slims down spuds
Fast-food chain to reduce certain types of fat in its french 16 times: said
fries with new cooking oil.
14 times: McDonalds
NEW YORK (CNN/Money) - McDonald's Corp. is cutting the
amount of "bad" fat in its french fries nearly in half, the fast-food
chain said Tuesday as it moves to make all its fried menu items
11 times: fries
healthier.
But does that mean the popular shoestring fries won't taste the
same? The company says no. "It's a win-win for our customers 6 times each: company french nutrition
because they are getting the same great french-fry taste along with
an even healthier nutrition profile," said Mike Roberts, president of
McDonald's USA. 5 times each: food oil percent reduce taste
But others are not so sure. McDonald's will not specifically discuss
the kind of oil it plans to use, but at least one nutrition expert says …
playing with the formula could mean a different taste.
Shares of Oak Brook, Ill.-based McDonald's (MCD: down $0.54 to
$23.22, Research, Estimates) were lower Tuesday afternoon. It
was unclear Tuesday whether competitors Burger King and
Wendy's International (WEN: down $0.80 to $34.91, Research, “Bag of Words”
Estimates) would follow suit. Neither company could immediately
be reached for comment.
…

--- Page 57 ---
W “ ” ?
HY DOES BAG OF WORDS WORK
• Words alone tell us a lot about content
Random: beating takes points falling another Dow 355
Alphabetical: 355 another beating Dow falling points
Actual: Dow takes another beating, falling 355 points
• BUT: ignoring word order & context can be misleading
junior college is not the same as college junior
building … code: software or architecture regulations?

--- Page 58 ---
T D / T M
HE OCUMENT ERM ATRIX IS
S
PARSE
1 23 45 6 78
Term c c c c c c c c
o oo oo o oo
D DD DD D DD
aid 0 0 0 1 0 0 0 1
all 0 1 0 1 0 1 0 0
back 1 0 1 0 0 0 1 0
brown 1 0 1 0 1 0 1 0
come 0 1 0 1 0 1 0 1
dog 0 0 1 0 1 0 0 0
fox 0 0 1 0 1 0 1 0
good 0 1 0 1 0 1 0 1
jump 0 0 1 0 0 0 0 0
lazy 1 0 1 0 1 0 1 0
men 0 1 0 1 0 0 0 1
now 0 1 0 0 0 1 0 1
over 1 0 1 0 1 0 1 1
party 0 0 0 0 0 1 0 1
quick 1 0 1 0 0 0 0 0
their 1 0 0 0 1 0 1 0
time 0 1 0 1 0 1 0 0
Notice all of the zeros – tha tis a lot of wasted space

--- Page 59 ---
I I
NVERTED NDEX
• In reality, this index is HUGE
• Need to store the contents across many
machines
• Need to do optimization tricks to make lookup
fast.
• How and why to build it?

--- Page 60 ---
W N A S , F D
E EED MALLER ASTER ATA
S
TRUCTURE
• Can we make this data structure smaller, keeping in
mind the need for fast retrieval?
• Observations:
• The nature of the search problem requires us to
quickly find which documents contain a term
• The term-document matrix is very sparse
• Some terms are more useful than others
• Solution: The Inverted Index

--- Page 61 ---
I I
NVERTED NDEX
• How to store the words for fast lookup
• Basic steps:
• Make a “dictionary” of all the words in all of the web
pages
• For each word, list all the documents it occurs in.
• Often omit very common words
• “stop words”
• Sometimes stem the words
• (also called morphological analysis)
• cats -> cat
• running -> run

--- Page 62 ---
Inverted Index Example
Image from http://developer.apple.com
/documentation/UserExperience/Conceptual/SearchKitConcepts/searchKit_basics/chapter_2_section_2.html

--- Page 63 ---
I I
NVERTED NDEX
• This is the primary data structure for text
indexes
• Main Idea:
• Invert documents into a big index
• Basic steps:
• Make a “dictionary” of all the tokens in the
collection
• For each token, list all the docs it occurs in.
• Do a few things to reduce redundancy in the data
structure

--- Page 64 ---
A I I H
N NVERTED NDEX AS
T (D ) P
ERMS ICTIONARY AND OSTINGS
Term Postings
aid 4, 8
all 2, 4, 6
back 1, 3, 7
brown 1, 3, 5, 7
come 2, 4, 6, 8
dog 3, 5
fox 3, 5, 7
good 2, 4, 6, 8
jump 3
lazy 1, 3, 5, 7
men 2, 4, 8
now 2, 6, 8
over 1, 3, 5, 7, 8
party 6, 8
quick 1, 3
their 1, 5, 7
time 2, 4, 6

--- Page 65 ---
C P
REATING OSTINGS
1 2 34 56 78
Term c c c c c c c c Postings
o o oo oo oo
D D DD DD DD
aid 0 0 0 1 0 0 0 1 4, 8
all 0 1 0 1 0 1 0 0 2, 4, 6
back 1 0 1 0 0 0 1 0 1, 3, 7
brown 1 0 1 0 1 0 1 0 1, 3, 5, 7
come 0 1 0 1 0 1 0 1 2, 4, 6, 8
dog 0 0 1 0 1 0 0 0 3, 5
fox 0 0 1 0 1 0 1 0 3, 5, 7
good 0 1 0 1 0 1 0 1 2, 4, 6, 8
jump 0 0 1 0 0 0 0 0 3
lazy 1 0 1 0 1 0 1 0 1, 3, 5, 7
men 0 1 0 1 0 0 0 1 2, 4, 8
now 0 1 0 0 0 1 0 1 2, 6, 8
over 1 0 1 0 1 0 1 1 1, 3, 5, 7, 8
party 0 0 0 0 0 1 0 1 6, 8
quick 1 0 1 0 0 0 0 0 1, 3
their 1 0 0 0 1 0 1 0 1, 5, 7
time 0 1 0 1 0 1 0 0 2, 4, 6

--- Page 66 ---
W ?
HAT GOES IN THE POSTINGS
• Boolean retrieval
• Just the document number
• Ranked Retrieval
• Document number and term weight (tf.idf, ...)
• Proximity operators
• Word offsets for each occurrence of the term

--- Page 67 ---
U T I I T
SING HE NVERTED NDEX IN HE
R P
ETRIEVAL ROCESS
• During retrieval:
• Find the relevant postings based on query
terms
• Manipulate the postings based on the query
• Return appropriate documents

--- Page 68 ---
H ?
OW BIG ARE THE POSTINGS
• Very compact for Boolean retrieval
• About 10% of the size of the documents
• Not much larger for ranked retrieval
• Perhaps 20% of collection size
• Enormous for proximity operators
• Sometimes larger than the document
collection

--- Page 69 ---
F C I
URTHER OMPRESSING THE NDEX
• Postings can still be quite large
• Especially if you have a large collection
e.g., 1 million documents → 20-bit document numbers
• Idea: encode differences instead of document numbers
37, 42, 43, 48, 97, 98, 243 →
37, 5, 1, 5, 49, 1, 145
• Many other ways to compress the postings
• What about dropping unimportant terms from the index?
• How much space does stopword removal save?

--- Page 70 ---
D I I
ECOUPLING THE NVERTED NDEX
The term Index Postings
aid 4, 8
all 2, 4, 6
back 1, 3, 7
brown 1, 3, 5, 7
come 2, 4, 6, 8
dog 3, 5
fox 3, 5, 7
good 2, 4, 6, 8
jump 3
lazy 1, 3, 5, 7
men 2, 4, 8
now 2, 6, 8
over 1, 3, 5, 7, 8
party 6, 8
quick 1, 3
their 1, 5, 7
time 2, 4, 6

--- Page 71 ---
T C
ERMS IN THE OLLECTION
• Let’s focus on the term index
• The postings are relatively simple
• During indexing: once you find the correct postings, add
information from current document
• During retrieval: once you find the correct postings,
manipulate based on query operator
• Questions
• How do you find the correct posting quickly?
• What happens when you come across a new term?

--- Page 72 ---
Linear Dictionary Lookup
Suppose we want to find the word “complex”
relaxation
• How long does this take, in the worst
astronomical
zebra case?
belligerent
subterfuge • Running time is proportional to
daffodil
number of entries in the dictionary
cadence
wingman • This algorithm is O(n)
loiter
= linear time algorithm
peace
arcade
respondent
Found it!
complex
tax
kingdom
jambalaya

--- Page 73 ---
With a Sorted Dictionary
Let’s try again, except this time with a sorted dictionary: find “complex”
arcade
• How long does this take, in the worst
astronomical
belligerent case?
cadence
complex Found it!
daffodil
jambalaya
kingdom
loiter
peace
relaxation
respondent
subterfuge
tax
wingman
zebra

--- Page 74 ---
B S : A
INARY EARCH NALYSIS
• Algorithm:
• Look in the middle entry of a region, call this x
• If the entry you’re looking for comes before x, then look in first
half, otherwise look in second half
• Repeat until you find what you’re looking for
• Analysis:
• Each time we look up an entry, we cut down the number to
consider by a half
• How many times can you divide a number by 2?
• This algorithm is O(lg n)

--- Page 75 ---
W ?
HICH IS FASTER
• Two algorithms:
• O(n): Sequentially search through every entry
• O(lg n): Binary search
• Big-O notation
• Tells us the asymptotic worst case running time
of an algorithm
• Allows us to compare the speed of different
algorithms

--- Page 76 ---
N T
EXT IME
• Considerations for Ranking
• Boolean Ranking
• Vector Space Rankng


================================================================================
FILE: 23_relevance_ir_eval_boolean_ranking.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 23: IR evaluation, Boolean Queries, Search Ranking, Zifp’s Law

--- Page 2 ---
Today’s Outline
Relevance in Search
Ranking Evaluation
Boolean Search
TF-IDF, BM25, Zipfian Distribution

--- Page 3 ---
What’s the right answer for this query?

--- Page 4 ---
H R S R ?
OW TO ANK EARCH ESULTS
• Assume you have
• 300M web pages or
• 30M images or
• 300,000 resumes
• All of which match a query
• “best car”
• “beautiful”
• “software engineer”
• How do you decide which is most relevant? Second most?

--- Page 5 ---
W I R
HAT S ELEVANCE
( )?
FOR SEARCH RANKING
And how can we measure it?

--- Page 6 ---
R : A K T IR
ELEVANCE EY OPIC IN
In what ways can a document be relevant to a query?
• Answer precise question precisely.
• Who is buried in grant’s tomb? Grant.
• Partially answer question.
• Where is Berkeley? Near Oakland.
• Suggest a source for more information.
• What is lymphodema? Look in this Medical Dictionary.
• Give background information.
• Suggest an expert person
• Others ...

--- Page 7 ---
A D R
EFINITION OF ELEVANCE
Relevance is the measure of a correspondence
degree utility
dimension connection
estimate satisfaction
appraisal fit
relation bearing
matching
existing between a document and a query
article request
textual form information used
reference point of view
information provided information need statement
fact
as determined by person
judge
user
requester
Information specialist
Saracevic. (1975) Relevance: A Review of and a Framework for Thinking on the Notion JASIST, 26(6), 321-343;

--- Page 8 ---
T IR E S
YPES OF VALUATION TRATEGIES
• System-centered studies
• Given documents, queries, and relevance judgments
• Try several variations of the system
• Measure which system returns the “best” hit list
• User-centered studies
• Given several users, and at least two retrieval systems
• Have each user try the same task on both systems
• Measure which system works the “best”

--- Page 9 ---
W E ?
HAT TO VALUATE
• Effectiveness
• How “good” are the items that are returned?
• How “accurate” is the ranking? (precision & recall)
• Efficiency
• Retrieval time, indexing time, index size
• Usability
• Learnability, frustration
• Novice vs. expert users

--- Page 10 ---
A E M
UTOMATIC VALUATION ODEL
Query Documents
IR Opaque Box
Ranked List
Evaluation
Relevance Judgments
Module
Measure of Effectiveness

--- Page 11 ---
A Q 6 R .
SSUME YOU HAVE A UERY AND WAYS TO ANK IT
W T B ?
HICH OF HESE IS EST

--- Page 12 ---
P R A R
RECISION AND ECALL FOR SSESSING ANKING
• Precision
• proportion of retrieved material that is relevant
• Recall
• proportion of relevant material that is retrieved
• F-Measure
• Balances between the two

--- Page 13 ---
V A S
IEWING S A ET
Space of all documents
Recall:
Precision:
# Relevant that are
# Relevant that are
retrieved /
retrieved /
# Relevant
# Retrieved
Relevant +
Relevant Retrieved
Retrieved (purple /
(purple /
lavender + purple)
blue + purple)
“Are the all the
“Are all the retrieved
relevant items
items relevant?” Not Relevant + Not Retrieved
retrieved?”

--- Page 14 ---
Measuring Precision
6/20
# Relevant that are retrieved / # Retrieved

--- Page 15 ---
Measuring Recall
# Relevant that are retrieved / # Relevant

--- Page 16 ---
Measuring Precision and Recall
6/20

--- Page 17 ---
Typical Precision-Recall Tradeoff
Slide from Manning & Raghavan via Ricci

--- Page 18 ---
F-M :
EASURE
B P R
ALANCING RECISION AND ECALL
( )
2 +1 PR
F =
2P + R
• Harmonic mean of recall and precision
• Beta controls relative importance of precision and recall
• Beta = 1: precision and recall equally important
• Beta = 5: recall five times more important than precision
FYI: Harmonic mean is the reciprocal of the arithmetic mean
of the reciprocals of the given set of observations

--- Page 19 ---
Precision and Recall and F-measure
Beta controls relative importance:
Beta = 1, P and R equally important
Beta = 5, R five times more important than P
6/20
( )
2 +1 PR
F =
2P + R

--- Page 20 ---
H D S S ’
OW DO WE COMPARE IFFERENT EARCH YSTEMS
P -R C ?
RECISION ECALL URVES
1
0.9
0.8
0.7
n
o 0.6
i
s
i 0.5
c
e
r 0.4
P
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Adapted from a presentation by Ellen Voorhees at the University of Maryland, March 29, 1999

--- Page 21 ---
Mean Average Precision (MAP)
Compute precision at each position
Average the scores for only where there are
relevant document (the circled values)

--- Page 22 ---
S -V V P R
INGLE ALUED ERSIONS OF AND
• Precision at a fixed number of documents
• Precision at 10 docs is often useful for Web search
• R-precision
• Precision at r documents, where r is the total number
of relevant documents
• Expected search length
• Average rank of the first relevant document

--- Page 23 ---
B T C
UILDING EST OLLECTIONS
• Where do test collections come from?
• Someone goes out and builds them (expensive)
• Or as the byproduct of some project
• TREC = Text REtrieval Conferences
• Sponsored by NIST
• Series of annual evaluations, started in 1992
• Organized into “tracks”
• Larger tracks may draw a few dozen participants
See proceedings online at http://trec.nist.gov/

--- Page 24 ---
Ad Hoc Topics
(as opposed to monitoring type information needs)
In TREC, a statement of information need is called a topic
Sample TREC topic (number 312) from the TREC-6 ad hoc track (Voorhees and Harman, 2000)

--- Page 25 ---
H O E R J ?
OW TO BTAIN NOUGH ELEVANCE UDGMENTS
• Exhaustive assessment is usually impractical
• TREC has 50 queries
• Collection has >1 million documents
• Random sampling won’t work
• If relevant docs are rare, none may be found!
• IR systems can help focus the sample (pooling method)
• Each system finds some relevant documents
• Different systems find different relevant documents
• Together, enough systems will find most of them
• Leverages cooperative evaluations

--- Page 26 ---
P M F O R
OOLING ETHODOLOGY OR BTAINING ELEVANCE
J
UDGEMENTS
• Systems submit top 1000 documents per topic
• Top 100 documents from each are judged
• Single pool, duplicates removed, arbitrary order
• Judged by the person who developed the topic
• Treat unevaluated documents as not relevant
• Compute MAP down to 1000 documents
• To make pooling work:
• Systems must do reasonably well
• Systems must not all “do the same thing”
• Gather topics and relevance judgments to create a reusable test collection

--- Page 27 ---
L F TREC
ESSONS ROM
• Absolute scores are not trustworthy
• Who’s doing the relevance judgment?
• How complete are the judgments?
• Relative rankings are stable
• Comparative conclusions are most valuable
• Cooperative evaluations produce reliable test collections
• Evaluation technology is predictive

--- Page 28 ---
R : A E
ECAP UTOMATIC VALUATION
• Test collections focus on IR ranking
• Automatic evaluation is one shot
• Ignores the richness of human interaction
• Evaluation measures focus on one notion of performance
• But users care about other things
• Goal is to compare systems
• Values may vary, but relative differences are stable

--- Page 29 ---
H S E W
OW EARCH NGINES ORK
Three main parts:
i. Gather the contents of all web pages (using a
program called a crawler or spider)
ii. Organize the contents of the pages in a way that
allows efficient retrieval (indexing)
iii. Take in a query, determine which pages match, and
show the results (ranking and display of results)

--- Page 30 ---
Q L S T
UERY ANGUAGES AND EARCH YPES
• A way to express the information need
• Main Types
• Boolean
• Keyword / Natural Language
• Graphical User Interface (GUI)
Boolean: specify a precise set of results, not ranked
Keyword/ NL: fuzzier matching, ranking is key

--- Page 31 ---
B
OOLEAN
Q S
UERIES AND EARCH

--- Page 32 ---
S Q L : B
IMPLE UERY ANGUAGE OOLEAN
Consists of Terms + Connectors (operators)
A statement defines which documents to retrieve
• Terms: words, phrases, synonym expansions
• Connectors: AND, OR, NOT
• Also can have parentheses for grouping

--- Page 33 ---
M B Q
EANING OF OOLEAN UERIES
All (and only) documents
Cat
containing “cat” (at least once)
All (and only) documents
Cat OR Dog
containing either “cat” or “dog”
(or both)
Cat AND Dog All (and only) documents
containing both “cat” and “dog”
All (and only) documents
Cat AND NOT Dog
containing “cat” BUT NOT “dog”

--- Page 34 ---
S R B Q
ET EPRESENTATION FOR OOLEAN UERIES
All documents
A B
C

--- Page 35 ---
B OR
OOLEAN
All documents
All of the Lavender
B
0 1
A
Documents
0 0 1
A B
All of the Blue
1 1 1
Documents
A OR B
C All of the Purple
Documents
All (and only) documents
Cat OR Dog
containing either “cat” or “dog”
(or both)

--- Page 36 ---
B A
OOLEAN ND
All documents
B
0 1
A All of the
0 0 0 Documents at the
A B
Intersection of
1 0 1
Blue AND Green
A AND B
C
All (and only) documents
Cat AND Dog
containing BOTH “cat” AND “dog”

--- Page 37 ---
B NOT
OOLEAN
All documents
All of the Lavender
B
0 1 Documents (NOT
the purple part)
A B
1 0
NOT B All of the Green
Documents
C
All documents EXCEPT those that
Not Dog
contain Dog

--- Page 38 ---
C : A N B
OMBINATION AND OT
All documents
B All of the Lavendar
0 1
A
Documents (NOT
0 0 0
the purple part)
A B
1 1 0
A NOT B
C
(= A AND NOT B)
All documents with Cat EXCEPT
Cat AND NOT Dog
those that contain Dog

--- Page 39 ---
B Q R
OOLEAN UERIES AND ETRIEVAL
• Weights assigned to terms are either “0” or “1”
• “0” represents “absence”: term isn’t in the document
• “1” represents “presence”: term is in the document
• Build queries by combining terms with Boolean operators
• AND, OR, NOT
• The system returns all documents that satisfy the query

--- Page 40 ---
B V C
OOLEAN IEW OF A OLLECTION
1 23 45 678
Term c c c c c c c c
o oo oo ooo
D DD DD DDD
aid 0 0 0 1 0 0 0 1
Each column represents the
all 0 1 0 1 0 1 0 0
view of a particular document:
back 1 0 1 0 0 0 1 0
brown 1 0 1 0 1 0 1 0 What terms are contained in
come 0 1 0 1 0 1 0 1 this document?
dog 0 0 1 0 1 0 0 0
fox 0 0 1 0 1 0 1 0 Each row represents the view
good 0 1 0 1 0 1 0 1
of a particular term: What
jump 0 0 1 0 0 0 0 0
documents contain this term?
lazy 1 0 1 0 1 0 1 0
men 0 1 0 1 0 0 0 1
now 0 1 0 0 0 1 0 1 To execute a query, pick out
over 1 0 1 0 1 0 1 1 rows corresponding to query
party 0 0 0 0 0 1 0 1
terms and then apply the logic
quick 1 0 1 0 0 0 0 0
table of corresponding
their 1 0 0 0 1 0 1 0
time 0 1 0 1 0 1 0 0 Boolean operator
Slide adapted from Jimmy Lin

--- Page 41 ---
Representing Documents for Boolean Queries
1 2
Document 1 t t
n n
e e
m m
u u
Term c c
The quick brown o o
D D
fox jumped over
aid 0 1
the lazy dog’s
all 0 1 Stopword
back.
back 1 0 List
brown 1 0
come 0 1
for
dog 1 0
is
fox 1 0
of
Document 2 good 0 1
the
jump 1 0
to
lazy 1 0
Now is the time men 0 1
for all good men
now 0 1
to come to the
over 1 0
aid of their party.
party 0 1
quick 1 0
their 0 1
time 0 1
Slide adapted from Jimmy Lin

--- Page 42 ---
S Q
AMPLE UERIES
1 2 3 4 5 6 7 8
Term c c c c c c c c
o o o o o o o o
D D D D D D D D
dog 0 0 1 0 1 0 0 0
fox 0 0 1 0 1 0 1 0
dog  fox 0 0 1 0 1 0 0 0 dog AND fox → Doc 3, Doc 5
dog  fox 0 0 1 0 1 0 1 0 dog OR fox → Doc 3, Doc 5, Doc 7
dog  fox 0 0 0 0 0 0 0 0 dog NOT fox → empty
fox  dog 0 0 0 0 0 0 1 0 fox NOT dog → Doc 7
1 2 3 4 5 6 7 8
Term c c c c c c c c
o o o o o o o o
D D D D D D D D
good 0 1 0 1 0 1 0 1
party 0 0 0 0 0 1 0 1
g  p 0 0 0 0 0 1 0 1 good AND party → Doc 6, Doc 8
over 1 0 1 0 1 0 1 1
g  p  o 0 0 0 0 0 1 0 0 good AND party NOT over → Doc 6
Slide adapted from Jimmy Lin

--- Page 43 ---
C B O
OMBINATIONS OF OOLEAN PERATIONS
• Cat
• Cat OR Dog Disjunct
• Cat AND Dog Conjunct
• (Cat AND Dog) Conjunct
• (Cat AND Dog) OR (Collar AND Leash) Disjunct of Conjuncts
• (Cat OR Dog) AND (Collar OR Leash) Conjunct of Disjuncts

--- Page 44 ---
Converting Natural Language to Boolean
“Measurement of the Formal Query:
width of cracks in cracks AND beams
Cracks
prestressed AND Width_measurement
concrete beams” AND Prestressed_concrete
Beams Width_measurement
Relaxed Query:
Prestressed
concrete (C AND B AND P) OR
(C AND B AND W) OR
(C AND W AND P) OR
(B AND W AND P)

--- Page 45 ---
F B Q
ACETED OOLEAN UERY
Strategy: break query into facets
• Conjunction of disjunctions
a1 OR a2 OR a3
AND
b1 OR b2
c1 OR c2 OR c3 OR c4

--- Page 46 ---
F B Q
ACETED OOLEAN UERY
Strategy: break query into facets
• Conjunction of disjunctions
a1 OR a2 OR a3
b1 OR b2
AND
c1 OR c2 OR c3 OR c4
• Each facet expresses a topic
“rain forest” OR jungle OR amazon
medicine OR remedy OR cure
AND across each line
Smith OR Zhou

--- Page 47 ---
Faceted Navigation Supports a Conjunct of Disjuncts
Exercise: what is the Boolean
query expressing these results?

--- Page 48 ---
Faceted Navigation Supports a Conjunct of Disjuncts
Show all products that are:
Nightstands AND
(brown OR gold) AND
($100-250 OR $500-750) AND
(Medium OR Expresso OR Gray)

--- Page 49 ---
O R D
RDERING OF ETRIEVED OCUMENTS
Pure Boolean Search has no ordering
In practice:
• Order chronologically (by time)
• Order by total number of “hits” on query terms
• What if one term has more hits than others?
• Is it better to one of each term or many of one term?
• Use some other statistical properties

--- Page 50 ---
P S
ROXIMITY EARCHES
• Proximity: terms occur within K positions of one
another
• pen w/5 paper
• A “Near” function can be more vague
• near(pen, paper)
• Sometimes order can be specified
• Also, Phrases and Collocations
• “United Nations” “Barack Obama”
• Phrase Variants
• “retrieval of information” “information retrieval”

--- Page 51 ---
Boolean Proximity
Queries Are Still Heavily
Used in Legal Search
Prototype of search
For patent examiners:
Note use of “same” operator
Meaning same paragraph
https://wiki.piug.org/pages/viewpage.action?pageId=33850015

--- Page 52 ---
patents.google.com

--- Page 53 ---
S : B S
UMMARY OOLEAN EARCHING
• Advantages
• simple queries are easy to understand
• relatively easy to implement
• Disadvantages
• difficult to specify what is wanted (have to state synonyms)
• too much returned, or too little
• ordering not well determined
• Dominant language in commercial systems until the WWW
• Still used quite a lot in legal searches
• Still heavily used in UIs in website search (faceted navigation)

--- Page 54 ---
R A
ANKING LGORITHMS

--- Page 55 ---
R R
ANKED ETRIEVAL
Order documents by how likely they are to be
relevant to the information need

--- Page 56 ---
R
ESULTS RANKING
• Search engine receives a query, then
• Looks up the words in the index, retrieves many documents, then
• Rank orders the pages and extracts “snippets” or summaries containing
query words.
• These are complex algorithms
Slide adapted from Lew & Davis

--- Page 58 ---
Dataset: State Governor Speeches
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 59 ---
Document frequencies follow a power law
(Zipf’s curve)
Exercise:
Which terms most
distinguish a relevant
document?
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 60 ---
W
ORD
F
REQUENCIES
L
HAVE ONG
T
AILS
Zipf’s Law
Zdeněk Macháček on Unsplash

--- Page 61 ---
Z ’ L
IPF S AW
(
E B , G K.
MPIRICAL EHAVIOR NOTED BY LINGUIST EORGE
Z )
IPF
For many phenomena, there is a relationship (a power law) between
the frequency of an event and the rank of that frequency among all
events.
• Token occurrences in text are not uniformly distributed
• They are also not normally distributed
• They do exhibit a Zipf distribution

--- Page 62 ---
W K D
HAT INDS OF ATA
E Z D ?
XHIBIT A IPF ISTRIBUTION
• Words in a text collection
• Social Network Popularity
• Website Popularity
• Document Size on Web
• And many many other phenomena like this

--- Page 63 ---
Book Sales, Ordered by
Rank
(Most Popular First)
r = rank (x-axis). Most popular is ranked #1
f = frequency (y-axis) The # items in the rank

--- Page 64 ---
Data Type: City Population

--- Page 65 ---
A W -
SSIGNING EIGHTS WITH TF IDF
Intuition:
Terms that appear often in a document should get higher weight
BUT terms that appear in many documents should get lower weights
This is linked to the fact that terms frequencies follow a power law (Zipf’s curves)
TF x IDF measure:
term frequency (tf)
inverse document frequency (idf)
Assign a TF x IDF weight to each term in each document
There are many ways to compute this measure
The reading shows BM-25 (also called Okapi); this is now frequently used

--- Page 66 ---
TF-IDF T W : S , Y
ERM EIGHTING IMPLE ET
E !
FFECTIVE
N
This is the idf part
w = tf log
i,j i,j
n
i
w
weight assigned to term i in document j
i, j
tf
number of occurrences of term i in document j
i, j
N
number of documents in entire collection
n
number of documents with term i
i
log(N/n) = log(N) – log (n)

--- Page 67 ---
Inverse Document Frequency (IDF)
(here called Collection Frequency Weight)
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 68 ---
Computing BM-25 (also called Okapi)
(This is still a very popular method)
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 69 ---
Computing BM-25
Normalize weight
of term i across
documents
log(N/n) = log(N) – log (n)
# terms in d(j)
(doc length)
Normalizes across all
document lengths
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 70 ---
C BM-25
OMPUTING
• K1 modifies the importance of term frequency
• Higher values would increase the influence of TF; K1=0 eliminates the influence
altogether.
• b modifies document length (between 0 and 1)
• “Setting b towards 1, e.g. b=.75, will reduce the effect of term frequency on the ground that it is
primarily attributable to verbosity. If b=0 there is no length adjustment effect, so greater length
counts for more” (perhaps the doc has multiple topics)
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 71 ---
Computing BM-25
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 72 ---
Computing BM-25
Query=“hurricane” Query=“hurricane Florence”
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 73 ---
S A R
OME DDITIONAL ANKING CRITERIA
• For a given candidate result page, use:
• Number of matching query words in the page
• Proximity of matching words to one another
• Location of terms within the page
• Location of terms within tags e.g. <title>, <h1>, link text, body
text
• Anchor text on pages pointing to this one
• Frequency of terms on the page and in general
• Link analysis of which pages point to this one
• (Sometimes) Click-through analysis: how often the page is
clicked on
• How “fresh” is the page
• Complex formulae combine these together.
• Combining these via machine learning started in the
2000’s
Slide adapted from Lew & Davis

--- Page 74 ---
M I
EASURING THE MPORTANCE OF
L
INKING
• PageRank Algorithm
• Idea: important pages are pointed to by other important pages
• Method:
• Each link from one page to another is counted as a “vote” for the destination
page
• But the importance of the starting page also influences the importance of the
destination page.
• And those pages scores, in turn, depend on those linking to them.
Image and explanation from http://www.economist.com/science/tq/displayStory.cfm?story_id=3172188

--- Page 75 ---
M I
EASURING THE MPORTANCE OF
L
INKING
P R
AGE ANK
• Example: each page starts with 100 points.
• Each page’s score is recalculated by adding up
the score from each incoming link.
• This is the score of the linking page divided
by the number of outgoing links it has.
• Example: the page in green has 2 outgoing
links and so its “points” are shared evenly by
the 2 pages it links to.
• Keep repeating the updates until no more
changes.
Image and explanation from http://www.economist.com/science/tq/displayStory.cfm?story_id=3172188

--- Page 76 ---
M R
ANIPULATING ANKING
• Motives
• Commercial, political, religious
• Promotion funded by advertising budget
• Operators
• Search Engine Optimizers
• Web masters
• Hosting services
• Forum and Websites
• Web master world ( www.webmasterworld.com )
• Searchengineland.com
Slide adapted from Manning, Raghavan, & Schuetze

--- Page 77 ---
A
FEW SPAM TECHNOLOGIES
• Cloaking
• Serve fake content to search engine robot
• DNS cloaking: Switch IP address. Impersonate
Cloaking
• Doorway pages SPAM
Y
• Pages optimized for a single keyword that re-direct to
the real target page
Is this a Search
• Keyword Spam Engine spider?
• Misleading meta-keywords, excessive repetition of a
term, fake “anchor text” Real
N
• Hidden text with colors, CSS tricks, etc. Doc
• Link spamming
• Mutual admiration societies, hidden links, awards
• Domain flooding: numerous domains that point or re-
direct to a target page
• Robots
Meta-Keywords =
• Fake click stream “… London hotels, hotel, holiday inn, hilton,
• Fake query stream discount, booking, reservation, sex, mp3,
• Millions of submissions via Add-Url britney spears, viagra, …”
Slide adapted from Manning, Raghavan, & Schuetze

--- Page 78 ---
P
AID RANKING
Pay-for-inclusion
• Deeper and more frequent indexing
• Sites are not distinguished in results display
Paid placement
• Keyword bidding for targeted ads
Slide adapted from Lew & Davis

--- Page 79 ---
S
UMMARY
• Document ranking is complex; we’ve only scratched
the surface and talked about classic approaches
• Machine learning using click data has become
dominant on the web (pre-LLM)
• That data isn’t available however for other search
(on your desktop, in your organization, etc)


================================================================================
FILE: 24_tokenization_rag_llms.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 24: Ranking Criteria, Tokenization, LLMs, RAG

--- Page 2 ---
Today’s Outline
Finish Search Ranking
LLMs
Tokenization
Dense Vectors
RAG

--- Page 3 ---
H S E W
OW EARCH NGINES ORK
Three main parts:
i. Gather the contents of all web pages (using a
program called a crawler or spider)
ii. Organize the contents of the pages in a way that
allows efficient retrieval (indexing)
iii. Take in a query, determine which pages match, and
show the results (ranking and display of results)

--- Page 4 ---
L T
AST IME
• Search Ranking Evaluation
• Boolean Search
• Statistical Search with Ranking
• Zipf’s law
• TF-IDF
• BM-25

--- Page 5 ---
S A R
OME DDITIONAL ANKING CRITERIA
• For a given candidate result page, use:
• Number of matching query words in the page
• Proximity of matching words to one another
• Location of terms within the page
• Location of terms within tags e.g. <title>, <h1>, link text, body text
• Anchor text on pages pointing to this one
• Frequency of terms on the page and in general
• Link analysis of which pages point to this one
• (Sometimes) Click-through analysis: how often the page is clicked on
• How “fresh” is the page
• Complex formulae combine these together.
• Combining these via machine learning started in the 2000’s
Slide adapted from Lew & Davis

--- Page 6 ---
M I L
EASURING THE MPORTANCE OF INKING
PageRank Algorithm
• Idea: important pages are pointed to by other important pages
• Method:
• Each link from one page to another is counted as a “vote” for the destination page
• But the importance of the starting page also influences the importance of the
destination page.
• And those pages scores, in turn, depend on those linking to them.
Image and explanation from http://www.economist.com/science/tq/displayStory.cfm?story_id=3172188

--- Page 7 ---
M I L
EASURING THE MPORTANCE OF INKING
P R
AGE ANK
• Example: each page starts with 100 points.
• Each page’s score is recalculated by adding up the
score from each incoming link.
• This is the score of the linking page divided by
the number of outgoing links it has.
• Example: the page in green has 2 outgoing links
and so its “points” are shared evenly by the 2
pages it links to.
• Keep repeating the updates until no more changes.
Image and explanation from http://www.economist.com/science/tq/displayStory.cfm?story_id=3172188

--- Page 8 ---
M R
ANIPULATING ANKING
• Motives
• Commercial, political
• Promotion funded by advertising budget
• Operators
• Search Engine Optimizers
• Web masters
• Hosting services
• Forum and Websites
• Web master world
• Searchengineland.com
Slide adapted from Manning, Raghavan, & Schuetze

--- Page 9 ---
A
FEW SPAM TECHNOLOGIES
•
Cloaking
• Serve fake content to search engine robot
• DNS cloaking: Switch IP address. Impersonate
Cloaking
• Doorway pages SPAM
Y
• Pages optimized for a single keyword that re-direct to the
real target page
Is this a Search
• Keyword Spam Engine spider?
• Misleading meta-keywords, excessive repetition of a term,
fake “anchor text” Real
N
• Hidden text with colors, CSS tricks, etc. Doc
• Link spamming
• Mutual admiration societies, hidden links, awards
• Domain flooding: numerous domains that point or re-direct
to a target page
• Robots
Meta-Keywords =
• Fake click stream “… London hotels, hotel, holiday inn, hilton,
• Fake query stream discount, booking, reservation, sex, mp3,
• Millions of submissions via Add-Url britney spears, viagra, …”
Slide adapted from Manning, Raghavan, & Schuetze

--- Page 10 ---
P
AID RANKING
Pay-for-inclusion
• Deeper and more frequent indexing
• Sites are not distinguished in results display
Paid placement
• Keyword bidding for targeted ads
Slide adapted from Lew & Davis

--- Page 11 ---
S
UMMARY
• Document ranking is complex; we’ve only scratched the
surface and talked about classic approaches
• Machine learning using click data has become dominant
on the web (pre-LLM)
• That data isn’t available however for other search (on
your desktop, in your organization, etc)

--- Page 12 ---
T - L
RANSFORMER BASED ANGUAGE
M
ODELS

--- Page 13 ---
L M
ANGUAGE ODELING
• Goal: given a sequence of words, predict the next one
• Input: a fragment of a text (a prompt)
• Output: some more text
• When trained at scale with a special architecture (the transformer
model with many layers), becomes very good at generating text.
• In a sense, a large language model is a compression of a massive
amount of text input.

--- Page 14 ---
Language Modeling Training in Action
https://www.nytimes.com/interactive/2023/04/26/upshot/gpt-from-scratch.html

--- Page 15 ---
Language Modeling Training in Action
(does this remind you of
anything from lecture 1?)
https://www.nytimes.com/interactive/2023/04/26/upshot/gpt-from-scratch.html

--- Page 16 ---
Language Modeling Training in Action
https://www.nytimes.com/interactive/2023/04/26/upshot/gpt-from-scratch.html

--- Page 17 ---
LLM Pretraining
Pretraining is the act of training a model from scratch: the weights are
randomly initialized, and the training starts without any prior knowledge.
https://huggingface.co/learn/llm-course/en/chapter1/4

--- Page 18 ---
LLM Fine Tuning
Fine-tuning is the training done after a model has been pretrained.
First acquire a pretrained language model, then perform additional
training with a dataset specific to a task.
https://huggingface.co/learn/llm-course/en/chapter1/4

--- Page 19 ---
T M
RANSFORMER ODEL
• The transformer model is primarily composed of two blocks:
• Encoder (left): The encoder receives an input and builds a
representation of it (its features).
• Decoder (right): The decoder uses the encoder’s
representation (features) along with other inputs to generate
a target sequence.
https://huggingface.co/learn/llm-course/en/chapter1/4

--- Page 20 ---
T T M
YPES OF RANSFORMER ODELS
Language models generally fall into three architectural categories:
• Encoder-only models(like BERT):
• Use a bidirectional approach to understand context from both directions.
• Best for NLP tasks like classification, named entity recognition, and similarity comparison.
• Decoder-only models(like GPT, Llama):
• Process text from left to right and are particularly good at text generation tasks.
• Complete sentences, write essays, or generate code based on a prompt.
• Encoder-decoder models(like T5, BART):
• Combine both approaches, using an encoder to understand the input and a decoder to
generate output.
• For sequence-to-sequence tasks like translation, summarization.
https://huggingface.co/learn/llm-course/en/chapter1/5

--- Page 21 ---
Full Transformer Architecture
https://huggingface.co/learn/llm-course/en/chapter1/5

--- Page 22 ---
BERT
(Bi-directional Encoder Representations from
Transformers)
BERT takes a sequence of words as input which keep flowing up the
stack. Each layer applies self-attention, and passes its results through
a feed-forward network, and then hands it off to the next encoder.
In the last layer, each position outputs a vector
https://jalammar.github.io/illustrated-bert/

--- Page 23 ---
Reinforcement Learning with Human
Feedback
The raw output of
an LLM isn’t that
great as a user
interface.
RLHF is used to
improve
conversation,
reduce bias,
protect against
dangerous
queries, etc.
https://huggingface.co/blog/rlhf

--- Page 24 ---
Reinforcement Learning with Human Feedback (RLHF)
https://www.labellerr.com/blog/reinforcement-learning-from-human-feedback/

--- Page 25 ---
Reinforcement Learning with Human Feedback (RLHF)
https://www.labellerr.com/blog/reinforcement-learning-from-human-feedback/

--- Page 26 ---
Reinforcement Learning with Human Feedback (RLHF)
https://www.labellerr.com/blog/reinforcement-learning-from-human-feedback/

--- Page 27 ---
Putting an LLM Into Production
SFT: supervised fine-tuning
https://www.labellerr.com/blog/reinforcement-learning-from-human-feedback/

--- Page 28 ---
T
OKENIZATION
Reading from Chapter 2 of Introduction to Information Retrieval goes into
detail

--- Page 29 ---
H ?
OW MANY WORDS IN A SENTENCE
• "I do uh main- mainly business data processing"
• Fragments, filled pauses
• "Seuss’s cat in the hat is different from other
cats!"
• Lemma: same stem, part of speech, rough word
sense
• cat and cats = same lemma
• Wordform: the full inflected surface form
• cat and cats = different wordforms

--- Page 30 ---
I T
SSUES IN OKENIZATION
• Can't just blindly remove punctuation:
• m.p.h., Ph.D., AT&T, cap’n
• prices ($45.55)
• dates (01/02/06)
• URLs (http://www.stanford.edu)
• hashtags (#nlproc)
• email addresses (someone@cs.colorado.edu)
• Clitic: a word that doesn't stand on its own
• "are" in we're, French "je" in j'ai, "le" in l'honneur
• When should multiword expressions (MWE) be
words?
• New York, rock ’n’ roll

--- Page 31 ---
H
OW TO DO WORD TOKENIZATION IN
C ?
HINESE
•姚明进入总决赛 “Yao Ming reaches the finals”
•3 words?
•姚明 进入 总决赛
•YaoMing reaches finals
•5 words?
•姚 明 进入 总 决赛
•Yao Ming reaches overall finals
•7 characters? (don't use words at all):
•姚 明 进 入 总 决 赛
•Yao Ming enter enter overall decision game
Slide from Jurafsky

--- Page 32 ---
H H A ?
OW TO ANDLE LL THIS
• Some embeddings use character
representations
• Today, LLMs often use subword encodings
• Byte Pair Encoding
(Sennrich et al. 2016)
• WordPiece
(Schuster & Nakajima 2012)

--- Page 33 ---
Byte-Pair Encoding
Gemini

--- Page 34 ---
B P E
YTE AIR NCODING
First, form the base vocabulary (all characters that occur in the training data)
https://www.cs.umd.edu/~miyyer/cs685/slides/tokenization.pdf

--- Page 35 ---
B P E
YTE AIR NCODING
Next, count the frequency of each character pair in the data, and choose the
one that occurs most frequently
https://www.cs.umd.edu/~miyyer/cs685/slides/tokenization.pdf

--- Page 36 ---
B P E
YTE AIR NCODING
Next, choose the most common pair (ug) and then merge the characters together into
one symbol. Add this new symbol to the vocabulary. Then, retokenize the data, and
repeat.
https://www.cs.umd.edu/~miyyer/cs685/slides/tokenization.pdf

--- Page 37 ---
B P E
YTE AIR NCODING
Eventually, after a fixed number of merge steps, stop
https://www.cs.umd.edu/~miyyer/cs685/slides/tokenization.pdf

--- Page 38 ---
Exercise: See How LLMs Tokenize
Text
https://tiktokenizer.vercel.app/

--- Page 39 ---
R A
ETRIEVAL UGMENTED
G
ENERATION

--- Page 40 ---
W D LLM S T N
HY O EARCH OOLS EED
RAG?
• Reduce hallucinations
• Show proof for what is being claimed
• Give credit for where the info came from
Link to
supporting
resources

--- Page 41 ---
Standard AI Chatbots vs RAG
Systems
Mahajan, Retrieval-augmented generation: The technical foundation of intelligent AI Chatbots, World Journal of Advanced Research and Reviews, 26(1), 2025

--- Page 42 ---
Standard AI Chatbots vs RAG
Systems
Mahajan, Retrieval-augmented generation: The technical foundation of intelligent AI Chatbots, World Journal of Advanced Research and Reviews, 26(1), 2025

--- Page 43 ---
Basic RAG
https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6

--- Page 44 ---
L ’ R V
ET S EVISIT ECTOR
E
MBEDDINGS

--- Page 45 ---
W V D
ORD ECTORS AND OCUMENT
V
ECTORS
• In a previous lecture, we saw how words can be
represented as vectors (embeddings)
• Vectors represent the contexts that words appear in
• We computed similarity between words by using
cosine similarity
• Documents can also be represented as vectors
• One approach: Represent each document as a
vector of its tf-idf term weights
• Compare document similarity using cosine similarity

--- Page 46 ---
Computing Word Embeddings with
Distributions Makes a Richer
Representation
Think of the colors as showing complex nuance
about which words have appeared in the same
context
These are real numbers instead of frequency
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/
counts

--- Page 47 ---
Plotting Vectors on a 2D Graph
fast
large
small A vector is not just a point on a graph;
it's the path from the center to that
point, defined by its components.
slow X-axis: a scale from Large (+X) to Slow (-X)
Y-axis: a scale from Fast (+Y) to Slow (-Y)
(0,0) is the “average” word

--- Page 48 ---
D V S
OCUMENTS IN ECTOR PACE
t
3
D
1
D
9
D
11
D
D 5
3
D
10
D D
4 2
t
1
D
7
D
t D 8 6
2
Assumption: Documents that are “close together”
in space are similar in meaning.

--- Page 49 ---
V S M
ECTOR PACE ODEL
• Documents are represented as vectors in term space
• Queries represented the same as documents
• Query and Document weights are based on length and
direction of their vector
• A vector distance measure (similarity measure) between the
query and documents is used to rank retrieved documents

--- Page 50 ---
Computing Similarity Scores to Determine
Which Documents are Close to the Query
D = (0.8, 0.3)
1
D = (0.2, 0.7)
2
Q = (0.4, 0.8)
1.0 Q
D
cos = 0.74
2
1
0.8
cos = 0.98
 2
0.6
2
0.4
 D
1 1
0.2
0.2 0.4 0.6 0.8 1.0

--- Page 51 ---
Can also use
document vectors
to computer
Similarity
Here, for State-of-
the-State
Speeches
Via BM-25 vectors
https://observablehq.com/@kerryrodden/introduction-to-text-analysis-with-tf-idf

--- Page 52 ---
TF-IDF ( ) D V
SPARSE VS ENSE ECTORS
• Tf-idf vectors: fast to compute, storage is smaller because vectors are
sparse
• BUT they require exact matches between query terms and documents
• LLMs / word embeddings allow matching of concepts even if the words
used differ between query and document
• These “dense vectors” are much more powerful for similarity search
• BUT this is much more complex to compute, store, and retrieve

--- Page 53 ---
D V R
ENSE ECTORS FOR EPRESENTING
P
ARAGRAPHS
• Start with a word embedding model like BERT
• Then train new vector embeddings:
• Get pairs of paragraphs known to be similar/different
• Use contrastive learning to train an encoder model so
that:
• Positive pairs have high cosine similarity
• Negative pairs have low cosine similarity
• Get positive/negative pairs from click logs, QA datasets,
etc

--- Page 54 ---
R D V
ETRIEVING WITH ENSE ECTORS
• Have to do a nearest-neighbor search to find
relevant vectors for the query
• Many specialized indexes and vector stores
have been developed to speed this up

--- Page 55 ---
Ranking Text “Chunks”
https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6

--- Page 56 ---
Index into Vector Store with Summary
Vectors
https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6

--- Page 57 ---
RAG
Gao et al., Retrieval-Augmented Generation for Large Language Models: A Survey, https://arxiv.org/abs/2312.10997 2023

--- Page 58 ---
D P S V
ENSE LUS PARSE ECTORS
• Because dense (semantic) and sparse (lexical) vectors have
complementary strengths, RAG systems often use Hybrid Search.
• This involves:
1.Generating both a dense vector (for meaning) and a sparse
vector (like BM25, for exact keywords) for the query.
2.Retrieving documents based on both the vector similarity
search (FAISS) and the BM25 score.
3.Combining the results using a weighted formula to get the best
of both worlds.
From Gemini

--- Page 59 ---
Hybrid Search Uses both
Dense and Sparse (BM-25) Vectors
https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6

--- Page 60 ---
Agent-based Models and RAG
https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6

--- Page 61 ---
Implementation Considerations
HNSW: Hierarchical Navigable Small World graph-based index
Mahajan, Retrieval-augmented generation: The technical foundation of intelligent AI Chatbots, World Journal of Advanced Research and Reviews, 26(1), 2025

--- Page 62 ---
S : LLM RAG
UMMARY S AND
• LLMs generate text to follow a prompt
• They are a super-summary of the text they’ve seen
• They can make up information
• RAG is a method for anchoring what they generate to
original sources
• However, it can be difficult to get RAG to rank passages
well


================================================================================
FILE: 25_misinformation.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 25: Misinformation & Disinformation

--- Page 2 ---
H S / D ?
OW ARE THESE IMILAR IFFERENT
• Rumors
• Misinformation
• Disinformation

--- Page 3 ---
M
ISINFORMATION
AND
D
ISINFORMATION
Waldemar Brandtonunsplash

--- Page 4 ---
Unofficial and unverified
stories about events that
Rumors
may turn out to be true or
false.
Slide adapted from Jevin West

--- Page 5 ---
Information that is false,
Misinformation
but not necessarily
intentionally false.
Slide adapted from Jevin West

--- Page 6 ---
False or misleading
information that is
purposefully seeded and/or
Disinformation
spread for a specific
objective — e.g., financial,
political, or reputational.
Slide adapted from Jevin West

--- Page 7 ---
Often built around a kernel
of truth, but layered with
distortions or exaggerations.
Disinformation
Functions as a campaign —
as set of information actions
— rather than a single piece
of content.
Slide adapted from Jevin West

--- Page 8 ---
Mis vs DisInformation:
Motives Matter
MISINFORMATION DISINFORMATION
Information that is false,but not False and misleading information that is purposely
necessarily intentionally false. seeded and/or spread for a specific objective.
● ● ● ●
Uncertainty To protect people Seed and spread Political objective
● ● ●
Out of fear A display of chaos Undermine trust
● ●
To inform people identity Monetary gains among the public
Slide adapted from Jevin West

--- Page 9 ---
https://firstdraftnews.org/articles/fake-news-complicated/

--- Page 10 ---
Types of Mis/Disinformation
Slide adapted from Jevin West

--- Page 11 ---
Types of Mis/Disinformation
Slide adapted from Jevin West

--- Page 12 ---
Types of Mis/Disinformation
Slide adapted from Jevin West

--- Page 13 ---
Types of Mis/Disinformation
Slide adapted from Jevin West

--- Page 14 ---
Types of Mis/Disinformation
Slide adapted from Jevin West

--- Page 15 ---
Spreading Mis/DisInformation –Trolls (Impersonators)
Starbird et al., Disinformation as Collaborative Work, CHI 2019
Slide adapted from Jevin West

--- Page 16 ---
S
TUDY OF
U V “A -M ”
SE OF ISUALIZATION BY NTI ASKERS
• Analyzed Facebook and Twitter posts with grounded coding
“This paper investigates how pandemic visualizations circulated on social
media, and shows that people who mistrust the scientific establishment
often deploy the same rhetoric of data-driven decision-making used by
experts, but to advocate for radical policy changes.”
https://vis.csail.mit.edu/covid-story/
Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data Practices to Promote Unorthodox Science Online, Lee et al., CHI 2021

--- Page 17 ---
W AI?
HAT ABOUT
https://deepdreamgenerator.com/#gallery

--- Page 18 ---
Automatically Generated Text
With no training, crowd workers were no
better than chance at guessing if
paragraphs generated by human or GPT-
3 model
Clark et al., All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text, ACL’21

--- Page 19 ---
Automatically Generated Text
With training, accuracy improves
slightly to 55%
Clark et al., All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text, ACL’21

--- Page 20 ---
A G T
UTOMATICALLY ENERATED EXT
• Why might this be a misinfo problem? Or is it not?
• Is it hard for humans to write convincing
misinformation without the help of programs?

--- Page 21 ---
https://www.theguardian.com/commentisfree/2023/mar/27/pope-coat-ai-image-baby-boomers

--- Page 22 ---
AI-based Media Manipulation (Deep Fakes)
Slide adapted from Jevin West

--- Page 23 ---
AI-based Media Manipulation (Deep Fakes)
Slide adapted from Jevin West

--- Page 24 ---
AI-based Media Manipulation (Deep Fakes)
Slide adapted from Jevin West

--- Page 25 ---
H C
OW TO OMBAT
M ?
ISINFORMATION
Nijwam Swargiary from unsplash

--- Page 26 ---
S :
TRATEGY
C L
ONTENT ABELS

--- Page 28 ---
https://about.fb.com/news/2019/12/combatting-misinformation-on-instagram/

--- Page 29 ---
https://about.fb.com/news/2021/05/taking-action-against-people-who-repeatedly-share-misinformation/

--- Page 30 ---
Counter-Measure: Fact Checking Web Sites

--- Page 31 ---
“R C ” S
UMOR ASCADES TUDY
• Study done on Facebook in 2013
• “Rumors” categorized as true, false, or maybe
• How do they spread in social networks with “share” buttons?
• What happens when someone links to a fact-checking cite like Snopes.com?
A sample of 249,035 comments on either photos or shares of
photos, posted during July and August 2013 and containing a valid
link to a rumor covered by Snopes
Tagged 16,672 individual cascades, containing 62,497,651 shares.
Rumor cascades, Friggeri et al, AAAI ICWSM 2014

--- Page 32 ---
Rumor cascades, Friggeri et al, AAAI ICWSM 2014

--- Page 33 ---
Rumor cascades, Friggeri et al, AAAI ICWSM 2014

--- Page 34 ---
Evidence that rapid fact-checking can help
mitigate false rumors (a bit)
Rumor cascades, Friggeri et al, AAAI ICWSM 2014

--- Page 35 ---
S :
TRATEGY
D /P
E REBUNKING

--- Page 37 ---
S F C M
TRATEGIES OR OMBATING ISINFROMATION
• Standard approach: debunking
• Can help, but there are barriers
• Can backfire, but there is increasing evidence that this is
not as much of an issue as once thought
• Newer, more successful approach: inoculation
• Expose people to weakened versions of the
misinformation
• Slowly build up cognitive resistance
• Prophylactic vs Therapeutic (for pre-existing views)
Lewandowsky et al., The Debunking Handbook 2020

--- Page 38 ---
P
REBUNKING
• Prevention (Prebunking)
• The most effective strategy is to prevent misinformation from taking root in the first place. This is
achieved through a technique called inoculation or prebunking.
• This involves:
• Forewarning people that they may be exposed to misinformation.
• Explaining the manipulative tactics or misleading argumentation strategies used by those
who spread false information (e.g., ad hominem attacks, false analogies, emotional appeals).
• By exposing people to a weakened dose of the techniques, they cultivate "cognitive antibodies"
that make them resilient to subsequent manipulation attempts.

--- Page 39 ---
P /D M
RE EBUNKING ETHOD
• Replace a myth with a clear, simple factual alternative
• Avoid reinforcing the myth through repetition or complexity,
• Use visuals to strengthen understanding
• Inoculate people against misleading tactics
• Delivered this with respectful communication.

--- Page 42 ---
A P V
NIMATED REBUNKING IDEOS
https://inoculation.science/inoculation-videos/

--- Page 43 ---
Debunking + AI

--- Page 44 ---
U AI D
SING TO EBUNK
• Hypothesized that interventions based on factual, corrective information may seem ineffective
simply because they lack sufficient depth and personalization.
• To test this, used an LLM chatbot to generate bespoke arguments.
• Study design:
• 2190 Americans articulated—in their own words—a conspiracy theory in which they believe,
along with the evidence they think supports this theory.
• Then engaged in a three-round conversation with the LLM GPT-4 Turbo,
• Prompted to respond to this specific evidence while trying to reduce participants’ belief in the
conspiracy
• Control condition: discuss an unrelated topic
• Results:
• Reduced participants’ belief in their chosen conspiracy theory by 20% on average.
Durably reducing conspiracy beliefs through dialogues with AILinks to an external site., Costello, Pennycock, & Rand, Science 2024 385 (6714)

--- Page 45 ---
U AI D
SING TO EBUNK
Exercise: Check out some conversations:
https://tinyurl.com/5n6kruc7
Durably reducing conspiracy beliefs through dialogues with AILinks to an external site., Costello, Pennycock, & Rand, Science 2024 385 (6714)

--- Page 46 ---
S :
TRATEGY
L R
EGISLATION AND EGULATION

--- Page 48 ---
H E R
OW LSE TO ESPOND TO
M /D ?
IS ISINFORMATION


================================================================================
FILE: 26_fairness_bias.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 26: Fairness and Bias in Automated Systems

--- Page 2 ---
F P
INAL ROJECTS
• Updated Rubric Posted
• Pushed back the due date from Wed Dec 10 to Sat
Dec 13!

--- Page 3 ---
Today’s Outline
What is Fairness / Bias?
Focus: Dataset Creation
Focus: ML Algoritms
Fairness / Bias in Search Ranking

--- Page 4 ---
Two Readings

--- Page 5 ---
W F ?
HAT IS AIRNESS
What does it mean for an algorithm/system to be “fair” or “unfair”?

--- Page 6 ---
M C R T
ANY LOSELY ELATED OPICS
• Fairness in ranking
• (job applicants, political search results)
• Search results representativeness
• (exposure of diverse information)
• Fairness in automated classification
• (recognizing demographics of people in images)
• Fairness in application of algorithms to real-world tasks
• (recidivism risk assessments, assessing teachers, recommending advertisements)
• Bias in generative language models
• (story generation, image generation)

--- Page 7 ---
S L D
OME EGAL EFINITIONS
• Protected groups: Groups that are protected from discrimination by law, e.g.,
based on sex, race, age, disability, color, creed, national origin, or religion.
• Disparate Treatment: Intentional discrimination, where people in a protected
class are deliberately treated differently.
• Disparate Impact: Unintentional discrimination, where the procedures are the
same for everyone, but people in a protected class are negatively affected.
https://rayneslaw.com/what-is-the-difference-between-disparate-impact-and-disparate-treatment-discrimination/

--- Page 8 ---
W B ?
HAT IS IAS
What does it mean for an algorithm/system to be “biased” or “unbiased”?

--- Page 9 ---
W B ?
HAT IS IAS
Bias: “computer systems that systematically and unfairly discriminate against certain individuals or
groups of individuals in favor of others.”
• Preexisting bias (arising from biases present in individuals or society),
• Technical bias (arising from technical constraints), and
• Emergent bias (unforeseen discriminatory outcomes arising in real use, often arises when an
algorithm is used by an audience or in a context for which it was not designed)
They argue: “freedom from bias should be counted among the select set of criteria – including
reliability, accuracy, and efficiency -- according to which the quality of systems in use in society
should be judged.”
Friedman, B. and Nissenbaum, H. (1996) Bias in Computer Systems.ACM Transactions onInformation Systems, 14, 330-347.

--- Page 10 ---
T B
ECHNICAL IAS
• Computer tools:
• e.g., in a database for matching organ donors with potential transplant recipients,
certain individuals retrieved and displayed on initial screens are favored systematically
for a match over individuals displayed on later screens
• Decontextualized Algorithms Bias that originates from the use of an algorithm that fails to
treat all groups fairly under all significant conditions
• e.g., a scheduling algorithm that schedules airplanes for take-off relies on the
alphabetic listing of the airlines to rank order flights ready within a given period of
time
• Random Number Generation:
• e.g., an imperfection in a random-number generator used to select recipients for a
scarce drug leads systematically to favoring individuals toward the end of the
database.
• Formalization of Human Constructs Bias that originates from attempts to make human
constructs such as discourse, judgments, or intuitions amenable to computers:
• e.g., a legal expert system advises defendants on whether or not to plea bargain by
assuming that law can be spelled out in an unambiguous manner that is not subject to
Friehdummana, Bn. aanndd N ihssuenmbaaunme, Hi.n (t1e99r6p)r Beiatsa itni oConmsp iunte cr oSynstteemxst.ACM Transactions onInformation Systems, 14, 330-347.

--- Page 11 ---
F M L
AIRNESSAND ACHINE EARNING
B B , H , N
OOKBY AROCAS ARDT AND ARAYANAN
“Something … is lost in moving to automated, predictive decision
making. Human decision makers rarely try to maximize predictive
accuracy at all costs; frequently, they might consider factors such as
whether the attributes used for prediction are morally relevant.”
“For example, although younger defendants are
statistically more likely to re-offend, judges are loath to
take this into account in deciding sentence lengths,
viewing younger defendants as less morally culpable.”

--- Page 12 ---
A B R -
DDRESSING IAS IN EAL
W
ORLD APPLICATIONS

--- Page 13 ---
Sept 2016

--- Page 14 ---
WMDs:
Measure what they can, not
“Good” Model:
what they should
baseball stats
Do not adjust agilely to
WMD: US News College
error
Rankings
Not answerable, secret
formula
Create their own reality

--- Page 15 ---
B A C
IAS IN UTOMATED LASSIFICATION
Facial recognition algorithms trained on biased
datasets led to under-recognition of protected
classes
2018

--- Page 16 ---
Nov 2016

--- Page 17 ---
Biased, Differential Ad Delivery
Sweeney, Discrimination in Online Ad Delivery, CACM 2013

--- Page 18 ---
F P :
OCUS OINT
D C
ATASET OLLECTION
What kinds of biases can enter at this phase of machine learning?

--- Page 19 ---
D B
ATASET IASES
• Sampling Bias
• Ex: More images of homes with lawns than without
• Measurement Bias
• Ex: Camera settings for creating images
• Label Bias
• Ex: Cultural differences in naming (e.g., grass vs lawn)
• Negative Set Bias
• Ex: Missing images from non-dominant culture
Srinivasan & Chander, Biases in AI Systems, CACM 2021

--- Page 20 ---
Idea: Fix Bias in the Data Stage of the
Pipeline
Wang et al., "REVISE: A tool for measuring and mitigating bias in visual datasets." European Conference on Computer Vision. 2020.

--- Page 21 ---
Wang et al., "REVISE: A tool for measuring and mitigating bias in visual datasets." European Conference on Computer Vision. 2020.

--- Page 22 ---
Wang et al., "REVISE: A tool for measuring and mitigating bias in visual datasets." European Conference on Computer Vision. 2020.

--- Page 23 ---
Wang et al., "REVISE: A tool for measuring and mitigating bias in visual datasets." European Conference on Computer Vision. 2020.

--- Page 24 ---
F M L
AIRNESS AND ACHINE EARNING
B .
AROCAS ET AL
“Is our goal to faithfully reflect the data? Or do we have an
obligation to question the data, and to design our systems to
conform to some notion of equitable behavior, regardless of
whether or not that’s supported by the data currently available to
us?”

--- Page 25 ---
Machine Learning Feedback Loop
https://fairmlbook.org/

--- Page 26 ---
Machine Learning Feedback Loop
Do we have to learn
from data? Why do
we?
Do we have to learn
from human’s
reactions to models?
Why do we?
https://fairmlbook.org/

--- Page 27 ---
Taxonomy of Bias Types in AI Pipeline
Srinivasan & Chander, Biases in AI Systems, CACM 2021

--- Page 28 ---
D B (B .)
E IASING ORACAS ET AL
“Most attempts to “debias” machine learning in
the current research literature assume simplistic
mathematical systems, often ignoring the effect
of algorithmic interventions on individuals and on
the long-term state of society.”

--- Page 29 ---
Failed Diversification Attempt
“The company spent years assembling teams that tried to
reduce any outputs from its technology that users might find
offensive. Google also worked to improve representation,
including showing more diverse pictures of professionals like
doctors and businesspeople in Google Image search results.
But now, social media users have blasted the company for
going too far in its effort to showcase racial diversity.”

--- Page 30 ---
“Prabhakar Raghavan, Google’s head of search, said in a blog post last month: “So
what went wrong? In short, two things. First, our tuning to ensure that Gemini
showed a range of people failed to account for cases that should clearly not show a
range. And second, over time, the model became way more cautious than we
intended and refused to answer certain prompts entirely … These two things led the
model to overcompensate in some cases and be over-conservative in others, leading
to images that were embarrassing and wrong.”

--- Page 31 ---
A R O (B .)
EASON FOR PTIMISM ORACAS ET AL
“A reason for optimism is that the turn to
automated decision-making and machine
learning offers an opportunity to reconnect with
the moral foundations of fairness. Algorithms
force us to be explicit about what we want to
achieve with decision-making.”

--- Page 32 ---
S A
OME DVANCES
• Datasheets and Factsheets
• Intentional data collection
• Algorithms as aids, but not decision makers
• Transparency in algorithms
• Methods for appeal outside of algorithms

--- Page 33 ---
F
AIRNESS IN
S R
EARCH ANKING

--- Page 34 ---
K T A P
EYWORDS IN ITLES OF CCEPTED APERS
(SIGIR T T C , C IR
IS A RADITIONAL ECHNICAL ONFERENCE ORE
R )
ESEARCH
• SIGIR’17: Fairness: 0 Bias: 0
• SIGIR’18: Fairness: 1 Bias: 2
• SIGIR’19: Fairness: 0 Bias: 1
• SIGIR’20: Fairness: 4 Bias: 1
• SIGIR’21: Fairness: 7 Bias: 4
• …
• SIGIR’24: Fairness: 13 Bias: 9
• SIGIR’25: Fairness: 17 Bias: 13

--- Page 36 ---
D N IR
IVERSITY AND OVELTY IN
(C .’08)
LARKE ET AL
• “For a given query, an information retrieval system should respond with a
ranked list that respects both the breadth of available information and any
ambiguity inherent in the query.”
• “Ideally, the document ordering for this query would properly account for the
interests of the overall user population.”
• Example: for “jaguar”, if cars are more popular than cats, it might make sense
to have the first few result be the jaguar car, and the next few be the felines.
• Differentiate:
• Novelty: the need to avoid redundancy
• (near duplicates; find more deeper info on same topic)
• Diversity: the need to resolve ambiguity
Clarke et al. Novelty and Diversity in Information Retrieval Evaluation SIGIR’08

--- Page 43 ---
W I S
EB VS MAGE EARCH
• Note the way the results differ for image vs web search
• One is more commercial, the other more visual
• Is this designed or accidental?

--- Page 44 ---
I D
MAGE IVERSITY
People noted that querying on general terms like
“CEO” on google image search returned
undiverse images.

--- Page 46 ---
W Q M ?
HAT DOES THE UERY EAN
• “CEO” alone prototype: implies famous CEO of very big
company
• ”CEOs” different than “CEO”
• Not the same for “custodian” or “software engineer”

--- Page 47 ---
T P V Q
HE ROBLEM WITH AGUE UERIES
Let’s compare some queries that are single
words, and others that make use of Russell’s
suggestion: refine your query with more specific
words

--- Page 52 ---
Fairness of Exposure in Rankings
Singh & Joachims, KDD’18
“[I]t is no longer just books that are being ranked, but there is hardly anything that is
not being ranked today – products, jobs, job seekers, opinions, potential romantic
partners.
Nevertheless, one of the guiding technical principles behind the optimization of ranking
systems still dates back to four decades ago – namely the Probability Ranking Principle
(PRP) [Robertson’77].
It states that the ideal ranking should order items in the decreasing order of their
probability of relevance, since this is the ranking that maximizes utility of the retrieval
system to the user for a broad range of common utility measures in Information
Retrieval.
But is this uncompromising focus on utility to the users still appropriate when we are
not ranking books in a library, but people, products and opinions?”

--- Page 53 ---
F F R
RAMING AIRNESS IN ANKING
• There is no single definition of what constitutes a fair ranking
• Fairness depends on context and application.
• Different notions of fairness imply different trade-offs in utility, which may be acceptable in
one situation but not in the other.
• For example, we may not want to convey strong rights to the books in a library when a
user is trying to locate a book, but the situation is different when candidates are being
ranked for a job opening.
• We are not limited to a single definition of fairness, since different application scenarios
probably require different trade-offs between the rights of the items and what can be
considered an acceptable loss in utility to the user
Singh & Joachims, Fairness of Exposure in Rankings, KDD’18

--- Page 54 ---
F F
ORMULATIONS OF AIRNESS FOR
R
ANKING
• Demographic Parity:
• Enforce that average exposure from documents in all groups is equal
• Disparate Treatment:
• Enforce that exposure of each group is proportional to their average
utility
• This takes relevance into account, but helps with “just-miss” cases,
where the ranked values for members of groups are close to one
another
• Disparate Impact:
• Enforce that the expected clickthrough rate of each group is
proportional to its average utility
• Goes beyond utility to take into account the impact of the exposure
Singh & Joachims, Fairness of Exposure in Rankings, KDD’18

--- Page 55 ---
G I S R
ENDER AND MAGE EARCH ANKINGS
S
TUDY
• Introduction: Experimentally evaluated the effects of how gender is represented in image
search results for occupations. Used very general queries like:
• “bartender”, “custodian”
• Findings:
• People have quite accurate estimates of the gender proportions of occupations
• Gender proportions in search results are close to those in actual occupations.
• However, there are slight exaggerations of gender ratios towards gender stereotypes for
many
• People rate search results quality higher when they are consistent with stereotypes for a
career
• Images matching the gender stereotype are rated more professional and less inappropriate
• Manipulating the representation of gender in image search results can shift people’s
perceptions about real-world distributions very slightly (by about 7%)
• Conclusion: There are tensions between desire for high-quality results and broader societal
goals for equality of representation in this space.
Kay, et al. Unequal Representation and Gender Stereotypes in Image Search Results for Occupations. CHI 2015

--- Page 56 ---
L -I F R
INKED N AIRNESS ANKING
A
PPROACH
• “For a given search or recommendation task, our algorithms seek to achieve a
desired distribution of top ranked results with respect to one or more protected
attributes.”
• “We show that such a framework can be tailored to achieve fairness criteria …
depending on the choice of the desired distribution.”
Cem et al. "Fairness-aware ranking in search & recommendation systems with application to Linkedin talent search."KDD’19.

--- Page 57 ---
L -I F R
INKED N AIRNESS ANKING
A
PPROACH
• Did A/B testing for representative ranking:
• “For each search request, the desired gender distribution over the
ranked candidate list is chosen to be the gender distribution over the
set of candidates that meet (i.e., qualify for) the search criteria”
• Hundreds of thousands of recruiters
• Comparison was a model optimized to make a successful hire
• “Our approach resulted in tremendous improvement in the fairness
metrics (nearly threefold increase in the number of search queries with
representative results) without affecting the business metrics”
• “This paved the way for deployment to 100% of LinkedIn Recruiter users
world-wide.”
Cem et al. "Fairness-aware ranking in search & recommendation systems with application to Linkedin talent search."KDD’19.

--- Page 58 ---
S
UMMARY
• Fairness is a social construct
• There are many different definitions of fairness; it is contextual
• For many applications, there is a tradeoff between fairness and
accuracy/relevance; the two often can’t be simultaneously
optimized
• One potential remedy is to revisit the assumptions behind the
data and the original calculation of accuracy/relevance

--- Page 59 ---
A W C
CADEMIC ORKSHOPS AND ONFERENCES
• Workshop on Algorithmic Bias in Search and Recommendation: (BIAS, 2020-
today)
• FATML.org: Fairness, Accountability, and Transparency in Machine Learning
(2014-today)
• ACM Conference on Fairness, Accountability, and Transparency (FAccT, 2018-
today)
• Symposium on the Foundations of Responsible Computing (math-y, 2020-today)

--- Page 60 ---
More Depth on These Topics:
I 203 and I 205
Prof. Morgan Ames Prof. Deirdre Mulligan
UCB I School UCB I School
Social Issues of Information Information Law & Policy

--- Page 61 ---
Designing Value-Based Technology:
I 213: User Interface Design &
Development
Prof. Niloufar Salehi
UCB I School

--- Page 62 ---
N T
EXT IME
• Intellectual Property
• Course Wrap-up


================================================================================
FILE: 27_intellectual_property_course_wrapup.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 27: Intellectual Property; Course Wrap up

--- Page 2 ---
Today’s Outline
Intellectual Property
Course Wrap Up
Course Evaluations

--- Page 3 ---
"Let's Go Crazy" #1
https://www.youtube.com/watch?v=N1KfJHFWlhQ

--- Page 6 ---
https://www.bbc.com/news/technology-44643419

--- Page 7 ---
W I I P ?
HAT S NTELLECTUAL ROPERTY
“A category of property that includes intangible creations of the human intellect”
wikipedia

--- Page 8 ---
https://ocpatentlawyer.com/four-types-intellectual-property-protect-idea/

--- Page 9 ---
Types of Intellectual Property

--- Page 12 ---
W C / C P
HAT AN ANNOT BE ROTECTED
C ?
BY OPYRIGHT
Let’s do an online poll:
pollev.com/i202

--- Page 13 ---
https://www.nytimes.com/2021/11/29/dining/recipe-theft-cookbook-plagiarism.html

--- Page 14 ---
https://www.wired.com/2011/01/hope-image-flap/

--- Page 15 ---
https://www.wired.com/2011/01/hope-image-flap/

--- Page 16 ---
What Does Fair Use Mean?
• “Fair use … lets people use and adapt copyrighted works without
getting the explicit permission of their owner”
• “Instead of setting out specific statutory exemptions to copyright, as
many other countries do, U.S. law issues four broad factors which
guide whether the permission-less use of a copyrighted work is fair.”
• “This means that fair use can evolve and change over time; it also
means that the only real way to find out if something is “fair use” is to
ask a federal court.”
Meyer, After 10 Years, Google Books Is Legal, The Atlantic, 2015

--- Page 17 ---
The Four Factors of Fair Use
Purpose and Character of Use Nature of Copyrighted Work
Was material transformed with new Dissemination of facts benefits the public,
expression or meaning? so you have more leeway to copy from
Was value added by creating new factual works than fiction. Fair use is
information, aesthetics, insights? narrower from unpublished work.
Just acknowledging source material is not enough to allow for copying for commercial use
Effect of Use Upon the Potential Market
Amount / Sustainability of Portion Taken
Deprives copyright owner from income?
The less you take, the more likely copying
Undermines potential market for the
can be considered fair use. But not if you
work?
copy the “heart” of the work.
https://fairuse.stanford.edu/overview/fair-use/four-factors

--- Page 18 ---
Fair Use in Teaching
https://copyright.universityofcalifornia.edu/use/teaching.html#p=in_the_classroom

--- Page 19 ---
T U K
RANSFORMATIVE SE IS EY
• Allows for search results snippets and thumbnails
• An image to help people decide what web site to go to
is different than being entertained or informed by it
• What about scanning library books (in cooperation with
libraries) – both in and outside of copyright – and making
searchable snippets available?
https://www.theatlantic.com/technology/archive/2015/10/fair-use-transformative-leval-google-books/411058/

--- Page 20 ---
Fast forward from 2007 to today…
How does copyright work on YouTube?

--- Page 21 ---
https://www.tunepocket.com/use-copyrighted-music-youtube/

--- Page 23 ---
YouTube Copyright Checks
https://support.google.com/

--- Page 24 ---
YouTube Copyright Checks
https://support.google.com/youtube/

--- Page 25 ---
B M , I , V
EWARE USES OF USIC MAGES IDEO
• Warning: remember the statement earlier that only a court can really decide!
• That said, it seems that you should be very careful with uses of:
• Music, including sampling: seems to lose in court if you do not obtain
permission
• Images (including tracing); see the Fairey case above; you need permission to
use images that are copyrighted in most cases
• Even if you own a piece of artwork, you can’t necessarily legally sell images of it
on the web – the artist might own the rights!
The good news: lots of progress has been made
on more reasonable approaches to online IP

--- Page 26 ---
L ’ T U
ET S EST OUR NDERSTANDING
Let’s do an online poll:
pollev.com/i202

--- Page 27 ---
W S F U
EB EARCH AND AIR SE
• Google’s caching of websites
• Field v Google 2006, Parker v Google 2007
• Found that crawling + caching served a new and valuable function, this is highly transformative,
and not a replacement for the original work
• Image search and thumbnails
• Perfect 10 v Amazon / Google 2007
• Found search-engine thumbnails and indexing are protected fair use, even when the underlying
works are copyrighted images.
• Web scraping
• Ticketmaster v RMG Technologies 2007
• Found against the scraper—but for contract breach and circumvention, not copyright
• Helps clarify that fair-use analysis applies when the issue is copying, not access violation.

--- Page 29 ---
G B F U
OOGLE OOKS AND AIR SE
• 2004: Google starts scanning books; wants to make “orphaned” works available
• 2005: Authors Guild sued to stop it
• 2011: almost settled on a for-fee model – the judge ruled against -- that would be a
monopoly!
• 2013: Judge Chin said the scans were fair use
• ““It advances the progress of the arts and sciences, while maintaining respectful
consideration for the rights of authors and other creative individuals, and without
adversely impacting the rights of copyright holders”
• 2014: Authors Guild appealed – and lost!
https://www.theatlantic.com/technology/archive/2015/10/fair-use-transformative-leval-google-books/411058/

--- Page 30 ---
G B F U
OOGLE OOKS AND AIR SE
Appeals Court Judge Leval wrote:
“For nearly 300 years, since shortly after the birth of copyright in England in 1710,
courts have recognized that, in certain circumstances, giving authors absolute
control over all copying from their works would tend in some circumstances to
limit, rather than expand, public knowledge”
“Google’s unauthorized digitizing of copyright-protected works, creation of a
search functionality, and display of snippets from those works are non-infringing
fair uses. The purpose of the copying is highly transformative, the public display
of text is limited, and the revelations do not provide a significant market substitute
for the protected aspects of the originals.”
https://www.theatlantic.com/technology/archive/2015/10/fair-use-transformative-leval-google-books/411058/

--- Page 31 ---
U G B C
PSHOT OF OOGLE OOKS ASE
Does this settlement mean you can write a web scraper to
download all of the journals from a publisher and do text
analysis on them?
No, it does not – if you did this using the UC Berkeley license
to access online journals, that would violate the terms of use
(and might shut down access for the entire campus!)

--- Page 32 ---
U G B C
PSHOT OF OOGLE OOKS ASE
Does this settlement mean you can write a web scraper to
download all of the journals from a publisher and do text
analysis on them?
No, it does not – if you did this using the UC Berkeley license
to access online journals, that would violate the terms of use
(and might shut down access for the entire campus!)

--- Page 33 ---
IP LLM : T D
AND S RAINING ATA

--- Page 34 ---
T LLM T D C
WO RAINING ATA ASES
• 2 cases: Bartz v. Anthropic and Kadrey v. Meta,
• Question is whether genAI developers made a fair use or not by collecting and copying in-
copyright works as training data for their LLMs.
• The judges largely found the developers’ training uses to be fair, although applying
differing analyses and coming to different conclusions on particular issues.
• The judges’ two biggest disagreements concerned:
• The implications of using “pirated” books as training data for foundation models and
• A novel theory of “market dilution” which speculates that the general, indirect market
effects of genAI outputs capable of producing massive numbers of potentially
competitive works is a market harm under copyright law.
Samuelson, Pam, Legally Speaking: Does Using In-Copyright Works as Training Data Infringe? CACM 2025

--- Page 35 ---
LLM T : P
RAINING REDICTION
• It looks likely that future training will be done on licensed
data and this issue will dissipate
• However, there are concerns that for information that is
not licensed, there will be little incentive to produce it
(especially web pages)

--- Page 36 ---
W US C O
HAT IS THE OPYRIGHT FFICE
S LLM /AI?
AYING ABOUT S
https://www.copyright.gov/ai/

--- Page 37 ---
IP and LLMs: Copyright
Summary by Gemini

--- Page 38 ---
A Brief
History of
the Creative
Commons
https://vimeo.com/251525073
This video is about the history of Creative Commons.
CC BY 4.0, 2018, Maran Wolston
creativecommons.org/licenses/by/4.0/

--- Page 40 ---
T G R M / I
O ET IGHTS TO USIC MAGES
• Make your own
• Get permission / pay for it
• Support / contribute to open access efforts
• Support efforts to reform copyright law

--- Page 41 ---
Google Images allows you to try to find
images with sharable rights

--- Page 42 ---
Many Organizations Trying To Make Free
Sharable Content Available

--- Page 43 ---
Academic Publishing is Slowly Winning the
Battle for Open Access

--- Page 44 ---
Academic Publishing is Slowly Winning the
Battle for Open Access

--- Page 46 ---
What Intellectual
Property
Considerations
Should You Consider
For your Final
Projects/Papers?

--- Page 47 ---
C W U
OURSE RAP P

--- Page 48 ---
Imagine you are viewing this boat from the dock.
What kinds of information might you want to find out about it?
What kinds of data might you want to find out about it?

--- Page 49 ---
B ’ L B
ORGES IBRARY OF ABEL
Image from Andrew DeGraff’s Plotted: A Literary Atlas

--- Page 50 ---
Data, Descriptions, Metadata, Metadata
Description
Phenomena in Data: What Data MetaData Metadata
the World Items are Representation Description
Collected Language
Photos of birds, Pixels Image size, EXIF (automatic)
stored on hard Image format, IPTC (manual);
drive Dat/time taken, now XMP is a
Lat/Long, new standard
Camera settings
Real bird Physical bird Measurements, Specimens
specimen type, date, database
instrument, etc Schema (e.g.
EME)
PDF document of Dollar amount Date, invoice Database
Work done an invoice embedded in number, payee, Schema
on a contract PDF amount due, etc

--- Page 51 ---
JSON vs XML
https://www.guru99.com/json-vs-xml-difference.html

--- Page 52 ---
XML can be nested (hierarchical)
http://webdam.inria.fr/Jorge/prog/birds.xml

--- Page 53 ---
Netflix
Resource Selection Strategy

--- Page 54 ---
What is the Right Way to Organize
Spices/Herbs?
Glushko, TDO wikipedia
by cuisine by price, eye appeal alphabetically

--- Page 55 ---
Example:
Smithsonian Natural History Collection

--- Page 56 ---
Smithsonian Collections Management
Policy
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 57 ---
There are many copies of this book
Many books have the same title
How to identify them?

--- Page 58 ---
• Prefix: currently either 978 or 979
• Registration group: country, geo, language
• Registrant: publisher or imprint
• Publication: edition and format of the item
• Check digit: mathematically validates the
rest of the number (sometimes called a
check sum)
www.isbn-international.org/content/what-isbn

--- Page 59 ---
100,000 Pyramid
https://www.youtube.com/watch?v=UJMx_YfZiW0

--- Page 60 ---
Which Games / Game Shows
are about Categories?
Categories Individual
information / facts /
data

--- Page 61 ---
S ) P I
( OME RINCIPLES FOR NTENTIONALLY
C C
REATING ATEGORIES
Car Brands
Enumeration
⚫
Climbable Things
Single Property
⚫
Expensive & Brittle Things
Multiple Properties
⚫
Things you take on a camping trip
Goal-based
⚫
Definition of a Triangle
Theory-based
⚫

--- Page 62 ---
C V C
LASSIC IEW OF ATEGORIES
1. Categories are defined by a list of properties
shared by all elements in a category (necessary &
sufficient)
2. Category membership is binary (in or out)
3. Because membership is defined by rules, every
member in the category is equally a member
3-lined figures Closed polygons
Example: triangles are 3-
sided closed polygons
Triangles
Stephen Palmer 2002

--- Page 63 ---
P
ROTOTYPES
• Which dog breed is central?
• Which are “better” or “worse” examples?
https://articles.hepper.com/great-dane-chihuahua-mix/ https://www.pageant.dog/breeds/dog/1120-Labrador-Retriever

--- Page 64 ---
E F P
VIDENCE OR ROTOTYPES
Typicality ratings
Order in which members are named
Time needed to verify category membership
Istockphoto.com

--- Page 65 ---
P . C I
ROTOTYPE VS LASSICAL N
P
RACTICE
• Although we think more naturally with fuzzy boundaries, we
sometimes are forced to make sharp distinctions
• Example: Dept of Motor Vehicles has to classify which
vehicles require licenses
• Answers questions like:
• Licenses?
• Helmets?
• Sidewalk?

--- Page 66 ---
Example: Vehicle
More Prototypical Closer to the Center

--- Page 67 ---
Superordinate and Subordinate
Levels
SUPERORDINATE animal furniture emotion
What are other
BASIC LEVEL dog chair happy
examples?
SUBORDINATE terrier rocker joy
• Children take longer to learn superordinate
• Superordinate not associated with mental images or motor
actions

--- Page 68 ---
DIFFERENT LEXICAL FORM SAME LEXICAL FORM
hypernyms hypernyms
(superordinate) (superordinate)
synonyms polysemes
WORD
sibling terms homographs
hyponyms
hyponyms
(subordinate)
(subordinate)

--- Page 69 ---
DIFFERENT LEXICAL FORM SAME LEXICAL FORM
hypernyms polysemes
cooking utensil
(superordinate)
pot (betting)
pot (flower)
pot
kettle
pot
hyponyms
coffee pot
(subordinate) homographs

--- Page 70 ---
H C M
IERARCHY OMPOSED OF A IX OF
C I D N
ONCEPTS S IFFICULT TO AVIGATE
• The problem: different attributes in one hierarchy:
• Sound type > location type > beak type
• But it is not a good solution to combine these either
• Ground-dwelling bird that sings with stubby beak
• Solution: faceted metadata! Separate out the attributes
and then assign multiple attributes to each information
item.
• Sound type
• Location type
• Beak type

--- Page 71 ---
S : F C
OLUTION ACETED ATEGORIES
• A set of different categories
• Identify different aspects, attributes, or
features
• Resources are labeled with multiple
categories
• Each category can be hierarchical
GeoRegion + Time/Date + Topic + Role

--- Page 72 ---
Occasion > Party
Dish > main > tacos Cuisine > Mexican
Occasion > Tailgate
Ingredients > Meat > Fish > Cod
Ingredients > Veg > Onion > Red Onion Preparation > Saute
Ingredients > Bread > Tortilla
https://www.epicurious.com/recipes/food/views/fish-tacos-352976

--- Page 73 ---
Five Hierarchical Faceted Categories
Clothing
Nature Hats
Animal Cowboy Hat
Mammal
Horse Is-a
Media
Is-a
Engraving
Wood Eng.
Occupations Is-a-type-of
Cowboy
Location
North America
America
Is-a-type-of Is-in or
A Bucking Bronco by Henry Wolf, https://art.famsf.org/henry-wolf/bucking-bronco-19633026024 is-part-of

--- Page 74 ---
Notice for art
objects, the
What
What
“what” refers
to both the
art and what
What
it depicts.
When
Thematic Let’s drill down
What into this
Where
thematic
Property
category
What

--- Page 75 ---
Who (author)
When
(published)
What (language)
What (genre)

--- Page 76 ---
O E : E
NTOLOGY XAMPLE COLOGY
More rules, more inferences
Concentrated-in(p,a) and Eat(b,a) -> Intoxicated-by(b,p)
Eat(a,b) and Eat(b,c) -> Eat(a,c)
Can conclude:
Pike intoxicated-by PCB
Human intoxicated-by PCB
Jean-Baptiste, Ontologies with Python, O’Reilly,
2020

--- Page 77 ---
Go Ontology Example
https://www.ebi.ac.uk/QuickGO/term/GO:0060887

--- Page 78 ---
E (I ): E W N
XERCISE NDIVIDUAL XPLORE ORD ET
Now try this one: visuwords.com
visuwords.com
Try “bank”, “money”, “joy”

--- Page 79 ---
V P E :
OCABULARY ROBLEM XAMPLE
N S R
AMING FOR MART OOMS
How did she solve this? Augmented reality and
autosuggest!
Meghan Clark, One-Shot Interactions with Intelligent Assistants in Unfamiliar Smart Spaces, UCB Dissertation, 2021

--- Page 80 ---
A Concept Has Many Relation Types
(Associations)
weaving
material
Is-a-kind-of
created-by
cotton
fabric
rayon
clothing used-for
made-from
wool
curtains

--- Page 81 ---
Taxonomic and Thematic Similarity
High Taxonomic, Low Thematic Low Taxonomic, High Thematic
Word 1 Word 2 Word 1 Word 2
Breakfast Dinner Helicopter Pilot
Helmet Crown Floss Teeth
Salt Sugar Pillow Head
Low Taxonomic, Low Thematic
High Taxonomic, High Thematic
Word 1 Word 2
Word 1 Word 2
Portrait Report
Ring Bracelet
Prisoner Pupil
Shingle Brick
Mirman
Bird Lamb
Tape Staple
Landrigan & Mirman, Taxonomic and Thematic Relatedness Ratings for 659 Word Pairs, JOPD 4, 2016

--- Page 82 ---
Taxonomic Vs Thematic Associations
Evidence for Different Specialization in
Regions in the Brain
Intracranial EEG readings suggest:
ATL specialized for taxonomic
relations
IPL specialized for thematic relations
Close coordination is also suggested
between the two regions.
Anterior Temporal Lobe (blue)
Inferior Parietal Lobule (red)
Thye et al, Intracranial EEG evidence of functional specialization for taxonomic and thematic relations, Cortex 140, 2021

--- Page 83 ---
Example: Grounded Coding of Tweets
Goal: better understand how people are writing outside
the classroom
Approach: use tweets to analyze the writing practices
of fans of Bruce Springsteen
Data: tweets before, during, and after a concert in 2012
Baby, We Were Born to Tweet: Springsteen Fans, The Writing Practices ofIn Situ Tweeting, and the Research
Possibilities for Twitter, Wolff, Kairos journal, 19.3, 2015 https://kairos.technorhetoric.net/19.3/topoi/wolff/
wikipedia

--- Page 84 ---
Measuring Inter-Annotator Agreement
Simple method: take the proportion of agreement
Coder A Coder B Agree? Coder A Coder B Agree?
porpoise dolphin disagree cat cat agree
porpoise dolphin disagree cat cat agree
3/10
dolphin dolphin agree cat cat agree
vs
dolphin porpoise disagree cat cat agree
porpoise dolphin disagree 7/10 porpoise dolphin disagree
porpoise dolphin disagree porpoise dolphin disagree
dolphin dolphin agree dolphin dolphin agree
dolphin porpoise disagree dolphin porpoise disagree
dog dog agree dog dog agree
dog cat disagree dog cat disagree
Are these equivalent? Dolphin/Porpoise harder to distinguish than cat/dog!

--- Page 85 ---
User Interface
Design
Information
Architecture
Information
Navigation
Design
Design
Graphic Design
Newman & Landay, DIS ‘00

--- Page 87 ---
Intuition: Words with Similar Context
Neighborhoods Have Similar Meaning
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/

--- Page 88 ---
Computing Word Embeddings with
Distributions Makes a Richer
Representation
Think of the colors as showing complex nuance
about which words have appeared in the same
context
These are real numbers instead of frequency
https://predictivehacks.com/a-high-level-introduction-to-word-embeddings/
counts

--- Page 89 ---
Plotting on a 2D Graph
fast
large
small A vector is not just a point on a graph;
it's the path from the center to that
point, defined by its components.
slow X-axis: a scale from Large (+X) to Slow (-X)
Y-axis: a scale from Fast (+Y) to Slow (-Y)
(0,0) is the “average” word

--- Page 90 ---
Computing Word Similarity
with Word Embeddings
Which relation types are scored as most similar?

--- Page 91 ---
I N A W N
MAGE ET ND ORD ET
WordNet inspired the creation of ImageNet and provides its structure
Deng, et al. "ImageNet: A large-scale hierarchical image database." IEEE CVPR 2009.

--- Page 92 ---
C
ONVOLUTIONS TO
“ ” I
AVERAGE OVER AN MAGE
But What is a Convolution? 3Blue1Brown https://www.youtube.com/watch?v=KuXjwB4LzSA

--- Page 93 ---
Convolutional Neural Networks
(CNNs)
Network Architecture
Convolutional Layer, Pooling Layer, Fully Connected Layer

--- Page 94 ---
C
LASSIFICATION
Let h(x) be the “true” mapping.
We never know it. How do we
find the best ĥ(x) to
approximate it?
One option: rule based
if x has characters in
unicode point range 0370-03FF:
ĥ(x) = greek

--- Page 95 ---
Bag of Words Representation
Jurafsky & Martin slides

--- Page 96 ---
H S E W
OW EARCH NGINES ORK
Three main parts:
i. Gather the contents of all web pages (using a
program called a crawler or spider)
ii. Organize the contents of the pages in a way
that allows efficient retrieval (indexing)
iii. Take in a query, determine which pages match,
and show the results (ranking and display of
results)

--- Page 98 ---
Berry Picking Model
• Users’ needs (and queries)
change as a result of the
search process
• Goals change in priority
• Information needs are not
satisfied by finding single
document; the trail of
Bates 1989
information is what’s important.
Main value is accumulated over the
course of the search

--- Page 99 ---
How Does the Web Work?
Say a user named Oski using his computer at
home (or in, say, Seoul) wants to find information
about i202?
What happens when he:
Brings up a search engine home page?
Types his query?
sylvain kalache at wikicommons First, we have to understand how the WWW
works!
Then we can understand search engines.

--- Page 100 ---
Routing Between Computers
• How does the computer at Oski’s
desk figure out where the i202 web
pages are?
• In order for him to use the WWW,
Oski’s computer must be connected
to another machine acting as a web
server (via his ISP).
• This machine is in turn connected to
other computers, some of which are iSchool
routers. Network
•
Routers figure out how to move information
from one part of the network to another.
•
There are many different possible routes.

--- Page 101 ---
S W S E A
TANDARD EB EARCH NGINE RCHITECTURE
Check for duplicates,
store the
crawl the
documents
web
Crawler
DocIds
machines
Create an
user
inverted
query index
Search
Inverted
engine
Show results
index
To user servers

--- Page 102 ---
Converting Natural Language to
Boolean
“Measurement of the Formal Query:
width of cracks in cracks AND beams
Cracks
prestressed AND Width_measurement
concrete beams” AND Prestressed_concrete
Beams Width_measurement
Relaxed Query:
Prestressed
concrete (C AND B AND P) OR
(C AND B AND W) OR
(C AND W AND P) OR
(B AND W AND P)

--- Page 103 ---
Foraging Theory
https://www.nngroup.com/articles/information-foraging/

--- Page 104 ---
Follow Links
Search
The Sensemaking Model
Ask Colleagues
Triage
Navigate Resources
“The process of searching for a representation and
encoding data in that representation to answer task-
Read Overviews
specific questions.” – Russell et al. CHI 1993
Take Notes
Search
Categorize Notes
Encoding
Write Summaries
Create Spreadsheets
Make Database Entries
Talk with Collaborators

--- Page 105 ---
Intelligence Analysts’ SenseMaking Loop
Pirolli & Card, The Sensemaking Process and Leverage Points for Analyst Technology as
Identified Through Cognitive Task Analysis ,PICIA, 2005

--- Page 106 ---
W
ORD
F
REQUENCIES HAVE
L T
ONG AILS
Zipf’s Law
Zdeněk Macháček on Unsplash

--- Page 107 ---
Computing Similarity Scores to Determine
Which Documents are Close to the Query
D = (0.8, 0.3)
1
D = (0.2, 0.7)
2
Q = (0.4, 0.8)
1.0 Q
D
cos = 0.74
2
1
0.8
cos = 0.98
 2
0.6
2
0.4
 D
1 1
0.2
0.2 0.4 0.6 0.8 1.0

--- Page 108 ---
TF.IDF E
XAMPLE
N
w = tf log
i,j i,j
n
i
Columns are document numbers, N=4; log base 10

--- Page 109 ---
D I I
ECOUPLING THE NVERTED NDEX
The term Index Postings
aid 4, 8
all 2, 4, 6
back 1, 3, 7
brown 1, 3, 5, 7
come 2, 4, 6, 8
dog 3, 5
fox 3, 5, 7
good 2, 4, 6, 8
jump 3
lazy 1, 3, 5, 7
men 2, 4, 8
now 2, 6, 8
over 1, 3, 5, 7, 8
party 6, 8
quick 1, 3
their 1, 5, 7
time 2, 4, 6

--- Page 110 ---
M I
EASURING THE MPORTANCE OF
L
INKING
PageRank Algorithm
• Idea: important pages are pointed to by other important pages
• Method:
• Each link from one page to another is counted as a “vote” for the destination
page
• But the importance of the starting page also influences the importance of the
destination page.
• And those pages scores, in turn, depend on those linking to them.
Image and explanation from http://www.economist.com/science/tq/displayStory.cfm?story_id=3172188

--- Page 111 ---
Measuring Precision and Recall
6/20

--- Page 112 ---
Language Modeling Training in Action
https://www.nytimes.com/interactive/2023/04/26/upshot/gpt-from-scratch.html

--- Page 113 ---
LLM Pretraining
Pretraining is the act of training a model from scratch: the weights are
randomly initialized, and the training starts without any prior knowledge.
https://huggingface.co/learn/llm-course/en/chapter1/4

--- Page 114 ---
Exercise: See How LLMs Tokenize
Text
https://tiktokenizer.vercel.app/

--- Page 115 ---
Hybrid Search Uses both
Dense and Sparse (BM-25) Vectors
https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6

--- Page 116 ---
https://www.theguardian.com/commentisfree/2023/mar/27/pope-coat-ai-image-baby-boomers

--- Page 117 ---
S F C
TRATEGIES OR OMBATING
M
ISINFROMATION
• Standard approach: debunking
• Can help, but there are barriers
• Can backfire, but there is increasing evidence that
this is not as much of an issue as once thought
• Newer, more successful approach: inoculation
• Expose people to weakened versions of the
misinformation
• Slowly build up cognitive resistance
• Prophylactic vs Therapeutic (for pre-existing views)
Lewandowsky et al., The Debunking Handbook 2020

--- Page 119 ---
W F ?
HAT IS AIRNESS
What does it mean for an algorithm/system to be “fair” or “unfair”?

--- Page 120 ---
K T A F
EYWORDS IN ITLES OF CCEPTED ULL
P
APERS
(SIGIR T T C , C IR
IS A RADITIONAL ECHNICAL ONFERENCE ORE
R )
ESEARCH
• SIGIR’17: Fairness: 0 Bias: 0
• SIGIR’18: Fairness: 1 Bias: 2
• SIGIR’19: Fairness: 0 Bias: 1
• SIGIR’20: Fair(ness): 4 Bias:
1
• SIGIR’21: Fair(ness): 7 Bias:
*excluding the statistical sense of “bias”
4

--- Page 121 ---
Machine Learning Feedback Loop
Do we have to learn
from data? Why do
we?
Do we have to learn
from human’s
reactions to models?
Why do we?
https://fairmlbook.org/

--- Page 122 ---
Fix Bias in the Data Stage of the
Pipeline
Wang et al., "REVISE: A tool for measuring and mitigating bias in visual datasets." European Conference on Computer Vision. 2020.

--- Page 123 ---
T ’ W !
HAT S A RAP

--- Page 124 ---
Thank you, Sunny and Sarah!!

--- Page 125 ---
C E
OURSE VALUATIONS
Please fill them out asap!


================================================================================
FILE: 2_collections.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Collections

--- Page 3 ---
Today’s Outline
Collections
Resources
Organizing Systems
Maintenance
Ethics

--- Page 4 ---
Do you organize your spices? How?
Marion Botella, Unsplash

--- Page 5 ---
What is the Right Way to Organize?
M Hearst
Glushko, TDO wikipedia
By cuisine By price, eye appeal? Alphabetically?

--- Page 6 ---
Collections and Organizing:

--- Page 7 ---
Collections and Organizing:
Start with a Universe of Data / Information / Resources
Selection: Why were
these items selected
for inclusion?
Resource:
Where did each
item originate from?

--- Page 8 ---
Organization: How should these be grouped?

--- Page 9 ---
What is the Organizing Attribute?

--- Page 10 ---
What are the Organizing Attributes?

--- Page 11 ---
What are the Organizing Attributes?

--- Page 12 ---
C & O S
OLLECTIONS RGANIZING YSTEMS
• Collection: A group of resources that have been selected for
some purpose.
• Organizing System: An intentionally arranged collection of
resources and the interactions they support.
• Intentional Arrangement: acts of organization by people (or
computational proxies)
Definitions from TDO

--- Page 13 ---
W T T F T R
HAT O AKE ROM HIS EADING
(O S R )
RGANIZING YSTEM OADMAP
• The questions to ask when designing a
collection
• The role of resources in collections
• Resource selection, organization,
interactions, and maintenance.
• The role of standards

--- Page 14 ---
The whole process:
Let’s break it down

--- Page 15 ---
Identify the universe of possible
resources / items / data /
information
TDO Definitions of Resource:
”Anything of value that can support goal-directed activity.”
“Any entity that is the subject of organization”

--- Page 16 ---
Q A A R
UESTIONS TO SK BOUT ESOURCES
What are the individual resources?
⚫
What is their granularity?
⚫
How do we identify them?
⚫
Which ones are identical?
⚫

--- Page 17 ---
Resource Granularity:
Example of Selling Cars

--- Page 18 ---
Resource Granularity:
Example of Car Parts
Image provenance undetermined, circa 2008

--- Page 19 ---
Q A A R
UESTIONS TO SK BOUT ESOURCES
What are the individual resources?
⚫
What is their granularity?
⚫
How do we identify them?
⚫
We’ll come back
to these
Which ones are identical?
⚫

--- Page 20 ---
R S
ESOURCE ELECTION

--- Page 21 ---
The whole process:
Let’s break it down

--- Page 22 ---
Resource Selection

--- Page 23 ---
Netflix
Resource Selection Strategy

--- Page 24 ---
S R
ELECTING ESOURCES
Selection is the process by which resources are
⚫
identified, evaluated, and added to a collection
Selection is an intentional process
⚫
Selection methods and criteria vary across domains
⚫

--- Page 25 ---
S S P
OME ELECTION RINCIPLES
Utility, usefulness,
Comprehensiveness Value
relevance
To support social
Scarcity or To establish a
goals (including
uniqueness reputation or brand
avoiding bias)

--- Page 26 ---
Example Organizing System:
What are the Resources? How Selected
https://unsplash.com/photos/zFlFvx-ygTo Random Sky

--- Page 27 ---
Example Organizing System:
What are the Resources? How Selected?

--- Page 30 ---
What are the Organizing Attributes?

--- Page 31 ---
≈ç√
≈ç√

--- Page 32 ---
NYTimes
Little,
Brown
Collection Organization Strategy

--- Page 33 ---
O S
RGANIZING YSTEMS
An intentionally arranged collection of resources and the interactions they support
Much more about this in later lectures!

--- Page 35 ---
Organization is determined
by the interactions to be
supported

--- Page 36 ---
I
NTERACTIONS
An interaction is an action, function, service, or capability that makes use of the
resources in a collection or the collection as a whole.

--- Page 37 ---
T Searching / Looking Up Browsing / Exploring
YPES OF
C
OLLECTION
I
NTERACTIONS
Retrieving (Accessing) / Using (reading, listening
Returning to, computing with,
measuring…)

--- Page 38 ---
E : UC B P L
XAMPLE ERKELEY SYCH IBRARY
• Resources: books, journals, media, …
• Selection Policy:
• Fill gaps in collection; recently published items of academic merit, faculty
requests
• Organization:
• Physical arrangement in space, by call number
• Online library catalog
• Interactions
• Searching
• Browsing / serendipitous discovery
• Circulation (check out, return both virtual and physical)
• Reading, Listening, …

--- Page 40 ---
A strategy for how the
collection and organization
will be maintained

--- Page 41 ---
Organizing System Maintenance
https://www.jdwilliams.co.uk/lifestyle/story/50-marie-kondo-quotes-to-inspire-your-spring-clean

--- Page 42 ---
O S M
RGANIZING YSTEM AINTENANCE
• Storage: physical and technical aspects of maintaining the
collection
• Preservation: maintaining resources to prevent damage or
deterioration; also protect against obsolescence online
• Curation / Governance: the processes by which a resource in a
collection is maintained over time, to improve access or to restore
or transform its representation or presentation.

--- Page 43 ---
P
ROVENANCE
The history of ownership of a resource

--- Page 44 ---
E C C
THICS IN OLLECTION REATION

--- Page 45 ---
D D
ATASHEETS FOR ATASETS
• Goal: improve transparency and accountability in
machine learning dataset collection
• Method: borrow an idea from engineering; describe
metadata in terms of data sheets

--- Page 46 ---
https://datasheets.maximintegrated.com/en/ds/MAX7219-MAX7221.pdf

--- Page 47 ---
D D
ATASHEETS FOR ATASETS
Provides a series of questions to ask when creating or using a dataset. These include:
Motivations: includes funding, authors, what tasks is the dataset intended to be used for.
Composition: metadata, whether the dataset contains sensitive information.
Collection Process: sources, including human sources, any known errors?
Processing: including details on computational processing
Distribution: how, to whom, and any restrictions on distribution.
Maintenance: who and how, and if others will be able to build on it
Legal & Ethical Issues: human subjects’ questions: who was involved in the collection process, and,
if people are involved, if consent was given for the data to be collected; privacy considerations

--- Page 48 ---
wikipedia
W T T F T R
HAT O AKE ROM HIS EADING
(D D )
ATASHEETS FOR ATASETS
• Datasets for machine learning are a kind of collection
Timnit Gebru
• Reflect on the decision process behind creating,
& 6 co-authors
distributing, and maintaining a dataset
• Consider potential social harms that can result from
non-reflective selection
• What are the questions that should be asked in creating
a dataset? Do they differ from the Glushko reading?

--- Page 49 ---
Example:
Smithsonian Natural History Collection

--- Page 50 ---
Smithsonian Collections Management Policy
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 51 ---
S C M P
MITHSONIAN OLLECTIONS ANAGEMENT OLICY
(S E )
ELECTED XCERPTS
“This document sets forth polices and guidance for the acquisition,
management, use, and disposal of the collections of the National
Museum of Natural History (NMNH)”
“The NMNH’s collections activities are conducted in compliance
with The Smithsonian Institution Statement of Values and Code of
Ethics; SD 103: Smithsonian Institution Standards of Conduct, the
Advisory Board Ethics Statement; SD 600: Collections Management;
and the SD 600 Implementation Manual.”
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 52 ---
S C M P
MITHSONIAN OLLECTIONS ANAGEMENT OLICY
(S E )
ELECTED XCERPTS
“Staff will consider and evaluate the concerns of indigenous source
communities regarding collections items, recordings, information,
collecting activities and use.”
“The Smithsonian repudiates the illicit traffic in objects and specimens that
contribute to the despoliation of museums, monuments, environments,
sites and species resulting in irreparable loss to science and humanity.
Items that have been stolen, unscientifically gathered or excavated, or
unethically acquired should not be made part of Smithsonian collections. “
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 53 ---
S C M P
MITHSONIAN OLLECTIONS ANAGEMENT OLICY
(S E )
ELECTED XCERPTS
“The concept of provenance refers to the history of
ownership of a collection item. Collecting departments
shall exercise due diligence in the acquisition of
collections, including making reasonable inquiries into
the provenance of collections items under consideration
for acquisition consistent with Smithsonian policy.”
Collections records must show decision-making
“
processes of acquisitions evaluation…”
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 54 ---
Example Collection Creation: SB1421 Datasets

--- Page 55 ---
E : SB1421 D
XAMPLE ATASET
https://sfpublicdefender.org/copmonitor/
• Resources:
• Selection Policy:
• Organization:
• Interactions:


================================================================================
FILE: 3_naming_resources.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Identifiers, Controlled Vocabularies

--- Page 2 ---
Today’s Outline
Brief Review
Identifiers, Identifier Standards
Controlled Vocabularies

--- Page 3 ---
L T
AST IME
• Collection: A group of resources that have been selected for
some purpose.
• Organizing System: An intentionally arranged collection of
resources and the interactions they support.
• Intentional Arrangement: acts of organization by people (or
computational proxies)
Definitions from TDO

--- Page 4 ---
C A C :
REATING OLLECTION
F Q
UNDAMENTAL UESTIONS
The scope and scale of The number and nature The time span or The physical or The relationship of the
the collection of users, and how they lifetime over which the technological organizing system to
will use the collection organizing system will environment in which other ones that overlap
operate the organizing system is with it in domain or
situated scope

--- Page 5 ---
Example:
Smithsonian Natural History Collection

--- Page 6 ---
Smithsonian Collections Management Policy
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 7 ---
S C M P
MITHSONIAN OLLECTIONS ANAGEMENT OLICY
(S E )
ELECTED XCERPTS
“Staff will consider and evaluate the concerns of indigenous source
communities regarding collections items, recordings, information,
collecting activities and use.”
“The Smithsonian repudiates the illicit traffic in objects and specimens that
contribute to the despoliation of museums, monuments, environments,
sites and species resulting in irreparable loss to science and humanity.
Items that have been stolen, unscientifically gathered or excavated, or
unethically acquired should not be made part of Smithsonian collections. “
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 8 ---
S C M P
MITHSONIAN OLLECTIONS ANAGEMENT OLICY
(S E )
ELECTED XCERPTS
“The concept of provenance refers to the history of
ownership of a collection item. Collecting departments
shall exercise due diligence in the acquisition of
collections, including making reasonable inquiries into
the provenance of collections items under consideration
for acquisition consistent with Smithsonian policy.”
Collections records must show decision-making
“
processes of acquisitions evaluation…”
https://naturalhistory.si.edu/research/nmnh-collections/museum-collections-policies

--- Page 9 ---
Last Time

--- Page 10 ---
Resource Examples

--- Page 11 ---
Q A A R
UESTIONS TO SK BOUT ESOURCES
What are the individual resources?
⚫
What is their granularity?
⚫
How do we identify them?
⚫
Which ones are identical?
⚫

--- Page 12 ---
Example Collection Creation: SB1421 Datasets

--- Page 13 ---
E : SB1421 D
XAMPLE ATASET
https://sfpublicdefender.org/copmonitor/
• Resources:
• Selection Policy:
• Organization:
• Interactions:

--- Page 14 ---
N I
AMING AND DENTIFYING

--- Page 15 ---
“I’ll meet you at the place
near the thing where we
went that time.”
Aaron Altman from Broadcast news

--- Page 16 ---
N I
AMING AND DENTIFYING
• Naming resources allows us to understand and talk
about them in relation to other resources.
• Identifiers: global unique ID (GUID) for a resource
allows us to refer to it unambiguously

--- Page 17 ---
I
DENTIFIERS
Identifier: A special type of name assigned in a
controlled way and governed by rules that define
possible values and naming conventions
Identifiers are UNIQUE if they refer to one and only
one resource within some defined context or scope
Identifiers are PERSISTENT if they resolve to the same
referent indefinitely, or as long as needed
18

--- Page 18 ---
W T A R I ?
HY HINK BOUT ESOURCE DENTIFIERS
• We identify resources so we can refer to them,
select them, organize them, interact with them, and
maintain them
• Also, to differentiate them from one another

--- Page 19 ---
H D W I R
OW O E DENTIFY ESOURCES
O W ?
N THE EB

--- Page 20 ---
N R C B T
AMING ESOURCES AN E RICKY
Names need to be
• Unambiguous
• Understandable
• Unite different variations
• Distinguish items that appear similar
We often use a combination
of unique ID and
understandable description

--- Page 21 ---
Dealing with Ambiguity:
Wikipedia Disambiguation Page

--- Page 22 ---
I
DENTIFIERS
Identifiers are UNSTRUCTURED if they have no
inherent meaning based on their values
STRUCTURED identifier schemes have meaning
(Over time they often become less meaningful)
Examples of Unstructured? Student ID #’s
Examples of Structured? URIs for web sites
https://en.wikipedia.org/wiki/Uniform_Resource_Identifier
23

--- Page 23 ---
There are many copies of this book
Many books have the same title
How to identify them?

--- Page 24 ---
ISBN:
I S B N
NTERNATIONAL TANDARD OOK UMBER
• Publishers purchase the numbers from an agency
• Details are nation-specific
• One ISBN is assigned to each edition and variation
• Reprintings do not get new ISBNs, just print numbers
• Different copies of the same book (same edition &
variation) have the same ISBN

--- Page 25 ---
Different Editions; Different ISBNs

--- Page 26 ---
ISBN Format:
• Prefix: currently either 978 or 979
• Registration group: country, geo, language
• Registrant: publisher or imprint
• Publication: edition and format of the item
• Check digit: mathematically validates the rest
of the number (sometimes called a check sum)
www.isbn-international.org/content/what-isbn

--- Page 27 ---
ISBN C D A
HECK IGIT LGORITHM
1. Take the first 12 digits of the 13-digit ISBN
2. Multiply each number in turn, from left to right by a number. The first digit is
multiplied by 1, the second by 3, the third by 1 again, the fourth by 3 again,
and so on to the eleventh which is multiplied by 1 and the twelfth by 3.
3. Add up the 12 answers.
4. Do a modulo 10 division on the result from step 3
5. Take that remainder result from step 4. If zero, then the check digit is zero.
Else subtract the remainder from 10. The answer to that is your check digit.
https://isbn-information.com/check-digit-for-the-13-digit-isbn.html

--- Page 28 ---
2 matches the last digit,
so the check passes
https://isbn-information.com/check-digit-for-the-13-digit-isbn.html

--- Page 29 ---
O W T D R ?
THER AYS O ISTINGUISH ESOURCES
Different model numbers:
https://www.bosch-home.com/us/productslist/dishwashers/top-controls

--- Page 30 ---
O W T D R ?
THER AYS O ISTINGUISH ESOURCES
Why do products have model, part and serial numbers?
Serial numbers are unique and distinguish
between many instances of the same
product/part & model
https://techterms.com/definition/serial_number

--- Page 31 ---
Representations of Names Can Be Noisy
Jones, R. Fingerprinting and Metadata Progress Report. Last.fm: The Blog, 2008., via Hemerly, Making Metadata: The Case of MusicBrainz, SSRN 2011

--- Page 32 ---
C V :
ONTROLLED OCABULARY
T R N N P
RY TO EDUCE THE OISY AME ROBLEM
“A fixed or closed set of description terms in some
domain with precise definitions that is used instead
of the vocabulary that people would otherwise use.”
--
TDO

--- Page 33 ---
T C V
YPES OF ONTROLLED OCABULARIES
Dictionaries
•
Authoritative names
•
Identifier Standards
•
Subject heading lists
•
Classification systems
•
35

--- Page 34 ---
Example: Authors in Google Scholar
Need to Make Names Unambiguous:

--- Page 35 ---
Solutions for Author Name Problem:
Standardize Global Unique ID

--- Page 36 ---
Solutions for Author Name Problem:
ORCID Standardized Global ID

--- Page 37 ---
Solutions for Author Name Problem:
ORCID Standardized Global ID

--- Page 39 ---
I L
DENTIFIERS FOR OCATION
Identifiers that are well-known or highly-structured
⚫
can be used to specify a location
South Hall, University of California, Berkeley
⚫
37.871432,-122.258499 (latitude / longitude)
⚫
This can vary
⚫
Addresses in Japan identify buildings by subdividing
⚫
land into smaller units as needed based on
population density
41

--- Page 40 ---
Controlled vocabulary for
geographic names
(including historical names)

--- Page 42 ---
C V
ONTROLLED OCABULARIES ARE
D C
ESIGN HOICES
• Choices:
• Binary vs multiple
• Static vs dynamic
• Example: Facebook Gender Choices

--- Page 43 ---
D D C
EWEY ECIMAL LASSIFICATION
• Started in 1876 by Melvil Dewey for the Amherst College library
• One of the most widely used classification systems in the world
(especially in public libraries)
• Number system designed for locating books on physical shelves
• Location formerly was based on when the book was acquired
• 10 main classes
• Each has 10 divisions
• Each has 10 sections
• Intended as a “universal classification”
• Cultural biases are very clear today
• Proprietary; licensed by OCLC (an international library system)

--- Page 44 ---
Example:
Dewey
Decimal
System
Top Levels
https://en.wikipedia.org/wikiList_of_Dewey_Decimal_classes

--- Page 45 ---
One set of
divisions of
the Dewey
Decimal
System,
showing its
glaring
cultural
biases
https://en.wikipedia.org/wikiList_of_Dewey_Decimal_classes

--- Page 46 ---
L C C
IBRARY OF ONGRESS LASSIFICATION
• Started in 1800 – war of 1812 burned all the books
• Restarted in 1897 with Thomas Jefferson’s personal,
rather broad collection
• Broader than Dewey, but still biased towards the
needs of government, U.S. in particular

--- Page 47 ---
Library of Congress Subjects

--- Page 48 ---
Library of
Congress
Top Levels

--- Page 50 ---
How are Controlled Vocabularies Used?

--- Page 53 ---
A & A
RT RCHITECTURE
T
HESAURUS
• Controlled vocabulary for art,
architecture and material
culture to “encourage
consistency in cataloging and
more efficient retrieval of
information”
• Hierarchical organization

--- Page 54 ---
Flamenco Project: (Prof Hearst’s Research)
Controlled Vocabulary Integrated Into Faceted Navigation
Flamenco.Berkeley.edu

--- Page 55 ---
Flamenco.Berkeley.edu

--- Page 56 ---
P R D
ROCESS OF ESOURCE ESCRIPTION
• Identify / scope the resources to be described
• Determine the purposes or uses of the descriptions
• Study the resource(s) to identify descriptive properties
• Design the description vocabulary (making use of existing standards)
• Design the description form and implementation
• Create the descriptions (either "by hand" or computational process)
• Evaluate the descriptions

--- Page 57 ---
N W : C
EXT EEK ATEGORIZATION


================================================================================
FILE: 4_categorization_part1.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Categorization, Part 1

--- Page 2 ---
Pyramid Game Rules
Player A: guess the category
Player B: give clues without
using words from the category
ABC
name

--- Page 3 ---
100,000 Pyramid
https://www.youtube.com/watch?v=UJMx_YfZiW0

--- Page 4 ---
W
HY IS THE CONTENT IN THIS COURSE
?
IMPORTANT

--- Page 5 ---
What we are studying about Info Org:
How Built
Information organization choices are influenced by
cognitive, cultural, technical, and historical factors.
Why it Matters
Information organization choices have deep
social, technical, and ethical consequences.

--- Page 6 ---
Course Topics in Organized into One Slide
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

--- Page 7 ---
This week’s focus
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

--- Page 8 ---
Today’s Outline
Who Creates Categories
Principles for Categorization
Classical vs Cognitive Categories

--- Page 9 ---
I C
NTRO TO ATEGORIES
• Categories are sets or groups of resources that are
assigned a common label
• Categories are equivalence classes of sets or groups of
things or abstract entities that we treat the same.
• Categories are involved whenever we perceive,
communicate, analyze, predict, classify
• All human languages and cultures divide the physical
and experiential “worlds” into categories
Glushko -- TDO

--- Page 10 ---
C F T P G
ATEGORIES ROM HE YRAMID AME
• Car Brands: Fiat, Toyota, Ford, Audi
• Spanish Words: “hola”, “Juan”, “despacito”
• Why You Wear a Tuxedo: going to a wedding, going to an awards show,
performing as a magician, pretending to be a penguin
• What a Clock Says: “The time is 10am”, “I have a minute hand and a
second hand”, “I am sitting on your wrist with a band around me”
• Things You Solve: puzzle, problem, for x
What are the
• Things You Climb: mountain, tall ladder, tree, society characteristics of these
category types?

--- Page 11 ---
Which Games / Game Shows
are about Categories?
Categories Individual information
/ facts / data

--- Page 12 ---
L ’ P F F
ET S LAY AMILY EUD

--- Page 13 ---
F F G
AMILY EUD AMEPLAY
• Players in two teams are shown a category name
• They are asked to guess the most likely category members
based on responses from a survey of 100 people.
• A team gets control if they guess the higher-ranking member
• A team wins if they guess all of the top-ranked members
before getting three wrong.

--- Page 14 ---
Example Family Feud Q/A
pollev.com/I202

--- Page 15 ---
Example Family Feud Q/A
https://parade.com/1188030/marynliles/family-feud-questions/

--- Page 16 ---
F F C
AMILY EUD ATEGORIZATION
The “right answers” are based on a response to a
survey. What might we assume about how people
choose what to say?

--- Page 17 ---
W I C C ?
HAT NSPIRES ATEGORY REATION
• Culturally / Cognitively Created Categories
• Individually-Created Categories
• Institutionally-Created Categories
• Mathematically / Scientifically Created Categories
• Computationally-Created Categories
Adapted from Glushko – Categorization in the World

--- Page 18 ---
C / C -C C
ULTURALLY OGNITIVELY REATED ATEGORIES
• We use tens of thousands of categories that are
embodied in our culture and language
• They develop slowly and typically change slowly
• Many have a perceptual or sensorimotor origin
based on natural boundaries or discontinuities in
perception and experience
More on this in a few slides
Glushko -- TDO

--- Page 19 ---
I -C C
NDIVIDUALLY REATED ATEGORIES
• Created to satisfy ad hoc requirements that emerge from an
individual’s unique experiences, preferences, and collections
• Created intentionally in response to specific organizing
requirements, often short-term ones
Glushko – Categorization in the Wild

--- Page 20 ---
I -C C
NSTITUTIONALLY REATED ATEGORIES
• Explicit construction of a category system to enable more control,
robustness, and interoperability than is possible with just the
culturally-shared system
• Are often the collaborative artifact of many individuals who
represent different organizational perspectives
• Usually developed via formal processes (in standards organizations
or legislative bodies or via scientific or mathematical discovery)
and require ongoing governance and maintenance
Example: Controlled Vocabulary Example: Categories based on Principles of Geometry
Glushko – Categorization in the Wild

--- Page 21 ---
I C C S :
NSTITUTIONALLY REATED ATEGORY YSTEM
UN STANDARD PRODUCTS AND SERVICES CODES (UNSPSC)
https://www.ungm.org/Public/UNSPSC

--- Page 22 ---
I C C S :
NSTITUTIONALLY REATED ATEGORY YSTEM
"G I " T C C
ROSS NCOME AX ODE ATEGORIES
Classifies income types
according to taxable or not
https://www.law.cornell.edu/uscode/text/26/61

--- Page 23 ---
I C C S :
NSTITUTIONALLY REATED ATEGORY YSTEM
D P
EFINITION OF THE CLASS OF SHAPES THAT FORM A YRAMID
A pyramid is a polyhedron (a geometric figure)
formed by connecting a polygonal base and a
point, called the apex. Each base edge and
apex form a triangle, called a lateral face. A
pyramid is a conic solid with a polygonal base.
https://en.wikipedia.org/wiki/Pyramid_(geometry)

--- Page 24 ---
I - :
NSTITUTIONALLY CREATED CATEGORY
U.S. C Q , 2020
ENSUS UESTIONNAIRE
Q R
UESTIONS ABOUT ACE
For the motivations for the form of these questions, see
https://www.census.gov/newsroom/blogs/random-samplings/2021/08/improvements-to-2020-
census-race-hispanic-origin-question-designs.html
For info about this question over time, see
https://www.nytimes.com/interactive/2023/10/16/us/census-race-ethnicity.html

--- Page 25 ---
C -C C
OMPUTATIONALLY REATED ATEGORIES
• Created computationally when a collection of
resources or resource descriptions is too large for
people to think about effectively
• “Supervised’ machine learning algorithms can try to find
patterns that characterize human-defined categories
• “Unsupervised” algorithms like clustering find
correlations among attributes to create categories that
may or may not be interpretable by people

--- Page 26 ---
Example: Computationally-created Categories
via Cluster Analysis
Goal: determine personality trait groupings for doctors and nurses
1. Analyzed reddit posts by physicians; assigned scores for ~200 personality characteristics
2. k-Means Clustering of authors’ posts according to these characteristics
3. Performed PCA to visualize the clusters
4. Analyzed the meaning of the clusters
https://www.receptiviti.com/post/harnessing-psycholinguistic-clusters-to-transform-people-insights-into-action

--- Page 27 ---
S ) P I C
( OME RINCIPLES FOR NTENTIONALLY REATING
C
ATEGORIES
Car Brands
Enumeration
⚫
Things that can be Climbed
Single Property
⚫
Expensive & Brittle Things
Multiple Properties
⚫
Goal-based Things you take on a camping trip
⚫
Theory-based Definition of a Triangle
⚫

--- Page 28 ---
P :
RINCIPLE
D C E
EFINING ATEGORIESBY NUMERATION
Car Brands
The simplest way to define a category is by
enumerating (listing) its members
The meaning of a category or concept is NOT a
property; it is simply the specific set of resources
This principle is easy to understand and implement
To learn it, you have to memorize its members
29

--- Page 29 ---
16 German States 29 Indian States

--- Page 30 ---
P :
RINCIPLE
D C A S P
EFINING ATEGORIES BY INGLE ROPERTY
Climbable Things Tall Things
Use only the values of any single property
Intrinsic static properties are often the easiest ones to
use (color, size, shape…)

--- Page 31 ---
S P
INGLE ROPERTY
C :
ATEGORY
“P - S ”
YRAMID HAPED

--- Page 32 ---
P :
RINCIPLE
D C M P
EFINING ATEGORIES BY ULTIPLE ROPERTIES
Things That Are Expensive & Breakable
Items being categorized can be described using
observed “separable” or “combining” properties
33

--- Page 33 ---
Pyramid Human Location?
If using one
Shape? Made?
attribute only,
these all go
Yes Yes Egypt
into the same
category
Yes No Glacier
But if using 3
Park
attributes,
they do not
Yes Yes UC
San Diego

--- Page 34 ---
P :
RINCIPLE
D C R
EFINING ATEGORIES BY A SET OF ULES
Example: Triangles are all and only 3-sided closed
polygons
Example: In the game of baseball, a foul ball is a
batted ball that lands or remains in foul territory, which
is the area outside the foul lines extended from home
plate past first and third base.
35

--- Page 35 ---
P :
RINCIPLE
D C B M
EFINING ATEGORIES Y ULTIPLE
S S P
HARED TATISTICAL ROPERTIES
Example: Document clustering with
BERT vectors
Subakti, A., Murfi, H. & Hariadi, N. The performance of BERT as data representation of text clustering. J Big Data9, 15 (2022).
https://doi.org/10.1186/s40537-022-00564-9
36

--- Page 36 ---
P :
RINCIPLE
D C B G
EFINING ATEGORIES Y OALS
Things to Bring Camping Why You Wear a Tuxedo
The members of each of these categories have few or
no discernable properties in common
These are unlikely to be lexicalized because of their
ad hoc-ness and context-dependence
37

--- Page 37 ---
Classical vs Cognitive Science-Based
Categories

--- Page 38 ---
C V
LASSICAL IEW OF
C
ATEGORIES
• Dates to Plato and Aristotle
• Platonic Ideal: There are two parallel worlds;
one of ideal forms that reflect a higher truth;
the other is physical reality
• These truths are discovered through logical
reflection
PlatofromRaphael'sThe School of Athens(1509–1511)

--- Page 39 ---
C V C
LASSIC IEW OF ATEGORIES
1. Categories are defined by a list of properties
shared by all elements in a category (necessary &
sufficient)
2. Category membership is binary (in or out)
3. Because membership is defined by rules, every
member in the category is equally a member
trian
Example: triangles are 3-
gles
sided closed polygons
3-lined figures Closed polygons
3-lined figures Closed polygons
Stephen Palmer 2002
Triangles

--- Page 40 ---
C V C
LASSIC IEW OF ATEGORIES
1. Categories are defined by a list of properties
shared by all elements in a category (necessary &
sufficient)
2. Category membership is binary (in or out)
3. Because membership is defined by rules, every
member in the category is equally a member
Example: define the category: “birds”
Everything that has feathers?
Everything that flies?
Every living thing with feathers that flies?

--- Page 41 ---
N
AME A CATEGORY WHOSE
MEMBERSHIP CAN BE DEFINED BY A
FEW PRECISE RULES

--- Page 42 ---
C V C
LASSIC IEW OF ATEGORIES
1. Categories are defined by a list of properties
shared by all elements in a category (necessary &
sufficient)
2. Category membership is binary (in or out)
3. Because membership is defined by rules, every
member in the category is equally a member
This view is
- Conceptually simple
- Straightforward for programs to implement
- Rules are very elaborate for real categories
But it is RARELY how people really categorize!

--- Page 43 ---
N S G
AME OME AMES

--- Page 44 ---
W A N S
HAT RE ECESSARY AND UFFICIENT
C S
ONDITIONS FOR OMETHING TO BE A
G ?
AME
A list of properties shared by all elements?

--- Page 45 ---
W ’ F E
ITTGENSTEIN S AMOUS XAMPLE OF
“G ”
AME
• No common properties are shared by all games
• Competition: card games, ball games, Olympic games
• Developmental play: Children’s games
• Luck: dice games
• Skill: chess
• No fixed boundary; can be extended to new games
• Video games
• Alternative to Classical Categorization theory:
• Concepts related by Family Resemblances

--- Page 46 ---
https://www.jesperjuul.net/ludologist/2015/06/10/what-is-a-game-redux/

--- Page 47 ---
I W ’ E
MPLICATIONS OF ITTGENSTEIN S XAMPLE
There may be defining features for typical
⚫
members
But there are no features that are necessary and
⚫
sufficient for all examples of the category
Different members vary substantially in how
⚫
typical or representative they are

--- Page 48 ---
P C V
ROBLEMS WITH THE LASSICAL IEW
It does not reflect how people categorize:
• People do not rely on abstract definitions or
lists of shared properties (Rosch 1973)
Example: Are curtains furniture?
• Some members are more typical than others
Example: Chicken as a bird vs eagle as a bird
• At least some aspects of categorization seem
to reflect the human body and mind
Examples: Color categories, emotion categories

--- Page 49 ---
N T
EXT IME
• Cognitive / linguistic influence on categorization,
cont.
• The institutionally created category of genre


================================================================================
FILE: 5_categorization_part2.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Categorization, part 2

--- Page 2 ---
This week’s focus
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

--- Page 3 ---
Today’s Outline
Let’s Play a Game!
Prototypes vs Classical Categories
Basic-Level Categories
Language, Concepts, & Categories
Genre as a Category

--- Page 4 ---
L ’ P G !
ET S LAY ANOTHER AME

--- Page 5 ---
The Game of Set
https://smart-games.org/en/set/start

--- Page 6 ---
T G S
HE AME OF ET
A SET consists of 3 cards
Each card has a color, shape, number, and fill
For each of the 4 categories, the 3 cards must display that category as either
a) all the same, or
b) all different.
Put another way: For each category, the 3 cards must avoid having 2 cards showing
one version of the category and the remaining card showing a different version.
https://smart-games.org/en/set/start

--- Page 7 ---
T G S
HE AME OF ET
What kind of category system is this?

--- Page 8 ---
T S G C
HE ET AME AND ATEGORIZATION
• Attributes: color, shape, number, texture
• Rule: a set must consist of exactly three cards; all 3
cards must be the same for an attribute or all different
• These rules define:
• What IS a set
• What IS NOT a set
• The game’s rules define a classic category

--- Page 9 ---
Classical vs Cognitive Science-Based
Categories

--- Page 10 ---
C / C -C
ULTURALLY OGNITIVELY REATED
C
ATEGORIES
• Learned implicitly through development via parent-
child interactions, language, and experience
• Learned explicitly through education, reading
• Informal and contextual acquisition makes
categories flexible, creative, generative, and biased
Glushko -- TDO

--- Page 11 ---
W A N S
HAT RE ECESSARY AND UFFICIENT
C S
ONDITIONS FOR OMETHING TO BE A
G ?
AME
A list of properties shared by all elements?

--- Page 12 ---
D G
EFINING AME WITH
N & S C
ECESSARY UFFICIENT ONDITIONS
• All activities that are done for amusement?
• Would include movies, hanging with friends …
• All activities that have rules?
• Would include school, filling out forms
• All activities that involve competition?
• Would include non-game activities like racing, competing for jobs
• All activities done for amusement that have rules and involve competition?
• Would exclude jigsaw puzzles – not competitive
• All activities done for amusement that have rules?
• Would exclude a child tossing a ball and seeing how many times she
can catch it

--- Page 13 ---
M -W : G
ERRIAM EBSTER AME
Splits it up into different concepts:
• “A physical or mental competition conducted according to
rules with the participants in direct opposition to each
other
• Activity engaged in for diversion or amusement : PLAY
• A procedure or strategy for gaining an end : TACTIC”
Different definitions signal a category whose
members are not described by all of the same
properties

--- Page 14 ---
O D : GA
XFORD ICTIONARY ME
“Game: form of play or sport, especially a
competitive one played according to rules and
decided by skill, strength, or luck.”
“or” and “especially” signal a category whose
members are not described by all of the same
properties

--- Page 15 ---
G E : T P
AME XAMPLE HE OINT
• There is no single set of features all members
have in common -- no single all-encompassing
definition
• Boundaries of categories are (usually) not
accurately described by one rule.

--- Page 16 ---
P C V
ROBLEMS WITH THE LASSICAL IEW
It does not reflect how people categorize:
• People do not rely on abstract definitions or
lists of shared properties (Rosch 1973)
Example: Are curtains furniture?
• Some members are more typical than others
Example: Chicken as a bird vs eagle as a bird
• At least some aspects of categorization seem
to reflect the human body and mind
Examples: Color categories, emotion categories

--- Page 17 ---
P : E R
ROTOTYPES LEANOR OSCH
• UC Berkeley linguistics professor (emeritus)
• Influential work on mental categorization and prototypes
• Prototype theory: People rely less on abstract
definitions of categories than on a comparison of the
given object or experience with what they deem to be
the object or experience best representing a category
("prototype")
https://en.wikipedia.org/wiki/Eleanor_Rosch

--- Page 18 ---
P
ROTOTYPES
• A prototype is a single best example that captures
the “central tendency” of a category
• The prototype can be thought of as an average of
the features of the category members.
• Gradience in category membership: some
members of the category are “better” examples
than others

--- Page 19 ---
G C M
RADIENCE IN ATEGORY EMBERSHIP
Which bird is most typical of the category bird?
Least typical? What does it mean that you can answer this question?

--- Page 20 ---
P
ROTOTYPES
• Which dog breed is central?
• Which are “better” or “worse” examples?
https://articles.hepper.com/great-dane-chihuahua-mix/ https://www.pageant.dog/breeds/dog/1120-Labrador-Retriever

--- Page 21 ---
E F P
VIDENCE OR ROTOTYPES
Typicality ratings
Order in which members are named
Time needed to verify category membership
Istockphoto.com

--- Page 22 ---
W
HAT IS THE DIFFERENCE
P
BETWEEN A ROTOTYPE AND A
S ?
TEREOTYPE

--- Page 23 ---
P . C V
ROTOTYPE VS LASSICAL IEW
• Prototype view: we start by having prototypes pointed
out to us and then cluster other things around them
• Definitional view (classical): we start with criteria and
then find some good examples
• Prototype view: the boundaries of a category are fuzzy
• Definitional view: draws sharp boundary lines

--- Page 24 ---
W E T
HAT IS THE FIRST XAMPLE HAT
C T M A …
OMES O IND OF
Vehicle

--- Page 25 ---
Example: Vehicle
More Prototypical is closer to the center
There is a gradience in category membership

--- Page 26 ---
W D C
HAT ETERMINES ATEGORY
P ?
ROTOTYPES
• Purpose categories – the key concepts center around what is
most needed for that purpose
• Things you need for international travel: suitcase, passport
• Things you need for camping: tent, sleeping bag
• Natural objects categories – key concepts center around:
• their appearance, what they might do to us (snakes)
• Human made object categories – key concepts center around:
• how they are used, how we physically interact with them
• Human psychological categories – key concepts center around:
• how they make us feel, how they impact our social interactions

--- Page 27 ---
C L
ATEGORY EVELS
• Basic-level categories: those categories at the highest level
of generality that still share many common attributes and
have fewer distinctive attributes (shape, color, size, etc.)
• Super-ordinate: more general categories such as animal will
share fewer attributes and demonstrate more variability.
• Subordinate: more specific categories, such as American
Robin will share even more attributes among themselves
Rosch. Principles of categorization. Cognition and Categorization, pages 27–48, 1978.
Ordonez, et al. "Predicting entry-level categories."IJCV 115.1 (2015): 29-43.

--- Page 28 ---
Superordinate and Subordinate
Levels
SUPERORDINATE animal furniture emotion
What are other
BASIC LEVEL dog chair happy
examples?
SUBORDINATE terrier rocker joy
• Children take longer to learn superordinate
• Superordinate not associated with mental images or motor
actions

--- Page 29 ---
W -
HAT IS SPECIAL ABOUT BASIC LEVEL
?
CATEGORIES
• They are typically learned first by children
• They are the most commonly used in language
• However, this can differ by culture
• People tend to recognize basic level categories
more quickly than super- or sub-ordinate categories
• Related to, but not the same as, prototypes

--- Page 30 ---
H D C L
OW O HILDREN EARN
C ?
ATEGORIES
• Prototype Formation: A process in which a single best
example (a prototype) is abstracted from experience with
different category members and stored in memory
• Infants shown images of specific animals from the same
category that varied on size of features responded to a novel
prototype animal as if it was more familiar than the detailed
examples they had seen.
• As children age, they become able to use other methods of
categorization, including using rules to learn new categories.
However, if the rules are complex, young children cannot form
the categories without prototype examples
Klinger & Dawson, Prototype formation in autism, Development & Psychopathy, 2001

--- Page 31 ---
P . C I
ROTOTYPE VS LASSICAL N
P
RACTICE
• Although we think more naturally with fuzzy boundaries, we
sometimes are forced to make sharp distinctions
• Example: Dept of Motor Vehicles has to classify which
vehicles require licenses
• Answers questions like:
• Licenses?
• Helmets?
• Sidewalk?

--- Page 32 ---
W C
HAT ARE THE ALIFORNIA
DMV C E-B ?
LASSES OF IKES
W NOT E-B ?
HAT ARE IKES
How do licenses, rules for use, vary among them?

--- Page 33 ---
Language? Concepts?
Which determines which?

--- Page 34 ---
Whorf’s Hypothesis:
Does Language Constrain concepts?
Does your language have different words for each of
these kinds of knives? If it doesn’t, can you still
distinguish them?

--- Page 35 ---
L C
ANGUAGE AND ATEGORIES
• Languages differ a great deal in the words they contain
(which concepts are lexicalized)
• The Whorfian hypothesis: language influences the
speaker’s worldview
• But we can understand concepts that we don’t have
words for.
• The current consensus is that the strong Whorf
hypothesis is incorrect, but language can influence
rather than determine thought.
42

--- Page 36 ---
C N R
OLOR AMING ESEARCH
• Berlin & Kay: there seem to be 11 basic color categories across 110
cultures
• Cultures disagreed about which subset of 11 colors count as basic
• Russian: no single word for blue
• French: no word for brown
• Rosch studied a tribe in New Guinea called the Dani that had only 2.
Nonetheless, the Dani recognized the same 11 basic colors more
quickly than non-basic colors.
• In other words, universally, some reds are better examples than other
reds.
• This also suggests that something embodied in determining what is a
“good” version of a color

--- Page 37 ---
C N R
OLOR AMING ESEARCH
• World color survey: 110 languages of non-industrialized
societies, 330 color chips
• Findings: the best-example choices for color terms in these
languages cluster near the prototypes for
English white, black, red, green, yellow, and blue
Terry Regier
Focal colors are universal after all, Regier, Kay, Cook, PNAS 2005 Paul Kay
Prof, UCB Linguistics
Prof, UCB Linguistics

--- Page 38 ---
Example Color Naming Systems
Color naming reflects optimal partitions of color space, Regier, Kay,Khetarpal, PNAS 2007

--- Page 39 ---
Bannert, Bartels, Journal of Neuroscience 8
Chang, NYTimes, 9/8/25 September 2025,

--- Page 40 ---
C N : T P
OLOR AMING HE OINT
• Categories develop as a combination of
cultural, cognitive, and experiential factors
• Color naming provides evidence that this
category has a prototype structural across all
cultures

--- Page 41 ---
C A B
ONCEPT CQUISITION AND LINDNESS
• Recent studies show that blind people acquire most of the same concepts as sighted
people in the same culture
• Much of this is conveyed through language and concept formation
• For example, blind and sighted adults are
• Equally likely to infer that two bananas (natural kinds) and two stop-signs (artifacts
with functional colors) are more likely to have the same color than two cars
(artifacts with non-functional colors)
• Make similar inferences about novel objects’ colors
• There are some differences in how blind vs sighted people make similarity judgements
about fruits and vegetables, but not about human made artifacts.
Kim, et al. "Shared understanding of color among sighted and blind adults." PNAS (2021).
Connolly et al. "Effect of congenital blindness on the semantic representation of some everyday concepts." PNAS 2007

--- Page 42 ---
How do we deal with a new concept?
Ordonez, et al. "Predicting entry-level categories."IJCV 115.1 (2015): 29-43.

--- Page 43 ---
W
HAT IS THE RELATIONSHIP BETWEEN NEW
?
CONCEPTS AND CATEGORIES
• We learn new concepts all the time
• Being “cancelled”
• The blockchain
• Fire-induced storms
• E-sports
• We can
• Slot it into an existing category, perhaps expanding its
boundaries
• Start to create a new category with this concept as a
prototype
• Shift boundaries of multiple existing boundaries

--- Page 44 ---
G
ENRE
Definition (Oxford Languages): “A category of artistic composition, as in
music or literature, characterized by similarities in form, style, or subject
matter”

--- Page 46 ---
W G ?
HAT IS ENRE
• “Any widely recognized class of texts defined by some common
communicative purpose or other functional traits, provided the
function is connected to some formal cues… and is extensible.”
• Genre is a heterogenous classificatory system, based on
• How the text was created
• The way it is distributed Register corresponds to context of use:
a formal speech, a broadcaster, talking
• The register of language it uses
with friends all have different registers.
• The kind of audience it is addressed to
• Examples: reporting, legal, scientific articles, fiction, editorial
Kessler, Nunberg, Schuetze. “Automatic Detection of Text Genre.”EACL 1997

--- Page 47 ---
most popular genres on the Billboard Top 100 chart
between 1980 and 2009
https://ibruins.weebly.com/visualizations.html

--- Page 48 ---
P G R
ETRUSICH ENRE EADING
R Q R
AISES UESTIONS ABOUT THE OLE OF
G
ENRE
• Do the changing patterns of consumption of
commercial music make genre irrelevant /
antiquated?
• What is the link between identity and musical
genre?
• Are there fixed attributes for defining musical
genre?
• How have non-musical attributes been "tangled up
in discriminatory ideologies”.

--- Page 49 ---
A D S
BOUT ISCUSSION OF ENSITIVE
T
OPICS
Practice Active Listening: listening to learn, understand, integrate and contribute
Carefully attend to what is said
Not just how it relates to what you want to say
Respect others’ knowledge
Give people the benefit of the doubt
We have different experiences and priors, and come with different knowledge and
sensitivities.
Ask other parties to explain what is meant
Ask for clarifications or further explanation of ambiguous ideas or statements
No question is a bad question.
Address disagreement purposely and openly
Acknowledge and clarify differences
Don’t impute justifications or intent, solicit more information
Critique the idea, not the person
Slide by Deirdre Mulligan

--- Page 50 ---
S : C
UMMARY ATEGORIZATION
Categorization is central to how we organize information
and the world
Categories are involved whenever we communicate,
analyze, predict, or classify
Categorization is much messier than our computer
systems and applications would like
But understanding how people (and each of us)
categorize can help us design better systems and
interfaces

--- Page 51 ---
• Terrific and fun book on
categorization and organization
• Written by an entertaining author
• His other books are great too

--- Page 52 ---
Fun online book

--- Page 53 ---
This week’s focus
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

--- Page 54 ---
Next week’s focus
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation


================================================================================
FILE: 7_faceted_structures.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 7: Faceted Categories & Navigation

--- Page 2 ---
Today’s Outline
Hierarchy / Taxonomy Brief Review
Faceted Categories
Review Tuesday’s Exercise
Facets vs Tags

--- Page 3 ---
W D H ?
HAT EFINES A IERARCHY
• Can be represented as a tree data structure:
• There is one root node
• Every node except the root has exactly one parent
• How this relates to category systems:
• Nodes represent categories and subcategories
• An information item is assigned to one and only
one node at some level in the tree

--- Page 5 ---
Primary Ways to Visualize Trees / Hierarchies

--- Page 6 ---
Showing a
Hierarchical Org
Chart with a Node
and Link Diagram
http://neuroscience.berkeley.edu/about/

--- Page 7 ---
Using Enclosure to Show Nested Hierarchy
The Treemap visualization organizes stocks into a hierarchy by their market sector.
The relative size shows the market capitalization, and color shows if prices increased or decreased
https://github.com/armanfeyzi/Treemap-chart

--- Page 8 ---
Often the Entire Tree is Too Large to Visualize

--- Page 9 ---
Tree vs Network (Graph): Comparison
BASIS FOR
TREE GRAPH
COMPARISON
Root node It has exactly Graph doesn't
one root node. have a root
node.
Cycles No cycles are Graph can have
permitted. cycles.
Complexity Less complex More complex
comparatively
Number of n-1 (n = # of Not defined
edges nodes)
Model type Hierarchical Network
https://techdifferences.com/difference-between-tree-and-graph.html

--- Page 10 ---
Family Trees?
(Not really a tree ... people have more than one parent)
Fred Wilma Barney Betty
Pebbles Bam-Bam

--- Page 11 ---
Family Trees
(And people can be distantly related, thus making a cycle…)
Groor Urg
Fred Wilma Barney Betty
Pebbles Bam-Bam

--- Page 12 ---
H T
IERARCHY VS AXONOMY
• Both have hierarchical organization
• In a taxonomy, the principle for containment is consistent all the
way down the tree;
• is-a
• part-of
• By contrast, for a hierarchy, the principle for containment can vary
• In our find-the-bird hierarchy, the first level is sound-of, the
second level is habitat-of, and the third level is beak-type-of

--- Page 13 ---
Hierarchy Composed of a Mix of Concepts Is
Difficult to Navigate
Chirps > Swimmer > Stubby beak
Long beak
Tree climber > Stubby beak
Long beak
Ground Based > Stubby beak
Long beak
Caws > Swimmer > Stubby beak
Long beak
Tree climber > …
Ground Based
Sings > Swimmer
Tree climber
Ground Based

--- Page 14 ---
Solution:
For easier navigation, we should break these
out into different features or facets
Sound: Chirps, Caws, or Sings
Habitat: Swimmer, Tree climber, or Ground Based
Beak: Stubby or Long
Navigation systems with faceted categories let the
information explorer decide which facet to start with.
By using facets instead of hierarchy, we don’t have
to repeat each category multiple times.

--- Page 15 ---
Say I want to classify these animals along 4
features
otter
penguin
robin
salmon
wolf
cobra
bat

--- Page 16 ---
A hierarchy requires me to repeat the
features
otter
start
penguin
robin
salmon
wolf
cobra
bat Locomotion: swim fly run
slither
Covering:
fur scales feathers fur scales feathers fur scales feathers …
Diet: fish fish fish fish fish fish
fish fish fish
rodents rodents rodents rodents rodents rodents
rodents rodents rodents
insects insects insects insects insects insects
insects insects insects
otter salmon bat robin wolf

--- Page 17 ---
T P H
HE ROBLEM WITH IERARCHY
• Inflexible
• Force the user to start with a particular category
• What if I don’t know the animal’s diet, but the interface
makes me start with that category?
• Wasteful
• Have to repeat combinations of categories
• Makes for extra clicking and extra coding
• Difficult to modify
• To add a new category type, must duplicate it
everywhere or change things everywhere

--- Page 18 ---
F C
ACETED ATEGORIES
Also known as Faceted Metadata, Faceted Navigation, Faceted Search, Poly
Hierarchy

--- Page 19 ---
F C
ACETED ATEGORIES
• A set of categories for describing a collection
• Each facet has distinct attributes, or features
• Resources are identified by multiple
categories
• Each category can be hierarchical
• Consistent relationship within each hierarchy

--- Page 20 ---
Example: Recipes
Would you want to browse
a hierarchy of recipes?
https://www.epicurious.com/recipes/food/views/fish-tacos-352976

--- Page 21 ---
Faceted Categories Instead!
Occasion > Party
Dish > tacos Cuisine > Mexican
Occasion > Tailgate
Ingredients > Cod
Ingredients > Red Onion Preparation > Saute
Ingredients > Tortilla
https://www.epicurious.com/recipes/food/views/fish-tacos-352976

--- Page 22 ---
We can (optionally) add hierarchy to facets
Occasion > Party
Dish > main > tacos Cuisine > Mexican
Occasion > Tailgate
Ingredients > Meat > Fish > Cod
Ingredients > Veg > Onion > Red Onion Preparation > Saute
Ingredients > Bread > Tortilla
https://www.epicurious.com/recipes/food/views/fish-tacos-352976

--- Page 23 ---
C F
ONSTRUCTING ACETS
• Break out each of the important concepts into their own facet
• Sometimes the facets are flat, sometimes hierarchal
• Assign labels to items from multiple facets
Preparation Method Fruits
Desserts
Fry Cherries
Cakes
Saute Berries
Cookies
Boil Blueberries
Ice Cream
Bake Strawberries
Sorbet
Broil Bananas
Flan
Freeze Pineapple

--- Page 24 ---
A F
SSIGNING ACETS
This allows multiple navigation paths to each
item
Preparation Method Fruits
Desserts
Fry Cherries
Cakes
Saute Berries
Cookies
Boil Blueberries
Ice cream
Bake Strawberries
Sorbet
Broil Bananas
Flan
Freeze Pineapple
Fruit > Pineapple Dessert > Sorbet
Dessert > Cake Fruit > Berries > Strawberries
Preparation > Bake Preparation > Freeze

--- Page 25 ---
Flamenco Project: (Prof Hearst’s Research in the 2000’s)
Theory and Practice behind Faceted Navigation

--- Page 26 ---
E :
XAMPLE
N P W
OBEL RIZE INNERS
Faceted navigation to easily explore who won which awards when
(Data from 2004)

--- Page 27 ---
Overview shows which categories are
available

--- Page 28 ---
Let’s start with the prize for literature

--- Page 29 ---
This shows us that 40 people have won for literature, 10
women, 91 men, and the country distribution

--- Page 30 ---
Let’s group these results by the decade they were
awarded in

--- Page 31 ---
Let’s narrow down to just the literature winners in
the 1920s.

--- Page 32 ---
Notice we’ve made a complex query with a few easy
clicks: Prize > literature AND Year > 1920s

--- Page 33 ---
Now we can broaden the query by removing one of the
categories; this changes the query to just Year > 1920s

--- Page 34 ---
The Flamenco system allowed the user to group by a
category value; this hasn’t been adopted by commercial
systems

--- Page 35 ---
Notice across all of the 1920’s, only 2 women got awards,
while 52 men did. Let’s drill into 1921

--- Page 36 ---
Einstein! Cool! Let’s take a detailed look at him

--- Page 37 ---
We can see the associated metadata and categories in the item
description

--- Page 38 ---
Let’s do a free text search, find all the
awardees from California

--- Page 39 ---
Note that category structure remains after the free text search

--- Page 40 ---
So let’s select a category -- economics

--- Page 41 ---
Cool! Economists from UC Berkeley!

--- Page 42 ---
F M A F
ACETED ETADATA LLOWS FOR LEXIBLE
N
AVIGATION
• Faceted categories can be taxonomic
• They can also be based on properties of the
items being organized
• They can also be based on cross-cutting
themes

--- Page 43 ---
Mixing and Matching with Facets
If you have cleanly separated facets, you can mix and
match in really powerful ways, to allow many kinds of
navigation
clothing
equipment age
season
gender
sport
size color price material

--- Page 44 ---
Mixing and Matching with Facets
This boot can be assigned to clothing, season
sport, age, gender, size, color, price, and material,
and shown under any combination of these.
clothing
equipment age
season
gender
sport
size color price material

--- Page 45 ---
Mixing and Matching with Facets
These items can appear after filtering by sport > snowboarding
clothing
equipment age
season
gender
sport
size color price material

--- Page 46 ---
Mixing and Matching with Facets
Filtering by equipment > snowboards narrows these
results to contain only snowboards
clothing
equipment age
season
gender
sport
size color price material

--- Page 47 ---
Mixing and Matching with Facets
Filtering by age can then narrow the results to show
only kids’ snowboards
clothing
equipment age
season
gender
sport
size color price material

--- Page 48 ---
Quantitative facets
For quantitative facets like price, a range of numbers
is often better than selecting individual values
clothing
equipment age
season
gender
sport
size color price material

--- Page 49 ---
H T D F ?
OW O EFINE THE ACETS
• Depends on the makeup of the collection
• Depends on the goals of the use
• Then ask the classic questions:
• Who, what, when, where, why, how much,
under what circumstances?

--- Page 50 ---
A F M
DVANTAGES OF ACETED ETADATA
• Seamless to add new facets and subcategories
• Seamless to add new items.
• Helps with “categorization wars”
• Don’t have to agree exactly where to place
something
• Can be implemented using a relational database.
• May be easier for automatic categorization

--- Page 51 ---
A F N
DVANTAGES OF ACETED AVIGATION
• Helps user infer what is in the collection;
• Evokes a feeling of “browsing the shelves”
• Let’s the user decide how to start, what order to apply
categories
• Seamlessly integrates keyword search with the organizational
structure
• Easy to build complex queries without encountering empty
results
• After refinement, categories that are no longer relevant to the
results disappear.

--- Page 52 ---
D F N
RAWBACKS OF ACETED AVIGATION
• Need good metadata
• There are many ways to design the interface
poorly
• Not showing results, just categories
• Not showing item count previews
• Not integrating search
• Not allowing flexible breadcrumb manipulation
• Poor layout, poor font use

--- Page 53 ---
H
OW DID YOU
ORGANIZE THE
?
ART IMAGES

--- Page 54 ---
C
HALLENGES
• Navigate by the subjects / content of the images
• Can you find images with both birds and trees?
• Can you find images with red birds?
• Can you find images with trees in a seascape?
• Need to support metadata-type attributes in all
combinations
• Price
• Artist (this needs to be in a hierarchy, or an
alphabetical list)
• Print type
• Size

--- Page 55 ---
Hierarchical Facets Focused on Metadata
(does not have subject of painting)

--- Page 56 ---
Flat Facets with an
underspecified subject category

--- Page 57 ---
Hierarchical structure on top
Interlinked structure beneath
No subject category

--- Page 58 ---
Hierarchical Taxonomy
for subject
Hierarchical Taxonomy
for perspective
Size and orientation
combined into a
hierarchy
(so orientation has to
be repeated 3 times,
under large, medium
and small)

--- Page 59 ---
Attempted to cover subject matter
However, concepts are mixed; living things are under
scenic, but what about non-scenic bird pictures?

--- Page 60 ---
Very clean
hierarchical faceted
organization
Still missing some
subjects like trees
and metadata like
print type

--- Page 61 ---
A mixture of
organizations with
some facets and
some mixed
hierarchy

--- Page 62 ---
T C C B M
HESE HALLENGES AN E ET WITH
W -D H F
ELL ESIGNED IERARCHICAL ACETED
M
ETADATA
• The key is to be able to mix from many different hierarchically-organized
attributes
• Say I want to see seascapes with pine trees in them that are romantic style,
in acrylic, cost less than $100, silver print, and with blue and red colors in
them
• Well-designed hierarchical facets can allow me to do this navigation:
• Media > Paint > Acrylic
Subject matter is often the
• Style > Romantic
hardest part because it is huge!
• Subject > Nature > Trees > Pine Trees
• Subject > Nature > Sea/Ocean
• Color > Red
• Color > Blue
• Price > 50 … 100
• Currency: USD

--- Page 63 ---
H E
OMEWORK XERCISE
• Three different questions
• Question 3 shows some images and asks you
to first make a hierarchical taxonomy and then
make a faceted category system

--- Page 64 ---
N W
EXT EEK
• Information Architecture and Categories
• Ontologies
• Overlapping categories / clusters


================================================================================
FILE: 8_ontologies.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 8: Ontologies; Tags vs Faceted Categories

--- Page 2 ---
Today’s Outline
Ontology Definition
Ontology Examples
AI-based Ontologies
Tags vs Faceted Categories
Upcoming Assignment

--- Page 3 ---
Exercise:
Create Faceted Metadata for this Image
Photo by J. Hearst, jhearst.typead.com

--- Page 4 ---
Faceted Metadata Limitation: Shows
Attributes,
but not Relations between Attributes
Aquamarine
Red
Orange
Door
Doorway
Wall
Photo by J. Hearst, jhearst.typead.com
▪ Does not show which color is associated with which object
▪ Ontologies can help with this limitation

--- Page 5 ---
W I O ?
HAT S AN NTOLOGY
Gr: onto: being, logy: science, study-of
• An information structure that often includes
relationships, constraints, and rules.
• A method for describing knowledge in such a way that it
can be process by a computer.
• The branch of metaphysics dealing with the nature of
being (we are not using this sense of the word)
Jean-Baptiste, Ontologies with Python, O’Reilly, 2020
Oxford dictionary https://www.forbes.com/sites/cognitiveworld/2019/03/24/taxonomies-vs-ontologies

--- Page 6 ---
Taxonomy vs Ontology
Taxonomy Ontology
• Tree / Hierarchical in structure
• Graph / Network in structure
• Relation is consistent throughout
• Relations can link objects to each
and signifies inclusion
other without inclusion
• (type-of, part-of)
• Relations can represent attributes
• Often includes rules and constraints

--- Page 7 ---
Tree vs. Network / Graph
BASIS FOR
TREE GRAPH
COMPARISON
Root node It has exactly Graph doesn't
one root node. have a root
node.
Cycles No cycles are Graph can have
permitted. cycles.
tree graph Complexity Less complex More complex
comparatively
Number of n-1 (n = # of Not defined
edges nodes)
Model type Hierarchical Network
https://techdifferences.com/difference-between-tree-and-graph.html

--- Page 8 ---
O E :
NTOLOGY XAMPLE
I T
NFERENCES FROM A AXONOMY
This is a partial taxonomy
is-a
Insect
is-a
Fish is-a
is-a
We can make inferences from taxonomies, using the implicit is-a rule:
𝑖𝑠𝑎 𝑎, 𝑏 𝐴𝑁𝐷 𝑖𝑠𝑎 𝑏, 𝑐 → 𝑖𝑠𝑎(𝑎, 𝑐)
In English:
A pike is a fish; a fish is an animal, therefore a pike is an animal.
Jean-Baptiste, Ontologies with Python, O’Reilly, 2020

--- Page 9 ---
O E
NTOLOGY XAMPLE
R L A A I
ELATION INKS LLOW DDITIONAL NFERENCES
We’ve added eats() relation links; Now it is an ontology
We can now make new rule and inferences; create new knowledge
𝑒𝑎𝑡𝑠 𝑎, 𝑏 𝐴𝑁𝐷 𝑒𝑎𝑡𝑠 𝑏, 𝑐 → 𝑒𝑎𝑡𝑠(𝑎, 𝑐)
Humans (indirectly) eat roaches
Jean-Baptiste, Ontologies with Python, O’Reilly, 2020

--- Page 10 ---
O E :
NTOLOGY XAMPLE
A M I R
DDING ORE NFERENCE ULES
More rules, more inferences
𝑐𝑜𝑛𝑐𝑒𝑛𝑡𝑟𝑎𝑡𝑒𝑑𝐼𝑛 𝑝, 𝑎 𝐴𝑁𝐷 𝑒𝑎𝑡 𝑏, 𝑎 → 𝑝𝑜𝑖𝑠𝑜𝑛𝑒𝑑𝐵𝑦 𝑏, 𝑝
𝑒𝑎𝑡𝑠 𝑎, 𝑏 𝐴𝑁𝐷 𝑒𝑎𝑡𝑠 𝑏, 𝑐 → 𝑒𝑎𝑡𝑠(𝑎, 𝑐)
Can conclude:
Pike poisonedBy PCB, therefore
Human poisonedBy PCB
Jean-Baptiste, Ontologies with Python, O’Reilly, 2020

--- Page 11 ---
O E :
NTOLOGY XAMPLE
A R T
DD MORE ELATION YPES
Jean-Baptiste, Ontologies with Python, O’Reilly, 2020

--- Page 12 ---
O E : IMDB
NTOLOGY XAMPLE

--- Page 14 ---
IMDB Entry: Actor
Many relation types!
Acted in
Photos of
Biography of
Known for
Related News
All actors born on Sept 16
All actors born in 1924
All actors with same
astrological sign
Nicknames
Quotes

--- Page 15 ---
IMDB Entry: Performance
Many relation types!
Directors
Writers
Actors
Screenwriters
Awards
Reviews
Box office statistics
Videos
Photos
Plot
Quotes
Soundtracks

--- Page 19 ---
U D O
SES FOR IGITAL NTOLOGIES
• Store and re-use knowledge
• WordNet (will see next week)
• Link Knowledge Across Different Sources
• Semantic Web
• Knowledge Graphs
• Automated Reasoning
• Formal logic, theorem provers, inference engines
• HermiT OWL reasoner, Pellet reasoner
• Graph-based Neural Net reasoning
Jean-Baptiste, Ontologies with Python, O’Reilly, 2020

--- Page 20 ---
U & N O
NDERSTANDING AVIGATING NTOLOGIES
• Ontologies are difficult to show visually
• They are also difficult to convey how to navigate
• Most websites provide a simplified view
• Inference is not usually done where users can see it, but instead
used behind the scenes to generate information
• Inference is usually much slower than what a relational database
can do equivalently

--- Page 21 ---
E : L -
XERCISE INKED IN
• What are the relations?
• What inferences can be done with them?
• How does the interface express these?

--- Page 22 ---
I O D
NFERRING NTOLOGIES FROM ATA
• We can impose connections by following links
• The “6 degrees of Kevin Bacon game”
• Play it with Wikipedia links to find new knowledge

--- Page 23 ---
https://www.sixdegreesofwikipedia.com/

--- Page 25 ---
T S W
HE EMANTIC EB
• Goal: support a distributed Web at the level of the data rather
than at the level of the presentation.
• A data item can point to another, using global references called
Uniform Resource Identifiers (URIs).
• Information about an entity can be distributed over the entire
web.
• A way to achieve inter-operability
• Lots of people work on it, but it doesn’t seem to ever really take
off in practical applications
Allemang & Hendler, Semantic Web for the Working Ontologist, O’Reilly, 2nd edition, 2011

--- Page 26 ---
G O (GO)
ENE NTOLOGY
• Annotates gene function: molecular function,
cellular components, biological processes
• Under active development for > 25 years!
• 40,000 terms
• 134,000 relations
• >9M annotated gene products

--- Page 27 ---
Go Ontology Example
https://www.ebi.ac.uk/QuickGO/term/GO:0060887

--- Page 28 ---
BRCA1 Gene Example
https://pantherdb.org/treeViewer/treeViewer.jsp?book=PTHR11289&species=agr&seq=HGNC=1101

--- Page 29 ---
BRCA1 Gene Example
https://pantherdb.org/treeViewer/treeViewer.jsp?book=PTHR11289&species=agr&seq=HGNC=1101

--- Page 30 ---
BRCA1 Gene Example: Disease Associations
https://www.alliancegenome.org/gene/HGNC:1101

--- Page 31 ---
BRCA1 Example
https://www.alliancegenome.org/gene/HGNC:1101

--- Page 32 ---
BRCA1 Example
https://www.alliancegenome.org/gene/HGNC:1101

--- Page 33 ---
BRCA1 Example
https://www.alliancegenome.org/gene/HGNC:1101

--- Page 34 ---
M G K B
AINTAINING THE O NOWLEDGE ASE
• Experts read papers and enter information
• Other experts can report on problems
• The standards used have evolved over time, reflecting improvements in
knowledge representation
• Algorithms analyze the data to help researchers analyze results and
formulate hypotheses
• Visualizations have been developed to simplify the complexity

--- Page 35 ---
E : E G O
XERCISE XPLORE THE ENE NTOLOGY
• Search for quickgo, click on the link
• https://www.ebi.ac.uk/QuickGO/annotations
• Explore one of the genes there
• See what kinds of relations are supported
• Follow links, look at the graph structures, the Ontology
tab

--- Page 36 ---
A Collection of Ontologies
http://www.obofoundry.org/

--- Page 37 ---
Principles for Ontology Creation
http://www.obofoundry.org/

--- Page 38 ---
AI-B C -S R
ASED OMMON ENSE EASONING
• Pre-LLM work in natural language processing
• Makes use of large datasets and deep neural nets
• Getting some interesting results now

--- Page 39 ---
https://github.com/allenai/comet-atomic-2020

--- Page 40 ---
Project link no longer active
https://mosaickg.apps.allenai.org/model_comet2020_entities

--- Page 41 ---
https://mosaickg.apps.allenai.org/model_comet2020_entities

--- Page 42 ---
https://mosaickg.apps.allenai.org/model_comet2020_entities

--- Page 43 ---
https://mosaickg.apps.allenai.org/model_comet2020_entities

--- Page 44 ---
https://mosaickg.apps.allenai.org/model_comet2020_entities

--- Page 45 ---
S : O
UMMARY NTOLOGIES
• Goes beyond taxonomies and faceted metadata to express
relationships, constraints, and rules
• Useful (maybe) for computer-assisted analysis
• Complex to build and maintain
• Difficult to show in an interface for everyday use
• AI methods are making progress in automating creation

--- Page 46 ---
T
AGS VS
F C
ACETED ATEGORIES

--- Page 47 ---
Faceted Categories vs Tags
Faceted categories Tags
• Multiple facets per item • Multiple tags per item
• Hierarchical (often) • Flat categories (usually)
• Intentionally Constructed • Any term is ok
Organization
• User-driven
• Controlled vocabulary
• Collection owner

--- Page 50 ---
Unsplash vs Google Images

--- Page 51 ---
Tags for an Unsplash Image
Ashley Batz
Unsplash

--- Page 52 ---
Google image groupings vs facets
Example: query on “string”
If this were organized, we’d have categories like:
- material (nylon, plastic)
- appearance (transparent, colors)
- uses (jewelry > bracelets, instruments > guitar, etc)

--- Page 53 ---
Faceted Categories vs Automated Tags
(google images, etc)
Faceted categories Tags
• Multiple facets per item • Multiple tags per item
• Hierarchical (often) • Flat categories (usually)
• Controlled vocabulary • Any term is ok
• Collection owner • Algorithmically and data driven
• Category labels intentionally selected • Categories data-driven, and uneven in
from an organized topic or field content coverage.

--- Page 54 ---
A : C F I S
SSIGNMENT REATE A ACETED MAGE EARCH AND
N I
AVIGATION NTERFACE
1. Select 25 images for a collection
2. Create the faceted hierarchical category structure
3. Assign the categories to the images
4. Put everything into the Algolia system
5. Write up your assignment

--- Page 55 ---
Example from previous class

--- Page 56 ---
Example from previous class

--- Page 57 ---
Example from previous class

--- Page 58 ---
A D
SSIGNMENT ETAILS
• Assigned one partner to work with
• Need to create a JSON file
• describes both the categories and the images
• Be sure to use a code editor to create the JSON

--- Page 59 ---
Algolia JSON Format
These are flat facets
Notice that you can
have a list of
attributes for a given
item (colors in this
example)
https://www.algolia.com/blog/engineering/facets-data-model-of-json-records

--- Page 60 ---
Algolia JSON Format
This is a hierarchical facet
Notice that you can have a list of facet attributes for a given item (two kinds
of level1 and two kinds of level2 in this example)
Thus, you can assign Le Guin to be an author of both Scifi and Lit,
And within scifi, to Time travel and within lit to Literary.
https://www.algolia.com/blog/engineering/facets-data-model-of-json-records

--- Page 61 ---
JSON format example
including hierarchical facet with multiple assignments


================================================================================
FILE: 9_information_architecture.pdf
================================================================================

--- Page 1 ---
I 202: I O
NFORMATION RGANIZATION
& R
ETRIEVAL
F 2025
ALL
Class 9: Information Architecture

--- Page 2 ---
Today’s Focus
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

--- Page 3 ---
Today’s Outline
Thematic Categories
Information Architecture
Breadcrumbs
Content Management Systems

--- Page 4 ---
H T D F ?
OW O EFINE THE ACETS
• Depends on the makeup of the collection
• Depends on the goals of the use
• Then ask the classic questions:
• Who, what, when, where, why, how much, under
what circumstances?

--- Page 5 ---
Five Hierarchical Faceted Categories
Clothing
Nature Hats
Animal Cowboy Hat
Mammal
Horse Is-a
Media
Is-a
Engraving
Wood Eng.
Occupations Is-a-type-of
Cowboy
Location
North America
America
Is-a-type-of Is-in or
is-part-of
A Bucking Bronco by Henry Wolf, https://art.famsf.org/henry-wolf/bucking-bronco-19633026024

--- Page 6 ---
Description: 19th c. paint horse; saddle and hackamore; spurs;
M
OTIVATION
bandana on rider; old time cowboy hat; underchin thong; flying
off.
By using faceted categories,
what are we not capturing?
The hat flew off;
The bandana stayed on.
The thong is part of the hat.
The bandana is on the cowboy
(not the horse).
The saddle is on the horse
(not the cowboy).
A Bucking Bronco by Henry Wolf, https://art.famsf.org/henry-wolf/bucking-bronco-19633026024

--- Page 7 ---
H T H M R ?
OW O ANDLE ISSING ELATIONS
• Ontology is one way, but complex for users
• Alternative: Thematic Categories
• Combine key facets into groups that are meaningful for the
application
• These can be hierarchical if the underlying facets are
Example:
Facet: Objects > Weapons
Facet: People > Military Personnel
Facet: Even: Battle
Combine these to make a Theme
Theme: Armed Conflict
Composed of all three facets

--- Page 9 ---
What
What
What
When
Thematic Let’s drill down
What into the Military
Where
Thematic
Property
Category
What

--- Page 11 ---
T ( )
HEMATIC ASSOCIATIONAL
C
ATEGORIES
• Combine one entity type that is strongly associated with
another, but is not taxonomically related
• Examples:
• Field of Study and Object of Study (Astronomy / Star)
• Process and its Agent (Frying / Pan)
• Concepts and their Properties (Flame / Heat)
• Action and Product of Action (Frying / Cooked Eggs)
• Concepts linked by Causality (Shooting / Murder)
• Activity and its Tools (Camping / Tent; Astronomy /
Telescope)

--- Page 13 ---
N C :
ETFLIX ATEGORIES
C C C T
OMBINES ATEGORIES TO REATE HEMES
• Netflix has been having humans tag videos since they were a
DVD mail service
• >3,000 categories
• Used to create traditional genre categories (Action) as well
as custom thematic categories (Action with a Side of
Romance)
• Have to know the code number (Action with a Side of Romance
is at http://netflix.com/browse/genre/81647318 )
https://www.netflix.com/tudum/articles/netflix-secret-codes-guide

--- Page 14 ---
N C : T
ETFLIX ATEGORIES HEMA
• Traditional genres: comedy, suspense
• Genres + modifiers: Feel-good dramas
• Metadata based categories: New releases
• Thematic categories: reluctant adults, fight-the-system

--- Page 16 ---
E : D N
XERCISE ISCUSS ETFLIX
C T
ATEGORY YPES
Focus on the ones that are combinations of categories:
Netflix Codes 2025: All The Secret Movie & Series Category Codes on Netflix
https://www.whats-on-netflix.com/news/the-netflix-id-bible-every-category-on-netflix/

--- Page 17 ---
Thematic Categories
https://xkcd.com/2456/

--- Page 18 ---
S : T C
UMMARY HEMATIC ATEGORIES
• To capture cross-cutting concepts, it can be helpful to
combine facets into themes
• These themes can take on several forms, including
• Combinations of related facets (weapons, soldiers, wars)
• Combinations of facets that support a commonly used service
(how to get a small business loan)
• Combinations of facets and modifiers (feel-good dramas)

--- Page 19 ---
I
NFORMATION
A
RCHITECTURE
Where the rubber (info organization) hits the road (website / mobile app design)
Hal Gatewood, Unsplash

--- Page 20 ---
I A (IA)
NFORMATION RCHITECTURE
• Asks: how to organize and structure information to aid
navigation and search?
• Methods are not algorithmic; require practice and
judgment
• Practitioner-driven (vs research-driven)

--- Page 21 ---
Web User
Interface
Information Design
Architecture
Information
Navigation
Design
Design
Graphic Design
Newman & Landay, DIS ‘00

--- Page 22 ---
Job listing description (9/25)

--- Page 23 ---
https://www.altexsoft.com/blog/uxdesign/information-architecture/

--- Page 24 ---
N
AVIGATION
B
READCRUMBS
Caroline Attwood, Unsplash

--- Page 25 ---
N B
AVIGATION READCRUMBS
• Remind the user of the path they have taken
• A combination of hyperlinks, filters, and queries
• Modifying it can expand the set of options
• Click on a prior link, or remove a filter (facet or other)
• There can be a tension between the dynamic nature of
a path taken and the desire to have a stable link for a
page.

--- Page 26 ---
N B
AVIGATION READCRUMBS
Remind the user of the path they have taken
A combination of hyperlinks, filters, and queries
Flamenco, Houzz, art.com, titlenine

--- Page 27 ---
N B
AVIGATION READCRUMBS
Remind the user of the path they have taken
A combination of hyperlinks, filters, and queries
Matching search terms to the breadcrumb
Yelp.com

--- Page 28 ---
N B
AVIGATION READCRUMBS
Modifying it can expand the set of options
Click on a prior link, or remove a filter (facet or
other)

--- Page 29 ---
N B
AVIGATION READCRUMBS
There can be a tension between the dynamic nature of a path taken
and the desire to have a stable link for a page.

--- Page 30 ---
N B
AVIGATION READCRUMBS
There can be a tension between the dynamic nature of a path taken
and the desire to have a stable link for a page.

--- Page 31 ---
N B
AVIGATION READCRUMBS
There can be a tension between the dynamic nature of a path taken
and the desire to have a stable link for a page.

--- Page 32 ---
Watch the Breadcrumbs from the Thematic
Category “snow”

--- Page 33 ---
Watch the Breadcrumbs

--- Page 34 ---
Thematic Category

--- Page 35 ---
A E S
NALYZING XISTING ITES
• Study the content, use, and placement of
• Categories on the page, with explanatory text,
• Tabs organizing content,
• Menus: sidebar vs top vs footer
• Breadcrumbs
• Repeated info: which, where, and why?
• These are all choices about how to organize content
• Diagramming them is a good way to learn

--- Page 36 ---
E R S
XAMPLES OF ICHLY TRUCTURED
S
ITES
• Many US Government websites were modernized in 2009-2016
• Focus on usability, plain text, helping people get their tasks done
• Huge collections of information; have to meet many peoples’ needs
• Often highlight services; these are often phrased as questions, and are
thematically structured
• Examples:
• sba.gov (Small Business Administration; activity-centric)
• usa.gov (Overview of all Federal Agencies; uses github!)
• cbo.gov (Congressional Budget Office; Lots of data and reports)
• uspto.gov (US Patent and Trademark Office, complex activities / data)

--- Page 39 ---
Example of Service website:
sba.gov organizes by “what you can do”

--- Page 40 ---
Example of Service website:
uspto.gov includes services too

--- Page 41 ---
https://digital.gov/resources/required-web-content-and-links/?dg

--- Page 42 ---
E : A S ’
XERCISE NALYZE A ITE S
I A
NFORMATION RCHITECTURE
• Look at one of these:
• sba.gov, uspto.gov, or cbo.gov
• rei.com, titlenine.com
• Try to infer the organization that underlies the pages
• Look for faceted categories, taxonomies, and thematic
relations
• Determine under what circumstances a page can be
accessed via multiple different paths

--- Page 43 ---
HCI C I S
OURSES IN THE CHOOL
• User Interface Design & Development (I 213)
Kimiko Ryokai Nilofaur Salehi
• User Experience Research (I 214)
• Information Visualization & Presentation (I 247)
• Interface Aesthetics (C256)
Stef Hutka Kay Ashoalu
• Tangible User Interfaces (C262)
• Front end web architecture (253A)
(Many others offered on an occasional basis)
Steve Fadden Laith Ulaby

--- Page 44 ---
C M S
ONTENT ANAGEMENT YSTEMS
• Helps organize the information
• Allows multiple authors
• Allows generation of websites from the info
• The technology is always changing

--- Page 45 ---
Content Management System:
Simple Deployment
“headless” CMS: uses a restful API
https://craftercms.org/blog/2017/03/wcm-architectures-coupled-decoupled-or-headless

--- Page 46 ---
https://www.contentstack.com/docs/developers/architecture-diagrams/simple-website-detailed-architecture/

--- Page 47 ---
S : I
UMMARY NFORMATION
A
RCHITECTURE
• Information architecture is a combination of a category
system and a navigation structure
• Website design, and information-heavy apps, are a
combination of information architecture, graphic, and
interaction design
• There are specialized content management systems that aid
in creating information architectures, but they are often
complex

--- Page 48 ---
Next week
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

--- Page 49 ---
Next week
Data / Information
Technology Support for Info Org
Collections
- Identifiers
Categories
- Metadata
- Types of categories
- Markup
- Cognitive / language aspects
- Schema / Databases
- Naming / Lexical similarity
- Search Ranking / Evaluation
- Structure
- Automated category creation
- Hierarchical / Taxonomy
- Automated similarity
- Faceted
- Overlapping / Clustering
Social / Ethical Aspects
- Network / Ontology
- Cultural Bias
- Use in Navigation & Search
- Intellectual Property
- Information Architecture
- Standards Process
- Faceted Navigation

